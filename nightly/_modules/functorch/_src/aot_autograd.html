


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>functorch._src.aot_autograd &mdash; functorch nightly documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/togglebutton.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/mystnb.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  <!-- Google Analytics -->
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117752657-2');
    </script>
  
  <!-- End Google Analytics -->
  

  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/functorch/versions.html'>nightly (1.14.0a0+gitbbaa063) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          
            
            
              
            
            
              <p class="caption"><span class="caption-text">functorch: Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../install.html">Install functorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/whirlwind_tour.html">Whirlwind Tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ux_limitations.html">UX Limitations</a></li>
</ul>
<p class="caption"><span class="caption-text">functorch API Reference and Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../functorch.html">functorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../experimental.html">functorch.experimental</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../aot_autograd.html">functorch.compile (experimental)</a></li>
</ul>
<p class="caption"><span class="caption-text">functorch Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/jacobians_hessians.html">Jacobians, Hessians, hvp, vhp, and more: composing functorch transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/ensembling.html">Model ensembling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/per_sample_grads.html">Per-sample-gradients</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/neural_tangent_kernels.html">Neural Tangent Kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/aot_autograd_optimizations.html">AOT Autograd - How to use and optimize?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/minifier.html">Using the Minifier</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
      <li>functorch._src.aot_autograd</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for functorch._src.aot_autograd</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">dataclasses</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">contextmanager</span><span class="p">,</span> <span class="n">nullcontext</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">wraps</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span>
<span class="kn">from</span> <span class="nn">torch.fx.experimental.proxy_tensor</span> <span class="kn">import</span> <span class="n">is_sym_node</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.fx.traceback</span> <span class="k">as</span> <span class="nn">fx_traceback</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.utils._pytree</span> <span class="k">as</span> <span class="nn">pytree</span>
<span class="kn">import</span> <span class="nn">torch.utils.dlpack</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">torch._subclasses</span> <span class="kn">import</span> <span class="n">FakeTensorMode</span><span class="p">,</span> <span class="n">CrossRefFakeMode</span>
<span class="kn">from</span> <span class="nn">torch.fx</span> <span class="kn">import</span> <span class="n">immutable_collections</span><span class="p">,</span> <span class="n">Interpreter</span>
<span class="kn">from</span> <span class="nn">torch.fx.experimental.symbolic_shapes</span> <span class="kn">import</span> <span class="n">ShapeEnv</span>
<span class="kn">from</span> <span class="nn">torch.nn.utils</span> <span class="kn">import</span> <span class="n">stateless</span>

<span class="kn">from</span> <span class="nn">functorch</span> <span class="kn">import</span> <span class="n">make_fx</span>
<span class="kn">from</span> <span class="nn">torch._dispatch.python</span> <span class="kn">import</span> <span class="n">enable_python_dispatcher</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">config</span>
<span class="kn">from</span> <span class="nn">.named_members_polyfill</span> <span class="kn">import</span> <span class="n">_named_buffers</span><span class="p">,</span> <span class="n">_named_parameters</span>
<span class="kn">from</span> <span class="nn">.partitioners</span> <span class="kn">import</span> <span class="n">default_partition</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">torchdynamo</span> <span class="kn">import</span> <span class="n">disable</span> <span class="k">as</span> <span class="n">disable_torchdynamo</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">disable_torchdynamo</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">torchdynamo.utils</span> <span class="kn">import</span> <span class="n">dynamo_timed</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">dynamo_timed</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="n">pytree</span><span class="o">.</span><span class="n">_register_pytree_node</span><span class="p">(</span>
    <span class="n">immutable_collections</span><span class="o">.</span><span class="n">immutable_list</span><span class="p">,</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="kc">None</span><span class="p">),</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="n">immutable_collections</span><span class="o">.</span><span class="n">immutable_list</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">pytree</span><span class="o">.</span><span class="n">_register_pytree_node</span><span class="p">(</span>
    <span class="n">immutable_collections</span><span class="o">.</span><span class="n">immutable_dict</span><span class="p">,</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">values</span><span class="p">()),</span> <span class="nb">list</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">keys</span><span class="p">())),</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="n">immutable_collections</span><span class="o">.</span><span class="n">immutable_dict</span><span class="p">(</span>
        <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">x</span><span class="p">)}</span>
    <span class="p">),</span>
<span class="p">)</span>

<span class="n">aten</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span>


<span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">preserve_rng_state</span><span class="p">():</span>
    <span class="n">rng_state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">get_rng_state</span><span class="p">())</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="n">cuda_rng_state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_rng_state</span><span class="p">())</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_rng_state</span><span class="p">(</span><span class="n">rng_state</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_rng_state</span><span class="p">(</span><span class="n">cuda_rng_state</span><span class="p">)</span>


<span class="c1"># Set up hooks so that during backward the fx&#39;s stack_trace is properly set</span>
<span class="n">callback_set</span> <span class="o">=</span> <span class="kc">False</span>


<span class="k">def</span> <span class="nf">setup_stacktrace_preservation_hooks</span><span class="p">(</span><span class="n">roots</span><span class="p">:</span> <span class="n">List</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">iter_graph</span><span class="p">(</span><span class="n">roots</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">roots</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">seen</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">roots</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">node</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">seen</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
                <span class="n">q</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>

        <span class="k">while</span> <span class="n">q</span><span class="p">:</span>
            <span class="n">node</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">popleft</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">fn</span><span class="p">,</span> <span class="n">_idx</span> <span class="ow">in</span> <span class="n">node</span><span class="o">.</span><span class="n">next_functions</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">fn</span> <span class="ow">in</span> <span class="n">seen</span> <span class="ow">or</span> <span class="n">fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="n">seen</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
                <span class="n">q</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>

            <span class="k">yield</span> <span class="n">node</span>

    <span class="k">def</span> <span class="nf">get_callback</span><span class="p">(</span><span class="n">saved_stack_</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">callback</span><span class="p">():</span>
            <span class="k">global</span> <span class="n">callback_set</span>
            <span class="n">fx_traceback</span><span class="o">.</span><span class="n">set_stack_trace</span><span class="p">(</span><span class="n">saved_stack_</span><span class="p">)</span>
            <span class="n">callback_set</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">return</span> <span class="n">callback</span>

    <span class="k">def</span> <span class="nf">get_prehook</span><span class="p">(</span><span class="n">stack_</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">prehook</span><span class="p">(</span><span class="n">grad_output</span><span class="p">):</span>
            <span class="k">global</span> <span class="n">callback_set</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">callback_set</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">variable</span><span class="o">.</span><span class="n">Variable</span><span class="o">.</span><span class="n">_execution_engine</span><span class="o">.</span><span class="n">queue_callback</span><span class="p">(</span>
                    <span class="n">get_callback</span><span class="p">(</span><span class="n">fx_traceback</span><span class="o">.</span><span class="n">format_stack</span><span class="p">())</span>
                <span class="p">)</span>
                <span class="n">callback_set</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="n">fx_traceback</span><span class="o">.</span><span class="n">set_stack_trace</span><span class="p">(</span><span class="n">stack_</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">prehook</span>

    <span class="k">def</span> <span class="nf">get_posthook</span><span class="p">(</span><span class="n">special_stack_</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">posthook</span><span class="p">(</span><span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
            <span class="n">fx_traceback</span><span class="o">.</span><span class="n">set_stack_trace</span><span class="p">(</span><span class="n">special_stack_</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">posthook</span>

    <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">iter_graph</span><span class="p">(</span><span class="n">roots</span><span class="p">):</span>
        <span class="n">forward_node_stack</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;traceback_&quot;</span><span class="p">,</span> <span class="p">[])</span>
        <span class="n">node</span><span class="o">.</span><span class="n">register_prehook</span><span class="p">(</span><span class="n">get_prehook</span><span class="p">(</span><span class="n">forward_node_stack</span><span class="p">))</span>

        <span class="n">special_stack</span> <span class="o">=</span> <span class="n">forward_node_stack</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">special_stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="s2">&quot;Gradient addition node due to multiple use of tensor around:&quot;</span>
        <span class="p">)</span>
        <span class="n">node</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span><span class="n">get_posthook</span><span class="p">(</span><span class="n">special_stack</span><span class="p">))</span>


<span class="c1"># This is a version of functionalization that is specifically designed</span>
<span class="c1"># for the AOTAutograd use case.  It might be generally applicable though</span>
<span class="c1"># (if so, move it out of this file), so I&#39;ve tried to give it a name that</span>
<span class="c1"># describes what it does.</span>
<span class="c1">#</span>
<span class="c1"># Given a function f, it produces a new function g that:</span>
<span class="c1">#</span>
<span class="c1">#   - Detaches all inputs before running f; the inner function</span>
<span class="c1">#     does not directly participate in any pre-existing autograd.</span>
<span class="c1">#     preserve_requires_grad is provided as a convenience to set the</span>
<span class="c1">#     requires_grad on the new detached leaves in sync with the originals.</span>
<span class="c1">#     (NB: In principle, you could backward through the pure operations</span>
<span class="c1">#     produced by functionalization; this is not used for AOTAutograd</span>
<span class="c1">#     and we have not tested it.)</span>
<span class="c1">#</span>
<span class="c1">#   - Functionalizes all operations on f, under the assumption that the passed</span>
<span class="c1">#     in function f must be &quot;observationally pure&quot;; that is, it cannot perform any</span>
<span class="c1">#     mutations (inplace data or view operations) on the passed in inputs, nor is</span>
<span class="c1">#     it allowed to directly close over tensors that aren&#39;t passed via its</span>
<span class="c1">#     arguments.  See</span>
<span class="c1">#     https://docs.google.com/document/d/19UoIh_SVrMy_b2Sx5ZaeOJttm6P0Qmyss2rdBuyfoic/edit</span>
<span class="c1">#     for discussion how how to implement the more complicated case.</span>
<span class="c1">#</span>
<span class="c1"># Unlike functorch&#39;s variant, this doesn&#39;t use the functorch level system,</span>
<span class="c1"># instead it directly uses PyTorch&#39;s conventional dispatcher to hit the</span>
<span class="c1"># functionalization key.  In particular, this means that FunctionalTensorWrapper</span>
<span class="c1"># can have autograd data stored directly on it.</span>
<span class="c1">#</span>
<span class="c1"># In typical AOTAutograd usage, the dispatch key order will look like:</span>
<span class="c1">#</span>
<span class="c1">#   Autograd - Functionalization ~~~~&gt; Proxy Mode - Fake Tensor</span>
<span class="c1">#       outer tensor                        inner tensor</span>
<span class="c1">#</span>
<span class="c1"># TODO: Provide a faster version of this that assumes flat arguments</span>
<span class="c1"># (so no pytree necessary)</span>
<span class="k">def</span> <span class="nf">detach_and_functionalize_pure</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">preserve_requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nd">@wraps</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">inner</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">to_fun</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
                <span class="n">r</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_to_functional_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
                <span class="c1"># NB: r is a leaf; it has no grad_fn relating</span>
                <span class="c1"># it to t.  If t has autograd metadata, that</span>
                <span class="c1"># metadata was preserved *inside* the r wrapper</span>
                <span class="k">if</span> <span class="n">preserve_requires_grad</span><span class="p">:</span>
                    <span class="n">r</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">requires_grad</span>
                <span class="k">return</span> <span class="n">r</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">t</span>

        <span class="n">f_args</span><span class="p">,</span> <span class="n">f_kwargs</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="n">to_fun</span><span class="p">,</span> <span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">_enable_functionalization</span><span class="p">(</span><span class="n">reapply_views</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">outs</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">f_args</span><span class="p">,</span> <span class="o">**</span><span class="n">f_kwargs</span><span class="p">)</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">_disable_functionalization</span><span class="p">()</span>

        <span class="c1"># Detect input mutation and error if found</span>
        <span class="n">flat_args</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">((</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>
        <span class="n">flat_f_args</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">((</span><span class="n">f_args</span><span class="p">,</span> <span class="n">f_kwargs</span><span class="p">))</span>

        <span class="c1"># This is just for sanity checking, can be skipped</span>
        <span class="k">for</span> <span class="n">arg</span><span class="p">,</span> <span class="n">f_arg</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">flat_args</span><span class="p">,</span> <span class="n">flat_f_args</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
                <span class="k">continue</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">_sync</span><span class="p">(</span><span class="n">f_arg</span><span class="p">)</span>
            <span class="n">new_arg</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_from_functional_tensor</span><span class="p">(</span><span class="n">f_arg</span><span class="p">)</span>
            <span class="c1"># I want to do this assert, but it is annoying because</span>
            <span class="c1"># we have operator tests that have mutating inputs.  So</span>
            <span class="c1"># I do something unsound instead</span>
            <span class="c1"># assert arg is new_arg, &quot;input argument was mutated, this is not valid&quot;</span>
            <span class="k">if</span> <span class="n">arg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">new_arg</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">arg</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">new_arg</span><span class="o">.</span><span class="n">shape</span>
                <span class="n">arg</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">new_arg</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">from_fun</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">_is_functional_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">t</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">_sync</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_from_functional_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="n">from_fun</span><span class="p">,</span> <span class="n">outs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">inner</span>


<span class="c1"># This creates a joint forwards-backwards function given both</span>
<span class="c1"># the primals (to run forwards) and tangents (to run backwards).</span>
<span class="c1">#</span>
<span class="c1"># It has a precondition which is that the passed in function</span>
<span class="c1"># must be observationally pure; it is not permitted to mutate</span>
<span class="c1"># the primals or tangents.</span>
<span class="k">def</span> <span class="nf">create_joint_forward_backward_pure</span><span class="p">(</span><span class="n">fn</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">joint_forward_backward</span><span class="p">(</span>
        <span class="n">primals</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">tangents</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]]:</span>
        <span class="c1"># Call the forward pass</span>
        <span class="n">outs</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">primals</span><span class="p">)</span>
        <span class="c1"># Get the inputs that need gradients</span>
        <span class="n">grad_primals</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">inputs_needs_grads</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">primals</span><span class="p">:</span>
            <span class="n">is_grad_tensor</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span>
            <span class="n">inputs_needs_grads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">is_grad_tensor</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">is_grad_tensor</span><span class="p">:</span>
                <span class="n">grad_primals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

        <span class="c1"># Get the outputs that need gradients</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">tangents</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">outs</span><span class="p">)</span>
        <span class="n">needed_outs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">needed_tangents</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">out</span><span class="p">,</span> <span class="n">tangent</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">outs</span><span class="p">,</span> <span class="n">tangents</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">out</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                <span class="n">needed_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
                <span class="n">needed_tangents</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tangent</span><span class="p">)</span>

        <span class="n">setup_stacktrace_preservation_hooks</span><span class="p">([</span><span class="n">out</span><span class="o">.</span><span class="n">grad_fn</span> <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">needed_outs</span><span class="p">])</span>

        <span class="n">backward_out</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># Call the backwards pass</span>
        <span class="k">if</span> <span class="n">grad_primals</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">fx_traceback</span><span class="o">.</span><span class="n">override_stack_trace</span><span class="p">():</span>
                <span class="n">backward_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span>
                    <span class="n">needed_outs</span><span class="p">,</span>
                    <span class="n">grad_primals</span><span class="p">,</span>
                    <span class="n">grad_outputs</span><span class="o">=</span><span class="n">needed_tangents</span><span class="p">,</span>
                    <span class="n">allow_unused</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>
        <span class="n">backward_out_iter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">backward_out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">outs</span><span class="p">,</span> <span class="p">[</span>
            <span class="nb">next</span><span class="p">(</span><span class="n">backward_out_iter</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="k">else</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">inputs_needs_grads</span>
        <span class="p">]</span>

    <span class="k">return</span> <span class="n">joint_forward_backward</span>


<span class="k">def</span> <span class="nf">normalize_as_list</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>


<span class="n">aot_autograd_decompositions</span> <span class="o">=</span> <span class="p">{}</span>


<span class="c1"># This is a list since looking forward, we can have this arbitrarily nested.</span>
<span class="n">graph_being_compiled</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">nth_graph</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;model&quot;</span>


<span class="k">def</span> <span class="nf">set_model_name</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">model_name</span>
    <span class="n">model_name</span> <span class="o">=</span> <span class="n">name</span>


<span class="k">def</span> <span class="nf">get_aot_compilation_context</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">graph_being_compiled</span><span class="p">),</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">nth_graph</span>


<span class="k">def</span> <span class="nf">get_aot_graph_name</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the name of the graph being compiled.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">graph_being_compiled</span><span class="p">,</span> <span class="n">nth_graph</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="s1">&#39;_&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">graph_being_compiled</span><span class="p">)</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">nth_graph</span><span class="si">}</span><span class="s2">&quot;</span>


<span class="n">get_graph_being_compiled</span> <span class="o">=</span> <span class="n">get_aot_graph_name</span>


<span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">track_graph_compiling</span><span class="p">(</span><span class="n">graph_name</span><span class="p">,</span> <span class="n">increment_index</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">graph_being_compiled</span>
    <span class="n">graph_being_compiled</span> <span class="o">=</span> <span class="p">[</span><span class="n">graph_name</span><span class="p">]</span>
    <span class="k">yield</span>
    <span class="k">if</span> <span class="n">increment_index</span><span class="p">:</span>
        <span class="k">global</span> <span class="n">nth_graph</span>
        <span class="n">nth_graph</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">graph_being_compiled</span> <span class="o">=</span> <span class="p">[]</span>


<span class="k">def</span> <span class="nf">make_boxed_func</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">g</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>

    <span class="n">g</span><span class="o">.</span><span class="n">_boxed_call</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">g</span>


<span class="k">def</span> <span class="nf">make_boxed_compiler</span><span class="p">(</span><span class="n">compiler</span><span class="p">):</span>
    <span class="nd">@wraps</span><span class="p">(</span><span class="n">compiler</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">fx_g</span><span class="p">,</span> <span class="n">inps</span><span class="p">):</span>
        <span class="n">out_f</span> <span class="o">=</span> <span class="n">compiler</span><span class="p">(</span><span class="n">fx_g</span><span class="p">,</span> <span class="n">inps</span><span class="p">)</span>
        <span class="n">fx_g</span> <span class="o">=</span> <span class="n">make_boxed_func</span><span class="p">(</span><span class="n">out_f</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">fx_g</span>

    <span class="k">return</span> <span class="n">f</span>


<span class="k">def</span> <span class="nf">call_func_with_args</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">steal_args</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">disable_amp</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">steal_args</span><span class="p">:</span>
        <span class="n">args</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">disable_amp</span><span class="p">:</span>
        <span class="n">guard</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_DisableAutocast</span><span class="p">()</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="s2">&quot;_boxed_call&quot;</span><span class="p">):</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">normalize_as_list</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">args</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># TODO: Please remove soon</span>
            <span class="c1"># https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Your compiler for AOTAutograd is returning a a function that doesn&#39;t take boxed arguments. &quot;</span>
                <span class="s2">&quot;Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. &quot;</span>
                <span class="s2">&quot;See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.&quot;</span>
            <span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">normalize_as_list</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">))</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">disable_amp</span><span class="p">:</span>
            <span class="k">del</span> <span class="n">guard</span>
    <span class="k">return</span> <span class="n">out</span>


<span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">AOTConfig</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for AOTDispatcher</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">fw_compiler</span><span class="p">:</span> <span class="n">Callable</span>
    <span class="n">bw_compiler</span><span class="p">:</span> <span class="n">Callable</span>
    <span class="n">partition_fn</span><span class="p">:</span> <span class="n">Callable</span>
    <span class="n">decompositions</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span>
    <span class="n">num_params_buffers</span><span class="p">:</span> <span class="nb">int</span>


<span class="k">def</span> <span class="nf">aot_dispatch_base</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">aot_config</span><span class="p">:</span> <span class="n">AOTConfig</span><span class="p">):</span>
    <span class="n">fw_module</span> <span class="o">=</span> <span class="n">make_fx</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">,</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span><span class="p">)(</span><span class="o">*</span><span class="n">flat_args</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">debug_graphs</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;====== Forward (only) graph ======&quot;</span><span class="p">)</span>
        <span class="n">fw_module</span><span class="o">.</span><span class="n">print_readable</span><span class="p">()</span>


    <span class="n">disable_amp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_is_any_autocast_enabled</span><span class="p">()</span>
    <span class="n">context</span> <span class="o">=</span> <span class="n">disable_autocast_manager</span> <span class="k">if</span> <span class="n">disable_amp</span> <span class="k">else</span> <span class="n">nullcontext</span>

    <span class="k">with</span> <span class="n">context</span><span class="p">(),</span> <span class="n">track_graph_compiling</span><span class="p">(</span><span class="s2">&quot;inference&quot;</span><span class="p">):</span>
        <span class="n">compiled_fw</span> <span class="o">=</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">fw_compiler</span><span class="p">(</span><span class="n">fw_module</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">)</span>

    <span class="nd">@wraps</span><span class="p">(</span><span class="n">compiled_fw</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">new_fn</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="n">fw_outs</span> <span class="o">=</span> <span class="n">call_func_with_args</span><span class="p">(</span><span class="n">compiled_fw</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">disable_amp</span><span class="o">=</span><span class="n">disable_amp</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">fw_outs</span>

    <span class="k">return</span> <span class="n">new_fn</span>


<span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">disable_autocast_manager</span><span class="p">():</span>
    <span class="n">guard</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_DisableAutocast</span><span class="p">()</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="k">del</span> <span class="n">guard</span>


<span class="k">def</span> <span class="nf">aot_dispatch_autograd</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">aot_config</span><span class="p">:</span> <span class="n">AOTConfig</span><span class="p">):</span>
    <span class="c1"># Deduplicate inputs.  Suppose you have:</span>
    <span class="c1">#</span>
    <span class="c1">#   [a, b, a, c]</span>
    <span class="c1">#</span>
    <span class="c1"># We want:</span>
    <span class="c1">#</span>
    <span class="c1">#   remove_dupe_args([a, b, a, c]) == [a, b, c]</span>
    <span class="c1">#   add_dupe_args([a, b, c]) == [a, b, a, c]</span>
    <span class="c1">#</span>
    <span class="c1"># This is done via (respectively):</span>
    <span class="c1">#</span>
    <span class="c1">#   seen_args = {2}  # what to drop</span>
    <span class="c1">#   add_dupe_map = {  # how to get args from the deduped list</span>
    <span class="c1">#       0: 0,</span>
    <span class="c1">#       1: 1,</span>
    <span class="c1">#       2: 0,</span>
    <span class="c1">#       3: 2,</span>
    <span class="c1">#   }</span>
    <span class="c1">#</span>
    <span class="c1"># Whether to use flat_args or deduped_flat_args?  flat_fn takes flat_args,</span>
    <span class="c1"># and the autograd.Function must take deduped_flat_args; everything</span>
    <span class="c1"># else is just getting the types right.</span>

    <span class="n">seen_args</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">keep_arg_mask</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">dropped_args</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">add_dupe_map</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">duped_arg_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)</span>

    <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># index into deduped_flat_args</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">flat_args</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">seen_args</span><span class="p">:</span>
            <span class="n">keep_arg_mask</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">dropped_args</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">add_dupe_map</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">seen_args</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
            <span class="k">continue</span>
        <span class="n">keep_arg_mask</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">seen_args</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">j</span>
        <span class="n">add_dupe_map</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">j</span>
        <span class="n">j</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="c1"># NB: Hot path, avoid set lookups here</span>
    <span class="k">def</span> <span class="nf">remove_dupe_args</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">dropped_args</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">args</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">t</span> <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">keep</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">keep_arg_mask</span><span class="p">)</span> <span class="k">if</span> <span class="n">keep</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">add_dupe_args</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">dropped_args</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">args</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">args</span><span class="p">[</span><span class="n">add_dupe_map</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">duped_arg_len</span><span class="p">)]</span>

    <span class="n">deduped_flat_args</span> <span class="o">=</span> <span class="n">remove_dupe_args</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)</span>

    <span class="n">joint_forward_backward</span> <span class="o">=</span> <span class="n">create_joint_forward_backward_pure</span><span class="p">(</span><span class="k">lambda</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">flat_fn</span><span class="p">(</span><span class="o">*</span><span class="n">add_dupe_args</span><span class="p">(</span><span class="n">args</span><span class="p">)))</span>

    <span class="n">out</span> <span class="o">=</span> <span class="n">flat_fn</span><span class="p">(</span><span class="o">*</span><span class="n">flat_args</span><span class="p">)</span>
    <span class="c1"># Collect info on which output tensors require gradients,</span>
    <span class="c1"># so we can mark them properly in the returned autograd.Function</span>
    <span class="n">_flat_outs_not_requiring_grad</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">(</span>
        <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">out</span>
        <span class="p">)</span>
    <span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">x</span><span class="p">,</span>
        <span class="n">out</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="n">_num_outs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">_num_outs</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="n">joint_inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">deduped_flat_args</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>

    <span class="n">disable_amp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_is_any_autocast_enabled</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">use_functionalize</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">enable_python_dispatcher</span><span class="p">():</span>
            <span class="n">fx_g</span> <span class="o">=</span> <span class="n">make_fx</span><span class="p">(</span>
                <span class="n">detach_and_functionalize_pure</span><span class="p">(</span><span class="n">joint_forward_backward</span><span class="p">),</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span>
            <span class="p">)(</span><span class="o">*</span><span class="n">joint_inputs</span><span class="p">)</span>
        <span class="n">fx_g</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">eliminate_dead_code</span><span class="p">()</span>
        <span class="n">fx_g</span><span class="o">.</span><span class="n">recompile</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;graph partitioning without functionalization is not sound, we may introduce errors&quot;</span><span class="p">)</span>
        <span class="n">fx_g</span> <span class="o">=</span> <span class="n">make_fx</span><span class="p">(</span><span class="n">joint_forward_backward</span><span class="p">,</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span><span class="p">)(</span><span class="o">*</span><span class="n">joint_inputs</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">debug_joint</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;====== Joint graph ======&quot;</span><span class="p">)</span>
        <span class="n">fx_g</span><span class="o">.</span><span class="n">print_readable</span><span class="p">()</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">with</span> <span class="n">track_graph_compiling</span><span class="p">(</span><span class="s2">&quot;joint&quot;</span><span class="p">):</span>
            <span class="n">fw_module</span><span class="p">,</span> <span class="n">bw_module</span> <span class="o">=</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">partition_fn</span><span class="p">(</span><span class="n">fx_g</span><span class="p">,</span> <span class="n">joint_inputs</span><span class="p">)</span>
            <span class="n">fw_outs</span> <span class="o">=</span> <span class="p">[</span><span class="n">n</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">fw_module</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span> <span class="k">if</span> <span class="n">n</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;output&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="c1"># we only need to bookkeep the symints that are saved for bw, not any symints</span>
            <span class="c1"># the user forward might have returned in its own output</span>
            <span class="n">fw_outs</span> <span class="o">=</span> <span class="n">fw_outs</span><span class="p">[</span><span class="n">_num_outs</span><span class="p">:]</span>
            <span class="n">symint_outs</span> <span class="o">=</span> <span class="p">[</span><span class="n">n</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">fw_outs</span> <span class="k">if</span> <span class="n">is_sym_node</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span>
            <span class="n">_num_symints</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">symint_outs</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">debug_graphs</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;====== Forward graph ======&quot;</span><span class="p">)</span>
            <span class="n">fw_module</span><span class="o">.</span><span class="n">print_readable</span><span class="p">()</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;====== Backward graph ======&quot;</span><span class="p">)</span>
            <span class="n">bw_module</span><span class="o">.</span><span class="n">print_readable</span><span class="p">()</span>

        <span class="k">with</span> <span class="n">track_graph_compiling</span><span class="p">(</span><span class="s2">&quot;forward&quot;</span><span class="p">):</span>
            <span class="n">compiled_fw_func</span> <span class="o">=</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">fw_compiler</span><span class="p">(</span><span class="n">fw_module</span><span class="p">,</span> <span class="n">deduped_flat_args</span><span class="p">)</span>

    <span class="k">class</span> <span class="nc">CompiledFunction</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
        <span class="n">compiled_fw</span> <span class="o">=</span> <span class="n">compiled_fw_func</span>
        <span class="n">compiled_bw</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">num_outs</span> <span class="o">=</span> <span class="n">_num_outs</span>
        <span class="n">num_symints</span> <span class="o">=</span> <span class="n">_num_symints</span>
        <span class="n">flat_outs_not_requiring_grad</span> <span class="o">=</span> <span class="n">_flat_outs_not_requiring_grad</span>

        <span class="nd">@staticmethod</span>
        <span class="nd">@disable_torchdynamo</span>
        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">deduped_flat_tensor_args</span><span class="p">):</span>
            <span class="n">fw_outs</span> <span class="o">=</span> <span class="n">call_func_with_args</span><span class="p">(</span>
                <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">compiled_fw</span><span class="p">,</span> <span class="n">deduped_flat_tensor_args</span><span class="p">,</span> <span class="n">disable_amp</span><span class="o">=</span><span class="n">disable_amp</span>
            <span class="p">)</span>
            <span class="n">num_outs</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">num_outs</span>
            <span class="n">num_symints</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">num_symints</span>
            <span class="c1"># Partitioners must put symint arguments at the end separate from tensor arguments</span>
            <span class="k">if</span> <span class="n">num_symints</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="o">*</span><span class="n">fw_outs</span><span class="p">[</span><span class="n">num_outs</span><span class="p">:</span><span class="o">-</span><span class="n">num_symints</span><span class="p">])</span>
                <span class="n">ctx</span><span class="o">.</span><span class="n">symints</span> <span class="o">=</span> <span class="n">fw_outs</span><span class="p">[</span><span class="o">-</span><span class="n">num_symints</span><span class="p">:]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="o">*</span><span class="n">fw_outs</span><span class="p">[</span><span class="n">num_outs</span><span class="p">:])</span>
                <span class="n">ctx</span><span class="o">.</span><span class="n">symints</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="n">fw_outs_not_requiring_grad</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">x</span> <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">fw_outs</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">num_outs</span><span class="p">])</span> <span class="k">if</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">flat_outs_not_requiring_grad</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="p">]</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">mark_non_differentiable</span><span class="p">(</span><span class="o">*</span><span class="n">fw_outs_not_requiring_grad</span><span class="p">)</span>

            <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">fw_outs</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">num_outs</span><span class="p">])</span>

        <span class="nd">@staticmethod</span>
        <span class="nd">@disable_torchdynamo</span>
        <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">flat_args</span><span class="p">):</span>
            <span class="n">contiguous_args</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">else</span> <span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">flat_args</span><span class="p">]</span>
            <span class="n">all_args</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">symints</span><span class="p">)</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span><span class="p">)</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">contiguous_args</span><span class="p">)</span>
            <span class="k">del</span> <span class="n">contiguous_args</span>
            <span class="k">if</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">compiled_bw</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># TODO - pass in fake tensors ?</span>
                <span class="n">context</span> <span class="o">=</span> <span class="n">disable_autocast_manager</span> <span class="k">if</span> <span class="n">disable_amp</span> <span class="k">else</span> <span class="n">nullcontext</span>
                <span class="k">with</span> <span class="n">context</span><span class="p">(),</span> <span class="n">track_graph_compiling</span><span class="p">(</span><span class="s2">&quot;backward&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">):</span>
                    <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">compiled_bw</span> <span class="o">=</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">bw_compiler</span><span class="p">(</span>
                        <span class="n">bw_module</span><span class="p">,</span> <span class="n">all_args</span>
                    <span class="p">)</span>

            <span class="n">ctx</span><span class="o">.</span><span class="n">maybe_clear_saved_tensors</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">call_func_with_args</span><span class="p">(</span>
                <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">compiled_bw</span><span class="p">,</span> <span class="n">all_args</span><span class="p">,</span> <span class="n">steal_args</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">disable_amp</span><span class="o">=</span><span class="n">disable_amp</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

    <span class="nd">@wraps</span><span class="p">(</span><span class="n">CompiledFunction</span><span class="o">.</span><span class="n">apply</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">compiled_function</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="o">*</span><span class="n">remove_dupe_args</span><span class="p">(</span><span class="n">args</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">compiled_function</span>


<span class="nd">@dynamo_timed</span>
<span class="k">def</span> <span class="nf">create_aot_dispatcher_function</span><span class="p">(</span>
    <span class="n">flat_fn</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">aot_config</span><span class="p">:</span> <span class="n">AOTConfig</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Traces the forward and backward graphs of the attr:`flat_fn` to generate a</span>
<span class="sd">    joint graph. The joint graph is an Fx graph with Aten ops. Please refer to</span>
<span class="sd">    the tracing mechanism to understand the graph capturing details.</span>

<span class="sd">    The joint graph is then passed through attr:`partition_fn` to isolate the</span>
<span class="sd">    forward and backward portions, which are then respectively compiled via the</span>
<span class="sd">    provided attr:`fw_compiler` and attr:`bw_compiler`.</span>

<span class="sd">    The resulting compiled forward and backward graphs are then wrapped up in a</span>
<span class="sd">    ``torch.autograd.Function`` object.</span>

<span class="sd">    The calling convention here is that the first aot_config.num_params_buffers</span>
<span class="sd">    inputs in flat_args are parameters and buffers, and the rest are inputs.</span>

<span class="sd">    We use this to assume that parameters/buffer&#39;s shapes don&#39;t change.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># This is the main entry point.</span>
    <span class="c1"># TODO: Chillee argues that dynamo itself should pass in fake tensors to</span>
    <span class="c1"># the list of arguments when compiling; at the moment we do not do this</span>

    <span class="k">if</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span> <span class="o">=</span> <span class="p">{</span>
        <span class="o">**</span><span class="n">aot_autograd_decompositions</span><span class="p">,</span>
        <span class="o">**</span><span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="c1"># NB: don&#39;t bother setting allow_fallback_kernels; this should not actually</span>
    <span class="c1"># be configurable in fake tensor, we should automatically do the right</span>
    <span class="c1"># thing</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">debug_fake_cross_ref</span><span class="p">:</span>
        <span class="c1"># This is a little messy but TorchDynamo directly changes `use_fake_tensor`</span>
        <span class="c1"># so it&#39;s not enough for user to change the config manually</span>
        <span class="c1"># TODO: have TorchDynamo read in `use_fake_tensor` from os environ /</span>
        <span class="c1"># coordinate flags</span>
        <span class="n">config</span><span class="o">.</span><span class="n">use_fake_tensor</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">use_dynamic_shapes</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">config</span><span class="o">.</span><span class="n">use_fake_tensor</span><span class="p">,</span> <span class="s2">&quot;Dynamic shapes only works with fake tensor&quot;</span>

    <span class="n">shape_env</span> <span class="o">=</span> <span class="n">ShapeEnv</span><span class="p">()</span> <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">use_dynamic_shapes</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">fake_mode</span> <span class="o">=</span> <span class="n">FakeTensorMode</span><span class="p">(</span><span class="n">shape_env</span><span class="o">=</span><span class="n">shape_env</span><span class="p">)</span> <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">use_fake_tensor</span> <span class="k">else</span> <span class="n">nullcontext</span><span class="p">()</span>
    <span class="n">cross_ref</span> <span class="o">=</span> <span class="n">CrossRefFakeMode</span><span class="p">()</span> <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">debug_fake_cross_ref</span> <span class="k">else</span> <span class="n">nullcontext</span><span class="p">()</span>
    <span class="n">python_dispatcher_mode</span> <span class="o">=</span> <span class="n">enable_python_dispatcher</span><span class="p">()</span> <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">use_dynamic_shapes</span> <span class="k">else</span> <span class="n">nullcontext</span><span class="p">()</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">set_multithreading_enabled</span><span class="p">(</span><span class="kc">False</span><span class="p">),</span> <span class="n">preserve_rng_state</span><span class="p">(),</span> <span class="n">cross_ref</span><span class="p">,</span> <span class="n">fake_mode</span><span class="p">,</span> <span class="n">python_dispatcher_mode</span><span class="p">:</span>

        <span class="k">def</span> <span class="nf">process_inputs</span><span class="p">(</span><span class="n">flat_args</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">use_fake_tensor</span><span class="p">:</span>
                <span class="k">def</span> <span class="nf">convert</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                        <span class="k">return</span> <span class="n">x</span>
                    <span class="k">if</span> <span class="n">idx</span> <span class="o">&lt;</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">num_params_buffers</span> <span class="ow">and</span> <span class="n">config</span><span class="o">.</span><span class="n">static_weight_shapes</span><span class="p">:</span>
                        <span class="k">return</span> <span class="n">fake_mode</span><span class="o">.</span><span class="n">from_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">static_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                    <span class="k">return</span> <span class="n">fake_mode</span><span class="o">.</span><span class="n">from_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">static_shapes</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

                <span class="k">return</span> <span class="p">[</span><span class="n">convert</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">flat_args</span>

        <span class="n">fake_flat_tensor_args</span> <span class="o">=</span> <span class="n">process_inputs</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)</span>

        <span class="n">needs_autograd</span> <span class="o">=</span> <span class="p">(</span>
            <span class="nb">any</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span>
                    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">fake_flat_tensor_args</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
                <span class="p">]</span>
            <span class="p">)</span>
            <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_grad_enabled</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="c1"># crappy version of dispatcher</span>
        <span class="c1"># TODO: Do this properly</span>
        <span class="k">if</span> <span class="n">needs_autograd</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">make_boxed_func</span><span class="p">(</span>
                <span class="n">aot_dispatch_autograd</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">,</span> <span class="n">fake_flat_tensor_args</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">aot_dispatch_base</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">,</span> <span class="n">fake_flat_tensor_args</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">)</span>


<span class="c1"># Inspired by autodidax (thanks!)</span>
<span class="k">class</span> <span class="nc">PytreeThunk</span><span class="p">:</span>
    <span class="n">spec</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># These are some kinda dumb microoptimizations that save about 3-4 us of overhead.</span>
    <span class="n">is_simple</span> <span class="o">=</span> <span class="p">(</span>
        <span class="kc">None</span>  <span class="c1"># if the output spec is a tuple/list, we won&#39;t bother unflattening it.</span>
    <span class="p">)</span>
    <span class="n">is_really_simple</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># if the output spec is a LeafSpec</span>

    <span class="k">def</span> <span class="nf">set</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spec</span><span class="p">):</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">spec</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">spec</span> <span class="o">==</span> <span class="n">spec</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">spec</span> <span class="o">=</span> <span class="n">spec</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">spec</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">]</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">pytree</span><span class="o">.</span><span class="n">LeafSpec</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">spec</span><span class="o">.</span><span class="n">children_specs</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_simple</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">spec</span><span class="p">,</span> <span class="n">pytree</span><span class="o">.</span><span class="n">LeafSpec</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_really_simple</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">unflatten</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_really_simple</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_simple</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x</span>
        <span class="k">return</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_unflatten</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">spec</span><span class="p">)</span>

<span class="n">KNOWN_TYPES</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">SymInt</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">SymFloat</span><span class="p">]</span>


<div class="viewcode-block" id="aot_function"><a class="viewcode-back" href="../../../generated/functorch.compile.aot_function.html#functorch.compile.aot_function">[docs]</a><span class="k">def</span> <span class="nf">aot_function</span><span class="p">(</span>
    <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">fw_compiler</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">bw_compiler</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">partition_fn</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">default_partition</span><span class="p">,</span>
    <span class="n">decompositions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_params_buffers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">hasher_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># deprecated</span>
    <span class="n">static_argnums</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># deprecated</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Traces the forward and backward graph of :attr:`fn` using torch dispatch</span>
<span class="sd">    mechanism, and then compiles the generated forward and backward graphs</span>
<span class="sd">    through :attr:`fw_compiler` and :attr:`bw_compiler`.</span>

<span class="sd">    :func:`aot_function` traces the forward and backward graph ahead of time,</span>
<span class="sd">    and generates a joint forward and backward graph.  :attr:`partition_fn` is</span>
<span class="sd">    then used to separate out forward and backward graphs. The partitioner</span>
<span class="sd">    function can be used to perform optimizations such as recomputation. One can</span>
<span class="sd">    set `decompositions` dictionary to decompose the operators into a sequence</span>
<span class="sd">    of core or simpler operators supported by the backend compilers.</span>

<span class="sd">    :func:`aot_function` uses a compilation cache, based on input tensor</span>
<span class="sd">    properties, to detect when there is a need of recompilation.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This API is experimental and likely to change.</span>

<span class="sd">    Args:</span>
<span class="sd">        fn (Callable): A Python function that takes one ore more arguments. Must</span>
<span class="sd">            return one or more Tensors.</span>
<span class="sd">        fw_compiler (Callable): A Python function that accepts an Fx graph with</span>
<span class="sd">            Aten ops and input args, and returns a Callable that semantically is</span>
<span class="sd">            equivalent to the input Fx graph.</span>
<span class="sd">        bw_compiler (Optional[Callable]): A Python function that accepts an</span>
<span class="sd">            Fx graph with Aten ops and input args, and returns a Callable that</span>
<span class="sd">            semantically is equivalent to the input Fx graph.  Default: None</span>
<span class="sd">            (when None, it defaults to the :attr:`fw_compiler`)</span>
<span class="sd">        partition_fn (Callable): A Python function that takes a joint forward</span>
<span class="sd">            and backward graph, and partitions it into separate forward and</span>
<span class="sd">            backward graphs.</span>
<span class="sd">        decompositions (Dict): A dictionary to define the decomposition of</span>
<span class="sd">            larger Aten ops into simpler or core Aten ops.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Returns a ``Callable`` that retains the eager behavior of the original</span>
<span class="sd">        :attr:`fn`, but with forward and backward graph compiled via</span>
<span class="sd">        :attr:`fw_compile` and :attr:`bw_compile`.</span>

<span class="sd">    A simple example usage of :func:`aot_function` is as follows. This example</span>
<span class="sd">    will print the forward and backward graphs of the function ``fn``</span>

<span class="sd">        &gt;&gt;&gt; fn = lambda x : x.sin().cos()</span>
<span class="sd">        &gt;&gt;&gt; def print_compile_fn(fx_module, args):</span>
<span class="sd">        &gt;&gt;&gt;     print(fx_module)</span>
<span class="sd">        &gt;&gt;&gt;     return fx_module</span>
<span class="sd">        &gt;&gt;&gt; aot_fn = aot_function(fn, print_compile_fn)</span>
<span class="sd">        &gt;&gt;&gt; x = torch.randn(4, 5, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; aot_fn(x)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">static_argnums</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;static_argnums has been deprecated - manually wrap your function or use torchdynamo.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">bw_compiler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">bw_compiler</span> <span class="o">=</span> <span class="n">fw_compiler</span>
    <span class="n">aot_config</span> <span class="o">=</span> <span class="n">AOTConfig</span><span class="p">(</span>
        <span class="n">fw_compiler</span><span class="o">=</span><span class="n">fw_compiler</span><span class="p">,</span>
        <span class="n">bw_compiler</span><span class="o">=</span><span class="n">bw_compiler</span><span class="p">,</span>
        <span class="n">partition_fn</span><span class="o">=</span><span class="n">partition_fn</span><span class="p">,</span>
        <span class="n">decompositions</span><span class="o">=</span><span class="n">decompositions</span><span class="p">,</span>
        <span class="n">num_params_buffers</span><span class="o">=</span><span class="n">num_params_buffers</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">cached_res</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="nd">@wraps</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">returned_function</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">nonlocal</span> <span class="n">cached_res</span>
        <span class="c1"># Now flatten the tensor args</span>
        <span class="n">flat_args</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">((</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>

        <span class="c1"># Compile the function and save it in the cache</span>
        <span class="k">if</span> <span class="n">cached_res</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Save the args_spec for flat_tensor_args to unflatten while tracing</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">tensor_args_spec</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">((</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>
            <span class="n">out_spec</span> <span class="o">=</span> <span class="n">PytreeThunk</span><span class="p">()</span>

            <span class="k">def</span> <span class="nf">flat_fn</span><span class="p">(</span><span class="o">*</span><span class="n">flat_args</span><span class="p">):</span>
                <span class="c1"># The input are flattened tensor args. Prepare the args in the</span>
                <span class="c1"># order that original function expects. Add static args as well.</span>
                <span class="c1"># They will appear as tensor constants in the traced graph.</span>
                <span class="k">nonlocal</span> <span class="n">out_spec</span>
                <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_unflatten</span><span class="p">(</span>
                    <span class="n">flat_args</span><span class="p">,</span> <span class="n">tensor_args_spec</span>
                <span class="p">)</span>
                <span class="n">tree_out</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
                <span class="n">flat_out</span><span class="p">,</span> <span class="n">spec</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">(</span><span class="n">tree_out</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">flat_out</span><span class="p">:</span>
                    <span class="n">is_known_type</span> <span class="o">=</span> <span class="kc">False</span>
                    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">KNOWN_TYPES</span><span class="p">:</span>
                        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">):</span>
                            <span class="n">is_known_type</span> <span class="o">=</span> <span class="kc">True</span>
                            <span class="k">break</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_known_type</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;Found </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="si">}</span><span class="s2"> in output, which is not a known type. &quot;</span>
                            <span class="s2">&quot;If this type holds tensors, you need to register a pytree for it. &quot;</span>
                            <span class="s2">&quot;See https://github.com/pytorch/functorch/issues/475 for a brief &quot;</span>
                            <span class="s2">&quot;explanation why. If you don&#39;t need to register a pytree, please &quot;</span>
                            <span class="s2">&quot;leave a comment explaining your use case and we&#39;ll make this more &quot;</span>
                            <span class="s2">&quot;ergonomic to deal with&quot;</span>
                        <span class="p">)</span>
                <span class="n">out_spec</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">spec</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">flat_out</span>

            <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">create_aot_dispatcher_function</span><span class="p">(</span>
                <span class="n">flat_fn</span><span class="p">,</span>
                <span class="n">flat_args</span><span class="p">,</span>
                <span class="n">aot_config</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">cached_res</span> <span class="o">=</span> <span class="p">(</span><span class="n">compiled_fn</span><span class="p">,</span> <span class="n">out_spec</span><span class="p">)</span>

        <span class="n">cached_fn</span><span class="p">,</span> <span class="n">out_spec</span> <span class="o">=</span> <span class="n">cached_res</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">cached_fn</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out_spec</span><span class="o">.</span><span class="n">unflatten</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">returned_function</span></div>


<div class="viewcode-block" id="aot_module"><a class="viewcode-back" href="../../../generated/functorch.compile.aot_module.html#functorch.compile.aot_module">[docs]</a><span class="k">def</span> <span class="nf">aot_module</span><span class="p">(</span><span class="n">mod</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Traces the forward and backward graph of :attr:`mod` using torch dispatch</span>
<span class="sd">    tracing mechanism. It is wrapper function, that underneath uses</span>
<span class="sd">    :func:`aot_function` to perform tracing and compilation.</span>

<span class="sd">    :func:`aot_module` lifts the parameters and buffers of ``nn.Module`` as inputs</span>
<span class="sd">    to a new callable which is then compiled through :func:`aot_function`.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This API is experimental and likely to change.</span>

<span class="sd">    Args:</span>
<span class="sd">        mod (Callable): A ``nn.Module`` module.</span>
<span class="sd">        args : args to be passed to :func:`aot_function`</span>
<span class="sd">        kwargs : kwargs to be passed to :func:`aot_function`</span>

<span class="sd">    Returns:</span>
<span class="sd">        Returns a ``nn.Module`` that retains the eager behavior of the original</span>
<span class="sd">        :attr:`mod`, but with forward and backward graph compiled.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">functional_call</span><span class="p">(</span><span class="n">named_params</span><span class="p">,</span> <span class="n">named_buffers</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">params_and_buffers</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="n">named_params</span><span class="p">,</span> <span class="o">**</span><span class="n">named_buffers</span><span class="p">}</span>
        <span class="k">return</span> <span class="n">stateless</span><span class="o">.</span><span class="n">functional_call</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">params_and_buffers</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>

    <span class="n">named_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">_named_parameters</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">remove_duplicate</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
    <span class="n">named_buffers</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">_named_buffers</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">remove_duplicate</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
    <span class="n">num_params_buffers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">named_params</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">named_buffers</span><span class="p">)</span>
    <span class="n">compiled_f</span> <span class="o">=</span> <span class="n">aot_function</span><span class="p">(</span><span class="n">functional_call</span><span class="p">,</span> <span class="n">num_params_buffers</span><span class="o">=</span><span class="n">num_params_buffers</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">class</span> <span class="nc">AOTModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="nb">super</span><span class="p">(</span><span class="n">AOTModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">orig_module</span> <span class="o">=</span> <span class="n">mod</span>

        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">compiled_f</span><span class="p">(</span>
                <span class="n">named_params</span><span class="p">,</span>
                <span class="n">named_buffers</span><span class="p">,</span>
                <span class="o">*</span><span class="n">args</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">return</span> <span class="n">AOTModule</span><span class="p">()</span></div>


<span class="k">def</span> <span class="nf">aot_module_simplified</span><span class="p">(</span><span class="n">mod</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="o">*</span><span class="n">top_args</span><span class="p">,</span> <span class="o">**</span><span class="n">top_kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This is the simplified or low overhead version of aot_module. For frontends</span>
<span class="sd">    like TorchDynamo, the input functions/modules to AOT are static and have</span>
<span class="sd">    unpacked inputs/outputs. This gives us an opportunity to remove the</span>
<span class="sd">        (1) pytree overhead to parse inputs/outputs,</span>
<span class="sd">        (2) AOT Autograd cache,</span>
<span class="sd">        (3) Reading of params/buffers in every forward call</span>

<span class="sd">    :func:`aot_module_simplified` removes these overheads.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1">#########################################################</span>

    <span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
        <span class="o">**</span><span class="nb">dict</span><span class="p">(</span><span class="n">_named_parameters</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">remove_duplicate</span><span class="o">=</span><span class="kc">False</span><span class="p">)),</span>
        <span class="o">**</span><span class="nb">dict</span><span class="p">(</span><span class="n">_named_buffers</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">remove_duplicate</span><span class="o">=</span><span class="kc">False</span><span class="p">)),</span>
    <span class="p">}</span>
    <span class="n">params_flat</span><span class="p">,</span> <span class="n">params_spec</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">params_flat</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">params_flat</span><span class="p">)</span>
    <span class="n">params_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">params_flat</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">functional_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">stateless</span><span class="o">.</span><span class="n">_reparametrize_module</span><span class="p">(</span>
            <span class="n">mod</span><span class="p">,</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_unflatten</span><span class="p">(</span><span class="n">args</span><span class="p">[:</span><span class="n">params_len</span><span class="p">],</span> <span class="n">params_spec</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">):</span>
                <span class="k">with</span> <span class="n">fx_traceback</span><span class="o">.</span><span class="n">override_stack_trace</span><span class="p">(),</span> <span class="n">warnings</span><span class="o">.</span><span class="n">catch_warnings</span><span class="p">():</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span>
                        <span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="s2">&quot;Anomaly Detection has been enabled.&quot;</span>
                    <span class="p">)</span>
                    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">detect_anomaly</span><span class="p">(</span><span class="n">check_nan</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
                        <span class="n">out</span> <span class="o">=</span> <span class="n">Interpreter</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">[</span><span class="n">params_len</span><span class="p">:],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">out</span> <span class="o">=</span> <span class="n">mod</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">[</span><span class="n">params_len</span><span class="p">:],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Graph output must be a tuple(). This is so that we can avoid &quot;</span>
                <span class="s2">&quot;pytree processing of the ouputs. Please change the module to &quot;</span>
                <span class="s2">&quot;have tuple outputs or use aot_module instead.&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">aot_function_simplified</span><span class="p">(</span>
        <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">fw_compiler</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">bw_compiler</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">partition_fn</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">default_partition</span><span class="p">,</span>
        <span class="n">decompositions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">hasher_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">static_argnums</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">static_argnums</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">bw_compiler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">bw_compiler</span> <span class="o">=</span> <span class="n">fw_compiler</span>
        <span class="n">aot_config</span> <span class="o">=</span> <span class="n">AOTConfig</span><span class="p">(</span>
            <span class="n">fw_compiler</span><span class="o">=</span><span class="n">fw_compiler</span><span class="p">,</span>
            <span class="n">bw_compiler</span><span class="o">=</span><span class="n">bw_compiler</span><span class="p">,</span>
            <span class="n">partition_fn</span><span class="o">=</span><span class="n">partition_fn</span><span class="p">,</span>
            <span class="n">decompositions</span><span class="o">=</span><span class="n">decompositions</span><span class="p">,</span>
            <span class="n">num_params_buffers</span><span class="o">=</span><span class="n">params_len</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">compiled_fn</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="nd">@wraps</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">new_func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
            <span class="k">nonlocal</span> <span class="n">compiled_fn</span>
            <span class="k">if</span> <span class="n">compiled_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">create_aot_dispatcher_function</span><span class="p">(</span>
                    <span class="n">fn</span><span class="p">,</span>
                    <span class="n">args</span><span class="p">,</span>
                    <span class="n">aot_config</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="n">compiled_fn</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">new_func</span>

    <span class="n">compiled_f</span> <span class="o">=</span> <span class="n">aot_function_simplified</span><span class="p">(</span><span class="n">functional_call</span><span class="p">,</span> <span class="o">*</span><span class="n">top_args</span><span class="p">,</span> <span class="o">**</span><span class="n">top_kwargs</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">top_kwargs</span><span class="p">:</span>

        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">compiled_f</span><span class="p">(</span>
                <span class="o">*</span><span class="n">params_flat</span><span class="p">,</span>
                <span class="o">*</span><span class="n">args</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>

        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">compiled_f</span><span class="p">(</span>
                <span class="o">*</span><span class="n">params_flat</span><span class="p">,</span>
                <span class="o">*</span><span class="n">args</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="n">forward</span><span class="o">.</span><span class="n">zero_grad</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">zero_grad</span>
    <span class="n">forward</span><span class="o">.</span><span class="n">named_parameters</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">named_parameters</span>
    <span class="k">return</span> <span class="n">forward</span>


<span class="n">compiled_function</span> <span class="o">=</span> <span class="n">aot_function</span>
<span class="n">compiled_module</span> <span class="o">=</span> <span class="n">aot_module</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/clipboard.min.js"></script>
         <script src="../../../_static/copybutton.js"></script>
         <script >let toggleHintShow = 'Click to show';</script>
         <script >let toggleHintHide = 'Click to hide';</script>
         <script >let toggleOpenOnPrint = 'true';</script>
         <script src="../../../_static/togglebutton.js"></script>
         <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>