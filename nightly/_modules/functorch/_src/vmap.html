


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>functorch._src.vmap &mdash; functorch nightly documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/togglebutton.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/mystnb.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  <!-- Google Analytics -->
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117752657-2');
    </script>
  
  <!-- End Google Analytics -->
  

  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/functorch/versions.html'>nightly (1.14.0a0+gitb3b9786) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          
            
            
              
            
            
              <p class="caption"><span class="caption-text">functorch: Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../install.html">Install functorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/whirlwind_tour.html">Whirlwind Tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ux_limitations.html">UX Limitations</a></li>
</ul>
<p class="caption"><span class="caption-text">functorch API Reference and Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../functorch.html">functorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../experimental.html">functorch.experimental</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../aot_autograd.html">functorch.compile (experimental)</a></li>
</ul>
<p class="caption"><span class="caption-text">functorch Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/jacobians_hessians.html">Jacobians, Hessians, hvp, vhp, and more: composing functorch transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/ensembling.html">Model ensembling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/per_sample_grads.html">Per-sample-gradients</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/neural_tangent_kernels.html">Neural Tangent Kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/aot_autograd_optimizations.html">AOT Autograd - How to use and optimize?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/minifier.html">Using the Minifier</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
      <li>functorch._src.vmap</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for functorch._src.vmap</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Facebook, Inc. and its affiliates.</span>
<span class="c1"># All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># This source code is licensed under the BSD-style license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span><span class="p">,</span> <span class="n">List</span>
<span class="kn">from</span> <span class="nn">torch.utils._pytree</span> <span class="kn">import</span> <span class="n">tree_flatten</span><span class="p">,</span> <span class="n">tree_unflatten</span><span class="p">,</span> <span class="n">_broadcast_to_and_flatten</span><span class="p">,</span> <span class="n">TreeSpec</span>
<span class="kn">from</span> <span class="nn">.pytree_hacks</span> <span class="kn">import</span> <span class="n">tree_map_</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<span class="kn">from</span> <span class="nn">torch._C._functorch</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_add_batch_dim</span><span class="p">,</span>
    <span class="n">_remove_batch_dim</span><span class="p">,</span>
    <span class="n">_vmap_decrement_nesting</span><span class="p">,</span>
    <span class="n">_vmap_increment_nesting</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">in_dims_t</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">]</span>
<span class="n">out_dims_t</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span>


<span class="k">def</span> <span class="nf">doesnt_support_saved_tensors_hooks</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
    <span class="n">message</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;functorch transforms don&#39;t yet support saved tensor hooks. &quot;</span>
        <span class="s2">&quot;Please open an issue with your use case.&quot;</span>
    <span class="p">)</span>

    <span class="nd">@functools</span><span class="o">.</span><span class="n">wraps</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">disable_saved_tensors_hooks</span><span class="p">(</span><span class="n">message</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fn</span>


<span class="c1"># Checks that all args-to-be-batched have the same batch dim size</span>
<span class="k">def</span> <span class="nf">_validate_and_get_batch_size</span><span class="p">(</span>
        <span class="n">flat_in_dims</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
        <span class="n">flat_args</span><span class="p">:</span> <span class="n">List</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="n">batch_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="n">arg</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">in_dim</span><span class="p">)</span> <span class="k">for</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">arg</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">flat_in_dims</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">)</span>
                   <span class="k">if</span> <span class="n">in_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_sizes</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;vmap: Expected at least one Tensor to vmap over&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">batch_sizes</span> <span class="ow">and</span> <span class="nb">any</span><span class="p">(</span><span class="n">size</span> <span class="o">!=</span> <span class="n">batch_sizes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">size</span> <span class="ow">in</span> <span class="n">batch_sizes</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s1">&#39;vmap: Expected all tensors to have the same size in the mapped &#39;</span>
            <span class="sa">f</span><span class="s1">&#39;dimension, got sizes </span><span class="si">{</span><span class="n">batch_sizes</span><span class="si">}</span><span class="s1"> for the mapped dimension&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">batch_sizes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">_num_outputs</span><span class="p">(</span><span class="n">batched_outputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batched_outputs</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">batched_outputs</span><span class="p">)</span>
    <span class="k">return</span> <span class="mi">1</span>

<span class="c1"># If value is a tuple, check it has length `num_elements`.</span>
<span class="c1"># If value is not a tuple, make a tuple with `value` repeated `num_elements` times</span>


<span class="k">def</span> <span class="nf">_as_tuple</span><span class="p">(</span><span class="n">value</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">num_elements</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">error_message_lambda</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[],</span> <span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">value</span><span class="p">,)</span> <span class="o">*</span> <span class="n">num_elements</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="o">!=</span> <span class="n">num_elements</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message_lambda</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">value</span>


<span class="k">def</span> <span class="nf">_process_batched_inputs</span><span class="p">(</span>
    <span class="n">in_dims</span><span class="p">:</span> <span class="n">in_dims_t</span><span class="p">,</span> <span class="n">args</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">func</span><span class="p">:</span> <span class="n">Callable</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">TreeSpec</span><span class="p">]:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">in_dims</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">in_dims</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s1">&#39;vmap(</span><span class="si">{</span><span class="n">_get_name</span><span class="p">(</span><span class="n">func</span><span class="p">)</span><span class="si">}</span><span class="s1">, in_dims=</span><span class="si">{</span><span class="n">in_dims</span><span class="si">}</span><span class="s1">, ...)(&lt;inputs&gt;): &#39;</span>
            <span class="sa">f</span><span class="s1">&#39;expected `in_dims` to be int or a (potentially nested) tuple &#39;</span>
            <span class="sa">f</span><span class="s1">&#39;matching the structure of inputs, got: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">in_dims</span><span class="p">)</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s1">&#39;vmap(</span><span class="si">{</span><span class="n">_get_name</span><span class="p">(</span><span class="n">func</span><span class="p">)</span><span class="si">}</span><span class="s1">)(&lt;inputs&gt;): got no inputs. Maybe you forgot to add &#39;</span>
            <span class="sa">f</span><span class="s1">&#39;inputs, or you are trying to vmap over a function with no inputs. &#39;</span>
            <span class="sa">f</span><span class="s1">&#39;The latter is unsupported.&#39;</span><span class="p">)</span>

    <span class="n">flat_args</span><span class="p">,</span> <span class="n">args_spec</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
    <span class="n">flat_in_dims</span> <span class="o">=</span> <span class="n">_broadcast_to_and_flatten</span><span class="p">(</span><span class="n">in_dims</span><span class="p">,</span> <span class="n">args_spec</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">flat_in_dims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s1">&#39;vmap(</span><span class="si">{</span><span class="n">_get_name</span><span class="p">(</span><span class="n">func</span><span class="p">)</span><span class="si">}</span><span class="s1">, in_dims=</span><span class="si">{</span><span class="n">in_dims</span><span class="si">}</span><span class="s1">, ...)(&lt;inputs&gt;): &#39;</span>
            <span class="sa">f</span><span class="s1">&#39;in_dims is not compatible with the structure of `inputs`. &#39;</span>
            <span class="sa">f</span><span class="s1">&#39;in_dims has structure </span><span class="si">{</span><span class="n">tree_flatten</span><span class="p">(</span><span class="n">in_dims</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s1"> but inputs &#39;</span>
            <span class="sa">f</span><span class="s1">&#39;has structure </span><span class="si">{</span><span class="n">args_spec</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">flat_args</span><span class="p">,</span> <span class="n">flat_in_dims</span><span class="p">)):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">in_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s1">&#39;vmap(</span><span class="si">{</span><span class="n">_get_name</span><span class="p">(</span><span class="n">func</span><span class="p">)</span><span class="si">}</span><span class="s1">, in_dims=</span><span class="si">{</span><span class="n">in_dims</span><span class="si">}</span><span class="s1">, ...)(&lt;inputs&gt;): &#39;</span>
                <span class="sa">f</span><span class="s1">&#39;Got in_dim=</span><span class="si">{</span><span class="n">in_dim</span><span class="si">}</span><span class="s1"> for an input but in_dim must be either &#39;</span>
                <span class="sa">f</span><span class="s1">&#39;an integer dimension or None.&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s1">&#39;vmap(</span><span class="si">{</span><span class="n">_get_name</span><span class="p">(</span><span class="n">func</span><span class="p">)</span><span class="si">}</span><span class="s1">, in_dims=</span><span class="si">{</span><span class="n">in_dims</span><span class="si">}</span><span class="s1">, ...)(&lt;inputs&gt;): &#39;</span>
                <span class="sa">f</span><span class="s1">&#39;Got in_dim=</span><span class="si">{</span><span class="n">in_dim</span><span class="si">}</span><span class="s1"> for an input but the input is of type &#39;</span>
                <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span><span class="si">}</span><span class="s1">. We cannot vmap over non-Tensor arguments, &#39;</span>
                <span class="sa">f</span><span class="s1">&#39;please use None as the respective in_dim&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">in_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">in_dim</span> <span class="o">&lt;</span> <span class="o">-</span><span class="n">arg</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="ow">or</span> <span class="n">in_dim</span> <span class="o">&gt;=</span> <span class="n">arg</span><span class="o">.</span><span class="n">dim</span><span class="p">()):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s1">&#39;vmap(</span><span class="si">{</span><span class="n">_get_name</span><span class="p">(</span><span class="n">func</span><span class="p">)</span><span class="si">}</span><span class="s1">, in_dims=</span><span class="si">{</span><span class="n">in_dims</span><span class="si">}</span><span class="s1">, ...)(&lt;inputs&gt;): &#39;</span>
                <span class="sa">f</span><span class="s1">&#39;Got in_dim=</span><span class="si">{</span><span class="n">in_dim</span><span class="si">}</span><span class="s1"> for some input, but that input is a Tensor &#39;</span>
                <span class="sa">f</span><span class="s1">&#39;of dimensionality </span><span class="si">{</span><span class="n">arg</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s1"> so expected in_dim to satisfy &#39;</span>
                <span class="sa">f</span><span class="s1">&#39;-</span><span class="si">{</span><span class="n">arg</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s1"> &lt;= in_dim &lt; </span><span class="si">{</span><span class="n">arg</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">in_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">in_dim</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">flat_in_dims</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">in_dim</span> <span class="o">%</span> <span class="n">arg</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">_validate_and_get_batch_size</span><span class="p">(</span><span class="n">flat_in_dims</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">),</span> <span class="n">flat_in_dims</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">,</span> <span class="n">args_spec</span>

<span class="c1"># Creates BatchedTensors for every Tensor in arg that should be batched.</span>
<span class="c1"># Returns the (potentially) batched arguments and the batch_size.</span>


<span class="k">def</span> <span class="nf">_create_batched_inputs</span><span class="p">(</span>
        <span class="n">flat_in_dims</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">flat_args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">vmap_level</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">args_spec</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">:</span>
    <span class="c1"># See NOTE [Ignored _remove_batch_dim, _add_batch_dim]</span>
    <span class="n">batched_inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">arg</span> <span class="k">if</span> <span class="n">in_dim</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span>
                      <span class="n">_add_batch_dim</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">vmap_level</span><span class="p">)</span>
                      <span class="k">for</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">arg</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">flat_in_dims</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">batched_inputs</span><span class="p">,</span> <span class="n">args_spec</span><span class="p">)</span>

<span class="c1"># Undos the batching (and any batch dimensions) associated with the `vmap_level`.</span>


<span class="k">def</span> <span class="nf">_unwrap_batched</span><span class="p">(</span>
        <span class="n">batched_outputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]],</span>
        <span class="n">out_dims</span><span class="p">:</span> <span class="n">out_dims_t</span><span class="p">,</span>
        <span class="n">vmap_level</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">:</span>
    <span class="n">flat_batched_outputs</span><span class="p">,</span> <span class="n">output_spec</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">(</span><span class="n">batched_outputs</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">flat_batched_outputs</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="k">continue</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;vmap(</span><span class="si">{</span><span class="n">_get_name</span><span class="p">(</span><span class="n">func</span><span class="p">)</span><span class="si">}</span><span class="s1">, ...): `</span><span class="si">{</span><span class="n">_get_name</span><span class="p">(</span><span class="n">func</span><span class="p">)</span><span class="si">}</span><span class="s1">` must only return &#39;</span>
                         <span class="sa">f</span><span class="s1">&#39;Tensors, got type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">out</span><span class="p">)</span><span class="si">}</span><span class="s1"> as a return.&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">incompatible_error</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s1">&#39;vmap(</span><span class="si">{</span><span class="n">_get_name</span><span class="p">(</span><span class="n">func</span><span class="p">)</span><span class="si">}</span><span class="s1">, ..., out_dims=</span><span class="si">{</span><span class="n">out_dims</span><span class="si">}</span><span class="s1">)(&lt;inputs&gt;): &#39;</span>
            <span class="sa">f</span><span class="s1">&#39;out_dims is not compatible with the structure of `outputs`. &#39;</span>
            <span class="sa">f</span><span class="s1">&#39;out_dims has structure </span><span class="si">{</span><span class="n">tree_flatten</span><span class="p">(</span><span class="n">out_dims</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s1"> but outputs &#39;</span>
            <span class="sa">f</span><span class="s1">&#39;has structure </span><span class="si">{</span><span class="n">output_spec</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batched_outputs</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="c1"># Some weird edge case requires us to spell out the following</span>
        <span class="c1"># see test_out_dims_edge_case</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out_dims</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">flat_out_dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">out_dims</span><span class="p">]</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out_dims</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">out_dims</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">flat_out_dims</span> <span class="o">=</span> <span class="n">out_dims</span>
            <span class="n">out_dims</span> <span class="o">=</span> <span class="n">out_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">incompatible_error</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">flat_out_dims</span> <span class="o">=</span> <span class="n">_broadcast_to_and_flatten</span><span class="p">(</span><span class="n">out_dims</span><span class="p">,</span> <span class="n">output_spec</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">flat_out_dims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">incompatible_error</span><span class="p">()</span>

    <span class="n">flat_outputs</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">_remove_batch_dim</span><span class="p">(</span><span class="n">batched_output</span><span class="p">,</span> <span class="n">vmap_level</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">batched_output</span><span class="p">,</span> <span class="n">out_dim</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">flat_batched_outputs</span><span class="p">,</span> <span class="n">flat_out_dims</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="k">return</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">flat_outputs</span><span class="p">,</span> <span class="n">output_spec</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_int</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">out_dims</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">return</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
        <span class="sa">f</span><span class="s1">&#39;vmap(</span><span class="si">{</span><span class="n">_get_name</span><span class="p">(</span><span class="n">func</span><span class="p">)</span><span class="si">}</span><span class="s1">, ..., out_dims=</span><span class="si">{</span><span class="n">out_dims</span><span class="si">}</span><span class="s1">): `out_dims` must be &#39;</span>
        <span class="sa">f</span><span class="s1">&#39;an int or a python collection of ints representing where in the outputs the &#39;</span>
        <span class="sa">f</span><span class="s1">&#39;vmapped dimension should appear.&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_out_dims_is_int_or_int_pytree</span><span class="p">(</span><span class="n">out_dims</span><span class="p">:</span> <span class="n">out_dims_t</span><span class="p">,</span> <span class="n">func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out_dims</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">return</span>
    <span class="n">tree_map_</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">_check_int</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">func</span><span class="p">,</span> <span class="n">out_dims</span><span class="o">=</span><span class="n">out_dims</span><span class="p">),</span> <span class="n">out_dims</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_get_name</span><span class="p">(</span><span class="n">func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="s1">&#39;__name__&#39;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">func</span><span class="o">.</span><span class="vm">__name__</span>

    <span class="c1"># Not all callables have __name__, in fact, only static functions/methods do.</span>
    <span class="c1"># A callable created via functools.partial or an nn.Module, to name some</span>
    <span class="c1"># examples, don&#39;t have a __name__.</span>
    <span class="k">return</span> <span class="nb">repr</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>

<span class="c1"># vmap(func)(inputs) wraps all Tensor inputs to be batched in BatchedTensors,</span>
<span class="c1"># sends those into func, and then unwraps the output BatchedTensors. Operations</span>
<span class="c1"># on BatchedTensors perform the batched operations that the user is asking for.</span>
<span class="c1">#</span>
<span class="c1"># vmap&#39;s randomness behavior differs from JAX&#39;s, which would require a PRNG key</span>
<span class="c1"># to be passed everywhere.</span>


<div class="viewcode-block" id="vmap"><a class="viewcode-back" href="../../../generated/functorch.vmap.html#functorch.vmap">[docs]</a><span class="k">def</span> <span class="nf">vmap</span><span class="p">(</span>
        <span class="n">func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">in_dims</span><span class="p">:</span> <span class="n">in_dims_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">out_dims</span><span class="p">:</span> <span class="n">out_dims_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">randomness</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;error&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    vmap is the vectorizing map; ``vmap(func)`` returns a new function that</span>
<span class="sd">    maps :attr:`func` over some dimension of the inputs. Semantically, vmap</span>
<span class="sd">    pushes the map into PyTorch operations called by :attr:`func`, effectively</span>
<span class="sd">    vectorizing those operations.</span>

<span class="sd">    vmap is useful for handling batch dimensions: one can write a function</span>
<span class="sd">    :attr:`func` that runs on examples and then lift it to a function that can</span>
<span class="sd">    take batches of examples with ``vmap(func)``. vmap can also be used to</span>
<span class="sd">    compute batched gradients when composed with autograd.</span>

<span class="sd">    Args:</span>
<span class="sd">        func (function): A Python function that takes one or more arguments.</span>
<span class="sd">            Must return one or more Tensors.</span>
<span class="sd">        in_dims (int or nested structure): Specifies which dimension of the</span>
<span class="sd">            inputs should be mapped over. :attr:`in_dims` should have a</span>
<span class="sd">            structure like the inputs. If the :attr:`in_dim` for a particular</span>
<span class="sd">            input is None, then that indicates there is no map dimension.</span>
<span class="sd">            Default: 0.</span>
<span class="sd">        out_dims (int or Tuple[int]): Specifies where the mapped dimension</span>
<span class="sd">            should appear in the outputs. If :attr:`out_dims` is a Tuple, then</span>
<span class="sd">            it should have one element per output. Default: 0.</span>
<span class="sd">        randomness (str): Specifies whether the randomness in this</span>
<span class="sd">            vmap should be the same or different across batches. If &#39;different&#39;,</span>
<span class="sd">            the randomness for each batch will be different. If &#39;same&#39;, the</span>
<span class="sd">            randomness will be the same across batches. If &#39;error&#39;, any calls to</span>
<span class="sd">            random functions will error. Default: &#39;error&#39;. WARNING: this flag</span>
<span class="sd">            only applies to random PyTorch operations and does not apply to</span>
<span class="sd">            Python&#39;s random module or numpy randomness.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Returns a new &quot;batched&quot; function. It takes the same inputs as</span>
<span class="sd">        :attr:`func`, except each input has an extra dimension at the index</span>
<span class="sd">        specified by :attr:`in_dims`. It takes returns the same outputs as</span>
<span class="sd">        :attr:`func`, except each output has an extra dimension at the index</span>
<span class="sd">        specified by :attr:`out_dims`.</span>

<span class="sd">    .. warning:</span>
<span class="sd">        :func:`vmap` works best with functional-style code. Please do not</span>
<span class="sd">        perform any side-effects in :attr:`func`, with the exception of</span>
<span class="sd">        in-place PyTorch operations. Examples of side-effects include mutating</span>
<span class="sd">        Python data structures and assigning values to variables not captured</span>
<span class="sd">        in :attr:`func`.</span>

<span class="sd">    One example of using :func:`vmap` is to compute batched dot products. PyTorch</span>
<span class="sd">    doesn&#39;t provide a batched ``torch.dot`` API; instead of unsuccessfully</span>
<span class="sd">    rummaging through docs, use :func:`vmap` to construct a new function.</span>

<span class="sd">        &gt;&gt;&gt; torch.dot                            # [D], [D] -&gt; []</span>
<span class="sd">        &gt;&gt;&gt; batched_dot = functorch.vmap(torch.dot)  # [N, D], [N, D] -&gt; [N]</span>
<span class="sd">        &gt;&gt;&gt; x, y = torch.randn(2, 5), torch.randn(2, 5)</span>
<span class="sd">        &gt;&gt;&gt; batched_dot(x, y)</span>

<span class="sd">    :func:`vmap` can be helpful in hiding batch dimensions, leading to a simpler</span>
<span class="sd">    model authoring experience.</span>

<span class="sd">        &gt;&gt;&gt; batch_size, feature_size = 3, 5</span>
<span class="sd">        &gt;&gt;&gt; weights = torch.randn(feature_size, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; def model(feature_vec):</span>
<span class="sd">        &gt;&gt;&gt;     # Very simple linear model with activation</span>
<span class="sd">        &gt;&gt;&gt;     return feature_vec.dot(weights).relu()</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; examples = torch.randn(batch_size, feature_size)</span>
<span class="sd">        &gt;&gt;&gt; result = functorch.vmap(model)(examples)</span>

<span class="sd">    :func:`vmap` can also help vectorize computations that were previously difficult</span>
<span class="sd">    or impossible to batch. One example is higher-order gradient computation.</span>
<span class="sd">    The PyTorch autograd engine computes vjps (vector-Jacobian products).</span>
<span class="sd">    Computing a full Jacobian matrix for some function f: R^N -&gt; R^N usually</span>
<span class="sd">    requires N calls to ``autograd.grad``, one per Jacobian row. Using :func:`vmap`,</span>
<span class="sd">    we can vectorize the whole computation, computing the Jacobian in a single</span>
<span class="sd">    call to ``autograd.grad``.</span>

<span class="sd">        &gt;&gt;&gt; # Setup</span>
<span class="sd">        &gt;&gt;&gt; N = 5</span>
<span class="sd">        &gt;&gt;&gt; f = lambda x: x ** 2</span>
<span class="sd">        &gt;&gt;&gt; x = torch.randn(N, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; y = f(x)</span>
<span class="sd">        &gt;&gt;&gt; I_N = torch.eye(N)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Sequential approach</span>
<span class="sd">        &gt;&gt;&gt; jacobian_rows = [torch.autograd.grad(y, x, v, retain_graph=True)[0]</span>
<span class="sd">        &gt;&gt;&gt;                  for v in I_N.unbind()]</span>
<span class="sd">        &gt;&gt;&gt; jacobian = torch.stack(jacobian_rows)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # vectorized gradient computation</span>
<span class="sd">        &gt;&gt;&gt; def get_vjp(v):</span>
<span class="sd">        &gt;&gt;&gt;     return torch.autograd.grad(y, x, v)</span>
<span class="sd">        &gt;&gt;&gt; jacobian = functorch.vmap(get_vjp)(I_N)</span>

<span class="sd">    :func:`vmap` can also be nested, producing an output with multiple batched dimensions</span>

<span class="sd">        &gt;&gt;&gt; torch.dot                            # [D], [D] -&gt; []</span>
<span class="sd">        &gt;&gt;&gt; batched_dot = functorch.vmap(functorch.vmap(torch.dot))  # [N1, N0, D], [N1, N0, D] -&gt; [N1, N0]</span>
<span class="sd">        &gt;&gt;&gt; x, y = torch.randn(2, 3, 5), torch.randn(2, 3, 5)</span>
<span class="sd">        &gt;&gt;&gt; batched_dot(x, y) # tensor of size [2, 3]</span>

<span class="sd">    If the inputs are not batched along the first dimension, :attr:`in_dims` specifies</span>
<span class="sd">    the dimension that each inputs are batched along as</span>

<span class="sd">        &gt;&gt;&gt; torch.dot                            # [N], [N] -&gt; []</span>
<span class="sd">        &gt;&gt;&gt; batched_dot = functorch.vmap(torch.dot, in_dims=1)  # [N, D], [N, D] -&gt; [D]</span>
<span class="sd">        &gt;&gt;&gt; x, y = torch.randn(2, 5), torch.randn(2, 5)</span>
<span class="sd">        &gt;&gt;&gt; batched_dot(x, y)   # output is [5] instead of [2] if batched along the 0th dimension</span>

<span class="sd">    If there are multiple inputs each of which is batched along different dimensions,</span>
<span class="sd">    :attr:`in_dims` must be a tuple with the batch dimension for each input as</span>

<span class="sd">        &gt;&gt;&gt; torch.dot                            # [D], [D] -&gt; []</span>
<span class="sd">        &gt;&gt;&gt; batched_dot = functorch.vmap(torch.dot, in_dims=(0, None))  # [N, D], [D] -&gt; [N]</span>
<span class="sd">        &gt;&gt;&gt; x, y = torch.randn(2, 5), torch.randn(5)</span>
<span class="sd">        &gt;&gt;&gt; batched_dot(x, y) # second arg doesn&#39;t have a batch dim because in_dim[1] was None</span>

<span class="sd">    If the input is a Python struct, :attr:`in_dims` must be a tuple containing a struct</span>
<span class="sd">    matching the shape of the input:</span>

<span class="sd">        &gt;&gt;&gt; f = lambda dict: torch.dot(dict[&#39;x&#39;], dict[&#39;y&#39;])</span>
<span class="sd">        &gt;&gt;&gt; x, y = torch.randn(2, 5), torch.randn(5)</span>
<span class="sd">        &gt;&gt;&gt; input = {&#39;x&#39;: x, &#39;y&#39;: y}</span>
<span class="sd">        &gt;&gt;&gt; batched_dot = functorch.vmap(f, in_dims=({&#39;x&#39;: 0, &#39;y&#39;: None},))</span>
<span class="sd">        &gt;&gt;&gt; batched_dot(input)</span>

<span class="sd">    By default, the output is batched along the first dimension. However, it can be batched</span>
<span class="sd">    along any dimension by using :attr:`out_dims`</span>

<span class="sd">        &gt;&gt;&gt; f = lambda x: x ** 2</span>
<span class="sd">        &gt;&gt;&gt; x = torch.randn(2, 5)</span>
<span class="sd">        &gt;&gt;&gt; batched_pow = functorch.vmap(f, out_dims=1)</span>
<span class="sd">        &gt;&gt;&gt; batched_pow(x) # [5, 2]</span>

<span class="sd">    For any function that uses kwargs, the returned function will not batch the kwargs but will</span>
<span class="sd">    accept kwargs</span>

<span class="sd">        &gt;&gt;&gt; x = torch.randn([2, 5])</span>
<span class="sd">        &gt;&gt;&gt; def f(x, scale=4.):</span>
<span class="sd">        &gt;&gt;&gt;   return x * scale</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; batched_pow = functorch.vmap(f)</span>
<span class="sd">        &gt;&gt;&gt; assert torch.allclose(batched_pow(x), x * 4)</span>
<span class="sd">        &gt;&gt;&gt; batched_pow(x, scale=x) # scale is not batched, output has shape [2, 2, 5]</span>

<span class="sd">    .. note::</span>
<span class="sd">        vmap does not provide general autobatching or handle variable-length</span>
<span class="sd">        sequences out of the box.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_randomness_arg</span><span class="p">(</span><span class="n">randomness</span><span class="p">)</span>

    <span class="nd">@functools</span><span class="o">.</span><span class="n">wraps</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">wrapped</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">_check_out_dims_is_int_or_int_pytree</span><span class="p">(</span><span class="n">out_dims</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">flat_in_dims</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">,</span> <span class="n">args_spec</span> <span class="o">=</span> <span class="n">_process_batched_inputs</span><span class="p">(</span><span class="n">in_dims</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">_flat_vmap</span><span class="p">(</span>
            <span class="n">func</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">flat_in_dims</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">,</span> <span class="n">args_spec</span><span class="p">,</span> <span class="n">out_dims</span><span class="p">,</span> <span class="n">randomness</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">wrapped</span></div>


<span class="k">def</span> <span class="nf">chunk_vmap</span><span class="p">(</span>
        <span class="n">func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">in_dims</span><span class="p">:</span> <span class="n">in_dims_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">out_dims</span><span class="p">:</span> <span class="n">out_dims_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">randomness</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;error&#39;</span><span class="p">,</span>
        <span class="n">chunks</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    chunk_vmap is the vectorizing map (vmap) using chunks of input data. It is a mix of vmap (which vectorizes</span>
<span class="sd">    everything) and map (which executes things sequentially). ``chunk_vmap`` vectorizes the input with number of</span>
<span class="sd">    chunks at a time. For more details about vectorizing map, see :func:`vmap`.</span>

<span class="sd">    Args:</span>
<span class="sd">        func (function): A Python function that takes one or more arguments.</span>
<span class="sd">            Must return one or more Tensors.</span>
<span class="sd">        in_dims (int or nested structure): Specifies which dimension of the</span>
<span class="sd">            inputs should be mapped over. :attr:`in_dims` should have a</span>
<span class="sd">            structure like the inputs. If the :attr:`in_dim` for a particular</span>
<span class="sd">            input is None, then that indicates there is no map dimension.</span>
<span class="sd">            Default: 0.</span>
<span class="sd">        out_dims (int or Tuple[int]): Specifies where the mapped dimension</span>
<span class="sd">            should appear in the outputs. If :attr:`out_dims` is a Tuple, then</span>
<span class="sd">            it should have one element per output. Default: 0.</span>
<span class="sd">        randomness (str): Specifies whether the randomness in this</span>
<span class="sd">            vmap should be the same or different across batches. If &#39;different&#39;,</span>
<span class="sd">            the randomness for each batch will be different. If &#39;same&#39;, the</span>
<span class="sd">            randomness will be the same across batches. If &#39;error&#39;, any calls to</span>
<span class="sd">            random functions will error. Default: &#39;error&#39;. WARNING: this flag</span>
<span class="sd">            only applies to random PyTorch operations and does not apply to</span>
<span class="sd">            Python&#39;s random module or numpy randomness.</span>
<span class="sd">        chunks (int): Number of chunks to use to split the input data. Default is 2.</span>
<span class="sd">            If equals to 1 then :func:`vmap` is called.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Returns a new &quot;batched&quot; function. It takes the same inputs as</span>
<span class="sd">        :attr:`func`, except each input has an extra dimension at the index</span>
<span class="sd">        specified by :attr:`in_dims`. It takes returns the same outputs as</span>
<span class="sd">        :attr:`func`, except each output has an extra dimension at the index</span>
<span class="sd">        specified by :attr:`out_dims`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_randomness_arg</span><span class="p">(</span><span class="n">randomness</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">chunks</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">vmap</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">in_dims</span><span class="o">=</span><span class="n">in_dims</span><span class="p">,</span> <span class="n">out_dims</span><span class="o">=</span><span class="n">out_dims</span><span class="p">,</span> <span class="n">randomness</span><span class="o">=</span><span class="n">randomness</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_chunk_flat_args</span><span class="p">(</span><span class="n">flat_args_</span><span class="p">,</span> <span class="n">flat_in_dims_</span><span class="p">,</span> <span class="n">chunks_</span><span class="p">):</span>
        <span class="n">flat_args_chunks</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
            <span class="n">t</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="n">chunks_</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">in_dim</span><span class="p">)</span> <span class="k">if</span> <span class="n">in_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">[</span><span class="n">t</span><span class="p">,</span> <span class="p">]</span> <span class="o">*</span> <span class="n">chunks_</span>
            <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">in_dim</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">flat_args_</span><span class="p">,</span> <span class="n">flat_in_dims_</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="c1"># transpose chunk dim and flatten structure</span>
        <span class="c1"># chunks_flat_args is a list of flatten args</span>
        <span class="n">chunks_flat_args</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">flat_args_chunks</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">chunks_flat_args</span>

    <span class="k">def</span> <span class="nf">_flatten_chunks_output</span><span class="p">(</span><span class="n">chunks_output_</span><span class="p">):</span>
        <span class="c1"># chunks_output is a list of chunked outputs</span>
        <span class="c1"># flatten chunked outputs:</span>
        <span class="n">flat_chunks_output</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">arg_spec_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">chunks_output_</span><span class="p">:</span>
            <span class="n">flat_output</span><span class="p">,</span> <span class="n">arg_specs</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
            <span class="n">flat_chunks_output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">flat_output</span><span class="p">)</span>
            <span class="n">arg_spec_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">arg_specs</span><span class="p">)</span>

        <span class="n">arg_spec</span> <span class="o">=</span> <span class="n">arg_spec_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># all specs should be the same</span>
        <span class="c1"># transpose chunk dim and flatten structure</span>
        <span class="c1"># flat_output_chunks is flat list of chunks</span>
        <span class="n">flat_output_chunks</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">flat_chunks_output</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">flat_output_chunks</span><span class="p">,</span> <span class="n">arg_spec</span>

    <span class="nd">@functools</span><span class="o">.</span><span class="n">wraps</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">wrapped_with_chunks</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">_check_out_dims_is_int_or_int_pytree</span><span class="p">(</span><span class="n">out_dims</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">flat_in_dims</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">,</span> <span class="n">args_spec</span> <span class="o">=</span> <span class="n">_process_batched_inputs</span><span class="p">(</span><span class="n">in_dims</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>
        <span class="c1"># Chunk flat arguments</span>
        <span class="n">chunks_flat_args</span> <span class="o">=</span> <span class="n">_get_chunk_flat_args</span><span class="p">(</span><span class="n">flat_args</span><span class="p">,</span> <span class="n">flat_in_dims</span><span class="p">,</span> <span class="n">chunks</span><span class="p">)</span>

        <span class="c1"># Apply vmap on chunks</span>
        <span class="n">chunks_output</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">rs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">get_rng_state</span><span class="p">()</span> <span class="k">if</span> <span class="n">randomness</span> <span class="o">==</span> <span class="s2">&quot;same&quot;</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="k">for</span> <span class="n">flat_args</span> <span class="ow">in</span> <span class="n">chunks_flat_args</span><span class="p">:</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="n">_validate_and_get_batch_size</span><span class="p">(</span><span class="n">flat_in_dims</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">rs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">set_rng_state</span><span class="p">(</span><span class="n">rs</span><span class="p">)</span>
            <span class="n">chunks_output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">_flat_vmap</span><span class="p">(</span>
                    <span class="n">func</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">flat_in_dims</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">,</span> <span class="n">args_spec</span><span class="p">,</span> <span class="n">out_dims</span><span class="p">,</span> <span class="n">randomness</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="n">flat_output_chunks</span><span class="p">,</span> <span class="n">arg_spec</span> <span class="o">=</span> <span class="n">_flatten_chunks_output</span><span class="p">(</span><span class="n">chunks_output</span><span class="p">)</span>
        <span class="c1"># Removing temporary variables helps to reduce memory usage on device like CUDA</span>
        <span class="k">del</span> <span class="n">chunks_output</span>

        <span class="c1"># concat chunks on out_dim</span>
        <span class="n">flat_out_dims</span> <span class="o">=</span> <span class="n">_broadcast_to_and_flatten</span><span class="p">(</span><span class="n">out_dims</span><span class="p">,</span> <span class="n">arg_spec</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">flat_out_dims</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">flat_output_chunks</span><span class="p">)</span>
        <span class="n">flat_output</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">out_dim</span> <span class="ow">in</span> <span class="n">flat_out_dims</span><span class="p">:</span>
            <span class="n">flat_output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">flat_output_chunks</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="n">out_dim</span><span class="p">))</span>
            <span class="c1"># release source data</span>
            <span class="k">del</span> <span class="n">flat_output_chunks</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">del</span> <span class="n">flat_output_chunks</span>

        <span class="c1"># finally unflatten the output</span>
        <span class="k">return</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">flat_output</span><span class="p">,</span> <span class="n">arg_spec</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">wrapped_with_chunks</span>


<span class="c1"># Vmap refactored helper funcions:</span>
<span class="k">def</span> <span class="nf">_check_randomness_arg</span><span class="p">(</span><span class="n">randomness</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">randomness</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;error&#39;</span><span class="p">,</span> <span class="s1">&#39;different&#39;</span><span class="p">,</span> <span class="s1">&#39;same&#39;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Only allowed values for randomness are &#39;error&#39;, &#39;different&#39;, or &#39;same&#39;. Got </span><span class="si">{</span><span class="n">randomness</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="nd">@doesnt_support_saved_tensors_hooks</span>
<span class="k">def</span> <span class="nf">_flat_vmap</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">flat_in_dims</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">,</span> <span class="n">args_spec</span><span class="p">,</span> <span class="n">out_dims</span><span class="p">,</span> <span class="n">randomness</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">vmap_level</span> <span class="o">=</span> <span class="n">_vmap_increment_nesting</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">randomness</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">batched_inputs</span> <span class="o">=</span> <span class="n">_create_batched_inputs</span><span class="p">(</span><span class="n">flat_in_dims</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">,</span> <span class="n">vmap_level</span><span class="p">,</span> <span class="n">args_spec</span><span class="p">)</span>
        <span class="n">batched_outputs</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">batched_inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">_unwrap_batched</span><span class="p">(</span><span class="n">batched_outputs</span><span class="p">,</span> <span class="n">out_dims</span><span class="p">,</span> <span class="n">vmap_level</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="n">_vmap_decrement_nesting</span><span class="p">()</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/clipboard.min.js"></script>
         <script src="../../../_static/copybutton.js"></script>
         <script >let toggleHintShow = 'Click to show';</script>
         <script >let toggleHintHide = 'Click to hide';</script>
         <script >let toggleOpenOnPrint = 'true';</script>
         <script src="../../../_static/togglebutton.js"></script>
         <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>