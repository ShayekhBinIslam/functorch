


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch._functorch.aot_autograd &mdash; functorch nightly documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/togglebutton.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/mystnb.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  <!-- Google Analytics -->
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117752657-2');
    </script>
  
  <!-- End Google Analytics -->
  

  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/functorch/versions.html'>nightly (2.0.0a0+gitb2c68c1) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          
            
            
              
            
            
              <p class="caption"><span class="caption-text">functorch: Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../install.html">Install functorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/whirlwind_tour.html">Whirlwind Tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ux_limitations.html">UX Limitations</a></li>
</ul>
<p class="caption"><span class="caption-text">functorch API Reference and Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../functorch.html">functorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../experimental.html">functorch.experimental</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../aot_autograd.html">functorch.compile (experimental)</a></li>
</ul>
<p class="caption"><span class="caption-text">functorch Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/jacobians_hessians.html">Jacobians, Hessians, hvp, vhp, and more: composing functorch transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/ensembling.html">Model ensembling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/per_sample_grads.html">Per-sample-gradients</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/neural_tangent_kernels.html">Neural Tangent Kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/aot_autograd_optimizations.html">AOT Autograd - How to use and optimize?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/minifier.html">Using the Minifier</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
      <li>torch._functorch.aot_autograd</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch._functorch.aot_autograd</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">dataclasses</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">contextmanager</span><span class="p">,</span> <span class="n">nullcontext</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">enum</span> <span class="kn">import</span> <span class="n">Enum</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">wraps</span><span class="p">,</span> <span class="n">partial</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>
<span class="kn">from</span> <span class="nn">torch.fx.experimental.proxy_tensor</span> <span class="kn">import</span> <span class="n">is_sym_node</span>
<span class="kn">import</span> <span class="nn">logging</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.fx.traceback</span> <span class="k">as</span> <span class="nn">fx_traceback</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.utils._pytree</span> <span class="k">as</span> <span class="nn">pytree</span>
<span class="kn">import</span> <span class="nn">torch.utils.dlpack</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">torch._dynamo.utils</span> <span class="kn">import</span> <span class="n">dynamo_timed</span>
<span class="kn">from</span> <span class="nn">torch._subclasses</span> <span class="kn">import</span> <span class="n">FakeTensorMode</span><span class="p">,</span> <span class="n">CrossRefFakeMode</span><span class="p">,</span> <span class="n">FakeTensor</span>
<span class="kn">from</span> <span class="nn">torch.fx</span> <span class="kn">import</span> <span class="n">immutable_collections</span><span class="p">,</span> <span class="n">Interpreter</span>
<span class="kn">from</span> <span class="nn">torch.fx.experimental.symbolic_shapes</span> <span class="kn">import</span> <span class="n">ShapeEnv</span>
<span class="kn">from</span> <span class="nn">torch.multiprocessing.reductions</span> <span class="kn">import</span> <span class="n">StorageWeakRef</span>
<span class="kn">from</span> <span class="nn">torch.nn.utils</span> <span class="kn">import</span> <span class="n">stateless</span>

<span class="kn">from</span> <span class="nn">functorch</span> <span class="kn">import</span> <span class="n">make_fx</span>
<span class="kn">from</span> <span class="nn">torch._dispatch.python</span> <span class="kn">import</span> <span class="n">enable_python_dispatcher</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">config</span>
<span class="kn">from</span> <span class="nn">.named_members_polyfill</span> <span class="kn">import</span> <span class="n">_named_buffers</span><span class="p">,</span> <span class="n">_named_parameters</span>
<span class="kn">from</span> <span class="nn">.partitioners</span> <span class="kn">import</span> <span class="n">default_partition</span>
<span class="kn">from</span> <span class="nn">torch._guards</span> <span class="kn">import</span> <span class="n">TracingContext</span><span class="p">,</span> <span class="n">DuplicateInputs</span>

<span class="n">log</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="n">MutationType</span> <span class="o">=</span> <span class="n">Enum</span><span class="p">(</span><span class="s2">&quot;MutationType&quot;</span><span class="p">,</span> <span class="p">(</span><span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="s2">&quot;metadata_only&quot;</span><span class="p">,</span> <span class="s2">&quot;data&quot;</span><span class="p">))</span>
<span class="n">OutputType</span> <span class="o">=</span> <span class="n">Enum</span><span class="p">(</span>
    <span class="s2">&quot;OutputType&quot;</span><span class="p">,</span> <span class="p">(</span><span class="s2">&quot;non_alias&quot;</span><span class="p">,</span> <span class="s2">&quot;alias_of_input&quot;</span><span class="p">,</span> <span class="s2">&quot;alias_of_intermediate&quot;</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">pytree</span><span class="o">.</span><span class="n">_register_pytree_node</span><span class="p">(</span>
    <span class="n">immutable_collections</span><span class="o">.</span><span class="n">immutable_list</span><span class="p">,</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="kc">None</span><span class="p">),</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="n">immutable_collections</span><span class="o">.</span><span class="n">immutable_list</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">pytree</span><span class="o">.</span><span class="n">_register_pytree_node</span><span class="p">(</span>
    <span class="n">immutable_collections</span><span class="o">.</span><span class="n">immutable_dict</span><span class="p">,</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">values</span><span class="p">()),</span> <span class="nb">list</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">keys</span><span class="p">())),</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="n">immutable_collections</span><span class="o">.</span><span class="n">immutable_dict</span><span class="p">(</span>
        <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">x</span><span class="p">)}</span>
    <span class="p">),</span>
<span class="p">)</span>

<span class="n">aten</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span>

<span class="c1"># This global counter increments every time we compile a graph with</span>
<span class="c1"># AOTAutograd.  You can use this to correlate runtime error messages</span>
<span class="c1"># with compile time (e.g., if you get an error at runtime saying</span>
<span class="c1"># compiled graph 3 failed, you can set a breakpoint at compile time</span>
<span class="c1"># for this graph number to investigate further at compile time.)</span>
<span class="c1">#</span>
<span class="c1"># NB: this is different from get_aot_compilation_context, which tracks</span>
<span class="c1"># each underlying graph that is compiled.  In contrast, AOT_COUNTER</span>
<span class="c1"># corresponds to top-level invocations of aot_module/aot_function;</span>
<span class="c1"># one counter is allocated per entire compiled block (but this block</span>
<span class="c1"># may involve compiling multiple subgraphs; e.g., for forwards/backwards)</span>
<span class="n">AOT_COUNTER</span> <span class="o">=</span> <span class="n">itertools</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>

<span class="n">KNOWN_TYPES</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">SymInt</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">SymFloat</span><span class="p">]</span>


<span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">preserve_rng_state</span><span class="p">():</span>
    <span class="n">rng_state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">get_rng_state</span><span class="p">())</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="n">cuda_rng_state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_rng_state</span><span class="p">())</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_rng_state</span><span class="p">(</span><span class="n">rng_state</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_rng_state</span><span class="p">(</span><span class="n">cuda_rng_state</span><span class="p">)</span>


<span class="c1"># Set up hooks so that during backward the fx&#39;s stack_trace is properly set</span>
<span class="n">callback_set</span> <span class="o">=</span> <span class="kc">False</span>


<span class="k">def</span> <span class="nf">setup_stacktrace_preservation_hooks</span><span class="p">(</span><span class="n">roots</span><span class="p">:</span> <span class="n">List</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">iter_graph</span><span class="p">(</span><span class="n">roots</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">roots</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">seen</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">roots</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">node</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">seen</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
                <span class="n">q</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>

        <span class="k">while</span> <span class="n">q</span><span class="p">:</span>
            <span class="n">node</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">popleft</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">fn</span><span class="p">,</span> <span class="n">_idx</span> <span class="ow">in</span> <span class="n">node</span><span class="o">.</span><span class="n">next_functions</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">fn</span> <span class="ow">in</span> <span class="n">seen</span> <span class="ow">or</span> <span class="n">fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="n">seen</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
                <span class="n">q</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>

            <span class="k">yield</span> <span class="n">node</span>

    <span class="k">def</span> <span class="nf">get_callback</span><span class="p">(</span><span class="n">saved_stack_</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">callback</span><span class="p">():</span>
            <span class="k">global</span> <span class="n">callback_set</span>
            <span class="n">fx_traceback</span><span class="o">.</span><span class="n">set_stack_trace</span><span class="p">(</span><span class="n">saved_stack_</span><span class="p">)</span>
            <span class="n">callback_set</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">return</span> <span class="n">callback</span>

    <span class="k">def</span> <span class="nf">get_prehook</span><span class="p">(</span><span class="n">stack_</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">prehook</span><span class="p">(</span><span class="n">grad_output</span><span class="p">):</span>
            <span class="k">global</span> <span class="n">callback_set</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">callback_set</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">variable</span><span class="o">.</span><span class="n">Variable</span><span class="o">.</span><span class="n">_execution_engine</span><span class="o">.</span><span class="n">queue_callback</span><span class="p">(</span>
                    <span class="n">get_callback</span><span class="p">(</span><span class="n">fx_traceback</span><span class="o">.</span><span class="n">format_stack</span><span class="p">())</span>
                <span class="p">)</span>
                <span class="n">callback_set</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="n">fx_traceback</span><span class="o">.</span><span class="n">set_stack_trace</span><span class="p">(</span><span class="n">stack_</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">prehook</span>

    <span class="k">def</span> <span class="nf">get_posthook</span><span class="p">(</span><span class="n">special_stack_</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">posthook</span><span class="p">(</span><span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
            <span class="n">fx_traceback</span><span class="o">.</span><span class="n">set_stack_trace</span><span class="p">(</span><span class="n">special_stack_</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">posthook</span>

    <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">iter_graph</span><span class="p">(</span><span class="n">roots</span><span class="p">):</span>
        <span class="n">forward_node_stack</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;traceback_&quot;</span><span class="p">,</span> <span class="p">[])</span>
        <span class="n">node</span><span class="o">.</span><span class="n">register_prehook</span><span class="p">(</span><span class="n">get_prehook</span><span class="p">(</span><span class="n">forward_node_stack</span><span class="p">))</span>

        <span class="n">special_stack</span> <span class="o">=</span> <span class="n">forward_node_stack</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">special_stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="s2">&quot;Gradient addition node due to multiple use of tensor around:&quot;</span>
        <span class="p">)</span>
        <span class="n">node</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span><span class="n">get_posthook</span><span class="p">(</span><span class="n">special_stack</span><span class="p">))</span>


<span class="c1"># This class tells us about a user&#39;s forward output that is an alias.</span>
<span class="c1"># It can be an alias of either a user forward input, of of a graph intermediate.</span>
<span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">OutputAliasInfo</span><span class="p">:</span>
    <span class="c1"># Tells us if this output is:</span>
    <span class="c1"># (1) a regular (non-aliased) output</span>
    <span class="c1"># (2) an alias of a forward input</span>
    <span class="c1"># (2) an alias of an intermediate (aka an alias of an output of the inner traced forward)</span>
    <span class="n">output_type</span><span class="p">:</span> <span class="n">OutputType</span>
    <span class="c1"># If (1) above, then</span>
    <span class="c1"># - Tells us that the base of this alias is user_fwd_input[base_idx]</span>
    <span class="c1">#   (This is an index into the inputs *before* we make synthetic bases)</span>
    <span class="c1"># If (2) above, then</span>
    <span class="c1"># - Tells us that the base of this alias is traced_fwd_outputs[base_idx]</span>
    <span class="c1">#   here, this refers to the index of the *direct* traced</span>
    <span class="n">base_idx</span><span class="p">:</span> <span class="nb">int</span>
    <span class="c1"># sizes, strides and storage offset of the aliased output are all returned as actual (sym)ints</span>
    <span class="c1"># in the compiled forward. These indices tell us where in the forward outputs to grab them.</span>
    <span class="n">sizes_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
    <span class="n">strides_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
    <span class="n">storage_offset_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
    <span class="c1"># We store the actual output alias that we traced in the forward (should be a fake tensor)</span>
    <span class="c1"># to grab any other non-symbolic properties on the output alias, like requires_grad.</span>
    <span class="c1"># It&#39;s optional here, for cases where the user directly returns an input as an output.</span>
    <span class="c1"># If output_type == non_alias, then these fields are also always None.</span>
    <span class="n">tensor_meta</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span>


<span class="c1"># This class tells us about how to perform a metadata mutation on forward inputs.</span>
<span class="c1"># it only applies to forward inputs that experience metadata-only mutations</span>
<span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">InputAliasInfo</span><span class="p">:</span>
    <span class="c1"># This object gives us information about how to perform a metadata-mutation</span>
    <span class="c1"># on original_fwd_inputs[base_idx]</span>
    <span class="c1">#   (This is an index into the inputs *before* we make synthetic bases)</span>
    <span class="n">base_idx</span><span class="p">:</span> <span class="nb">int</span>
    <span class="c1"># sizes, strides and storage offset of the aliased output are all returned as actual (sym)ints</span>
    <span class="c1"># in the compiled forward. These indices tell us where in the forward outputs to grab them.</span>
    <span class="n">sizes_idx</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">strides_idx</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">storage_offset_idx</span><span class="p">:</span> <span class="nb">int</span>
    <span class="c1"># We store the actual output alias that we traced in the forward (should be a fake tensor)</span>
    <span class="c1"># to grab any other non-symbolic properties on the output alias, like requires_grad.</span>
    <span class="n">tensor_meta</span><span class="p">:</span> <span class="n">Tensor</span>


<span class="c1"># This class encapsulates all aliasing + mutation info we need about the forward graph</span>
<span class="c1"># See a more detailed overview of the edge case handling at</span>
<span class="c1"># https://docs.google.com/document/d/19UoIh_SVrMy_b2Sx5ZaeOJttm6P0Qmyss2rdBuyfoic/edit</span>
<span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">ViewAndMutationMeta</span><span class="p">:</span>
    <span class="c1"># length: # user forward inputs</span>
    <span class="c1"># For every input, tells us whether the input:</span>
    <span class="c1"># (a) is not mutated</span>
    <span class="c1"># (b) only metadata is mutated</span>
    <span class="c1"># (c) data (and maybe metadta) is mutated</span>
    <span class="n">mutated_input_info</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">MutationType</span><span class="p">]</span>
    <span class="c1"># length: (# inputs of the user forward)</span>
    <span class="c1"># metadata_mutation_input_info[i] is not None &lt;====&gt; mutated_input_info[i] == MutationType.metadata_only</span>
    <span class="c1"># We stash the updated FakeTensor that we traced with in the forward in here,</span>
    <span class="c1"># that way we can use it to replay the metadata mutation</span>
    <span class="n">metadata_mutation_input_info</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">InputAliasInfo</span><span class="p">]]</span>
    <span class="c1"># length: # outputs in the compiled forward (not including output alias symints). Equal to:</span>
    <span class="c1"># length: (# inputs w data mutations) + (# outputs that don&#39;t alias inputs)</span>
    <span class="c1"># For every output *and* mutated input returned from the forward,</span>
    <span class="c1"># tells us whether or not the output should require gradients or not</span>
    <span class="n">requires_grad_out_info</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span>
    <span class="c1"># length: # fw outputs</span>
    <span class="n">aliased_output_info</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">OutputAliasInfo</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">gen_alias_from_base</span><span class="p">(</span>
    <span class="n">aliased_base_tensor</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">storage_offset</span><span class="p">,</span> <span class="n">target_meta_tensor</span>
<span class="p">):</span>
    <span class="c1"># handle R2C and C2R</span>
    <span class="k">if</span> <span class="n">aliased_base_tensor</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">target_meta_tensor</span><span class="o">.</span><span class="n">is_complex</span><span class="p">():</span>
        <span class="n">aliased_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">aliased_base_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">as_strided</span><span class="p">(</span>
            <span class="n">size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">storage_offset</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="n">aliased_base_tensor</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="ow">and</span> <span class="n">target_meta_tensor</span><span class="o">.</span><span class="n">is_complex</span><span class="p">():</span>
        <span class="n">aliased_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_complex</span><span class="p">(</span><span class="n">aliased_base_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">as_strided</span><span class="p">(</span>
            <span class="n">size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">storage_offset</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">aliased_out</span> <span class="o">=</span> <span class="n">aliased_base_tensor</span><span class="o">.</span><span class="n">as_strided</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">storage_offset</span><span class="p">)</span>
    <span class="c1"># For outputs aliasing inputs, we need to check if the requires-gradness has changed.</span>
    <span class="k">if</span> <span class="n">aliased_base_tensor</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">target_meta_tensor</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
        <span class="n">aliased_out</span> <span class="o">=</span> <span class="n">aliased_out</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="n">aliased_base_tensor</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">and</span> <span class="n">target_meta_tensor</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
        <span class="n">aliased_out</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">aliased_out</span>


<span class="c1"># This is a version of functionalization that is specifically designed</span>
<span class="c1"># for the AOTAutograd use case.</span>
<span class="c1">#</span>
<span class="c1"># Unlike functorch&#39;s variant, this doesn&#39;t use the functorch level system,</span>
<span class="c1"># instead it directly uses PyTorch&#39;s conventional dispatcher to hit the</span>
<span class="c1"># functionalization key.  In particular, this means that FunctionalTensorWrapper</span>
<span class="c1"># can have autograd data stored directly on it.</span>
<span class="c1">#</span>
<span class="c1"># In typical AOTAutograd usage, the dispatch key order will look like:</span>
<span class="c1">#</span>
<span class="c1">#   Autograd - Functionalization ~~~~&gt; Proxy Mode - Fake Tensor</span>
<span class="c1">#       outer tensor                        inner tensor</span>
<span class="c1">#</span>
<span class="c1"># TODO: Provide a faster version of this that assumes flat arguments</span>
<span class="c1"># (so no pytree necessary)</span>
<span class="k">def</span> <span class="nf">run_functionalized_fw_and_collect_metadata</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
    <span class="n">memo</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">to_fun</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">memo</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">memo</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
            <span class="n">r</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_to_functional_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">mirror_autograd_meta</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">memo</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span>
            <span class="k">return</span> <span class="n">r</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">t</span>

    <span class="k">def</span> <span class="nf">from_fun</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">_is_functional_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">t</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_sync</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_from_functional_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>

    <span class="nd">@wraps</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">inner</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="c1"># This function is meant to be run with the forward, which expects a flat list of tensor/symint/other args.</span>
        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">a</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="ow">in</span> <span class="n">KNOWN_TYPES</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">args</span><span class="p">)</span>

        <span class="n">collect_mutated_input_info</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">MutationType</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">collect_requires_grad_out_info</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">collect_aliased_output_info</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">OutputAliasInfo</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">collect_metadata_mutation_input_info</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">InputAliasInfo</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="n">f_args</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="n">to_fun</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">_enable_functionalization</span><span class="p">(</span><span class="n">reapply_views</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">outs</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">f_args</span><span class="p">)</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">_disable_functionalization</span><span class="p">()</span>

        <span class="n">flat_args</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
        <span class="n">flat_f_args</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">(</span><span class="n">f_args</span><span class="p">)</span>
        <span class="n">flat_outs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">(</span><span class="n">outs</span><span class="p">)</span>

        <span class="c1"># Inspect the state of the input tensor functional wrapper to detect input mutation info</span>
        <span class="n">inputs_with_mutated_data</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># If inp[i] has a metadata-only mutation, then maybe_inputs_with_mutated_metadata[i] contains the updated version</span>
        <span class="n">maybe_inputs_with_mutated_metadata</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">f_arg</span><span class="p">))</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">flat_args</span><span class="p">,</span> <span class="n">flat_f_args</span><span class="p">)):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
                <span class="n">new_arg</span> <span class="o">=</span> <span class="n">arg</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">_sync</span><span class="p">(</span><span class="n">f_arg</span><span class="p">)</span>
                <span class="n">new_arg</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_from_functional_tensor</span><span class="p">(</span><span class="n">f_arg</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">arg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">new_arg</span><span class="p">:</span>
                <span class="c1"># Note [Input mutation handling in aot autograd]</span>
                <span class="c1"># We use functionalization to detect two types in input mutations:</span>
                <span class="c1"># (1) metadata-only input mutations, like input.t_()</span>
                <span class="c1"># (2) data input mutations, like input.add_(1)</span>
                <span class="c1">#     inputs that have both data and metadata mutated get lumped into (2).</span>
                <span class="c1">#</span>
                <span class="c1"># Why do we distinguish these two cases? aot autograd needs to handle them very differently.</span>
                <span class="c1"># For data mutations, we return the updated inputs *directly* in the compiled forward graph.</span>
                <span class="c1"># e.g.</span>
                <span class="c1"># def f(x):</span>
                <span class="c1">#     x.mul_(2)</span>
                <span class="c1">#     out = x.mul(3)</span>
                <span class="c1">#     return out</span>
                <span class="c1">#</span>
                <span class="c1"># // This function gets compiled and dumped inside of an autograd.Function.forward()</span>
                <span class="c1"># def traced_forward(x):</span>
                <span class="c1">#     x_updated = x.mul(2)</span>
                <span class="c1">#     out = x_updated.mul(3)</span>
                <span class="c1">#     return x_updated, out</span>
                <span class="c1">#</span>
                <span class="c1"># // The returned function will call the compiled forward, and apply input mutations afterwards</span>
                <span class="c1"># def compiled_fn(x):</span>
                <span class="c1">#    x_updated, out = traced_forward(x)</span>
                <span class="c1">#    x.copy_(x_updated)</span>
                <span class="c1">#    return out</span>
                <span class="c1">#</span>
                <span class="c1"># For input metadata mutations, though, we cannot return the &quot;updated input&quot; in the forward graph,</span>
                <span class="c1"># Because it is an alias of an input. autograd.Function.forward can&#39;t handle arbitrary outputs that alias inputs.</span>
                <span class="c1"># Instead, we stash the &quot;updated input metadata&quot; during tracing</span>
                <span class="c1"># e.g.</span>
                <span class="c1"># def f(x):</span>
                <span class="c1">#     x.t_()</span>
                <span class="c1">#     out = x.mul(3)</span>
                <span class="c1">#     return out</span>
                <span class="c1">#</span>
                <span class="c1"># // This function gets compiled and dumped inside of an autograd.Function.forward()</span>
                <span class="c1"># // (We don&#39;t return x_updated. Just return the original fw out)</span>
                <span class="c1"># def traced_forward(x):</span>
                <span class="c1">#     x_updated = x.t()</span>
                <span class="c1">#     out = x_updated.mul(3)</span>
                <span class="c1">#     return out</span>
                <span class="c1">#</span>
                <span class="c1"># // The returned function will call the compiled forward, and apply input mutations afterwards</span>
                <span class="c1"># def compiled_fn(x):</span>
                <span class="c1">#    out = traced_forward(x)</span>
                <span class="c1">#    _x_updated_metadata = CompiledFunction.fw_metadata.metadata_mutation_input_info[0]</span>
                <span class="c1">#    x.as_strided_(_x_updated_metadata.size(), _x_updated_metadata.stride(), _x_updated_metadata.storage_offset())</span>
                <span class="c1">#    return out</span>
                <span class="k">if</span> <span class="n">StorageWeakRef</span><span class="p">(</span><span class="n">arg</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">())</span> <span class="o">==</span> <span class="n">StorageWeakRef</span><span class="p">(</span><span class="n">new_arg</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">()):</span>
                    <span class="c1"># We can use the storage aliasing of the inputs and updated inputs</span>
                    <span class="c1"># to detect when an input was actually updated, or just inplace-viewed.</span>
                    <span class="n">collect_mutated_input_info</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">MutationType</span><span class="o">.</span><span class="n">metadata_only</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">collect_mutated_input_info</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">MutationType</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
                    <span class="c1"># Only return mutated inputs that mutate *data*, not metadata</span>
                    <span class="c1"># Note [Input mutation handling in aot autograd]</span>
                    <span class="n">inputs_with_mutated_data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_arg</span><span class="p">)</span>
                    <span class="c1"># For every mutated input, we ALSO need to return info on</span>
                    <span class="c1"># whether than mutated input requires gradients. Why?</span>
                    <span class="c1"># Our custom autograd.Function.forward returns updated inputs as outputs,</span>
                    <span class="n">collect_requires_grad_out_info</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">f_arg</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">collect_mutated_input_info</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">MutationType</span><span class="o">.</span><span class="n">none</span><span class="p">)</span>

            <span class="n">maybe_inputs_with_mutated_metadata</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">new_arg</span>
                <span class="k">if</span> <span class="n">collect_mutated_input_info</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">MutationType</span><span class="o">.</span><span class="n">metadata_only</span>
                <span class="k">else</span> <span class="kc">None</span>
            <span class="p">)</span>

        <span class="k">def</span> <span class="nf">collect_grad_info</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
            <span class="c1"># Collect info on which output tensors require gradients,</span>
            <span class="c1"># so we can mark them properly in the returned autograd.Function.</span>
            <span class="c1"># We only collect requires_grad info on real forward outputs, and not on inputs.</span>
            <span class="n">collect_requires_grad_out_info</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">t</span><span class="o">.</span><span class="n">requires_grad</span>
            <span class="p">)</span>

        <span class="c1"># Note [output alias handling in aot autograd]</span>
        <span class="c1"># Given a function to compile where one of its outputs aliases an input,</span>
        <span class="c1"># we need to remove that output from the compiled graph and generate it off to the side.</span>
        <span class="c1"># e.g.</span>
        <span class="c1"># def f(x):</span>
        <span class="c1">#     return x.view(-1)</span>
        <span class="c1">#</span>
        <span class="c1"># Why? Two reasons:</span>
        <span class="c1"># (1) If your autograd.Function returns a view on an input in the forward, autograd.Function</span>
        <span class="c1">#     will not allow you to mutate it (This original came from arbitrary user code where the user might want to mutate)</span>
        <span class="c1"># (2) There&#39;s no reason to compile views anyway. We can just regenerate the view of the input off to the side,</span>
        <span class="c1">#</span>
        <span class="c1"># Another interesting case is when you have both mutation and aliasing:</span>
        <span class="c1"># def f(x):</span>
        <span class="c1">#     x.mul_(2)</span>
        <span class="c1">#     return x.view(-1)</span>
        <span class="c1">#</span>
        <span class="c1"># You could imagine that this output is now *safe* to compile and return in the autograd.Function,</span>
        <span class="c1"># because after functionalization runs, it will technically not alias an input:</span>
        <span class="c1"># def f_functionalized(x):</span>
        <span class="c1">#     x_updated = x.mul(2)</span>
        <span class="c1">#     return x_updated, x_updated.view(-1)</span>
        <span class="c1">#</span>
        <span class="c1"># However, this is still wrong: we can&#39;t return x_updated.view(-1) to the user. We are on the hook to return:</span>
        <span class="c1"># def traced_forward(x):</span>
        <span class="c1">#     x_updated = x.mul(2)</span>
        <span class="c1">#     return x_updated</span>
        <span class="c1">#</span>
        <span class="c1"># def compiled_fn(x)</span>
        <span class="c1">#     x_updated = traced_forward(x)</span>
        <span class="c1">#     x.copy_(x_updated)</span>
        <span class="c1">#     return x.view(-1)</span>
        <span class="c1">#</span>
        <span class="c1"># Why can&#39;t we return x_updated.view(-1) to the user?</span>
        <span class="c1"># It can have different metadata from x.view(-1)! Specifically, the input x could be a non-memory-dense tensor,</span>
        <span class="c1"># But the intermediate created by our graph, x_updated, will always be memory-dense.</span>
        <span class="k">def</span> <span class="nf">filter_and_record_aliased_outs</span><span class="p">(</span><span class="n">outputs</span><span class="p">):</span>
            <span class="c1"># NOTE: this dict will clobber keys if we have multiple inputs that alias.</span>
            <span class="c1"># Let&#39;s say inpA and inpB alias, and the user generated an output using out = inpA.view(...)</span>
            <span class="c1"># For now, since we&#39;re not handling the case with multiple _base&#39;s sharing a storage,</span>
            <span class="c1"># it is actually fine to arbitrarily pick which input to regenerate the aliased output from.</span>
            <span class="c1"># e.g. out_new = inpB.as_strided(out.size(), out.stride(), out.storage_offset())</span>
            <span class="c1">#</span>
            <span class="c1"># This will be more complicated when you have multiple _base tensors aliasing the same</span>
            <span class="c1"># underlying storage, when we eventually handle that.</span>
            <span class="c1"># We&#39;ll need to ensure that we generate the view off of the right base.</span>
            <span class="n">inp_storage_refs</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">inpt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">flat_f_args</span><span class="p">):</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inpt</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                    <span class="n">inp_storage_refs</span><span class="p">[</span><span class="n">StorageWeakRef</span><span class="p">(</span><span class="n">inpt</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">())]</span> <span class="o">=</span> <span class="n">idx</span>
            <span class="n">inp_tensor_ids</span> <span class="o">=</span> <span class="p">{</span><span class="nb">id</span><span class="p">(</span><span class="n">inpt</span><span class="p">)</span> <span class="k">for</span> <span class="n">inpt</span> <span class="ow">in</span> <span class="n">flat_f_args</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inpt</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)}</span>
            <span class="n">inp_storage_refs_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">inp_storage_refs</span><span class="p">)</span>

            <span class="n">non_aliased_input_outs</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="c1"># For a given output tensor that alias an input, tells us:</span>
            <span class="c1"># (1) the index of the input that we alias</span>
            <span class="c1"># (2) Whether or not the output is a view of the input, or if `output is input`</span>
            <span class="c1">#     (so we don&#39;t need to generate a view, and can return the input directly)</span>
            <span class="c1"># Note: if the function returns an output that *is* an input, we still cannot return it in the graph.</span>
            <span class="c1"># e.g.</span>
            <span class="c1">#   def f(x):</span>
            <span class="c1">#       x.add_(1)</span>
            <span class="c1">#       return x</span>
            <span class="c1"># Our compiled fw will return an &quot;x_updated&quot;, but it is *not* ok to return that to the user.</span>
            <span class="c1"># We need to manually do x.copy_(x_updated), and return the original x to the user.</span>
            <span class="c1"># Why? for example, the metadata between x and x_updated might be different (e.g. _is_leaf())</span>
            <span class="n">aliased_out_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>

            <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
                <span class="c1"># Note: When detecting input/output aliasing, we NEED to do it using the outer FunctionalTensorWrapper objects.</span>
                <span class="c1"># In the case where we mutate an input *and* return a view of it, the outer wrappers will still alias,</span>
                <span class="c1"># but the inner tensors no longer alias.</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">StorageWeakRef</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">())</span> <span class="ow">in</span> <span class="n">inp_storage_refs</span><span class="p">:</span>
                    <span class="n">aliased_inp_idx</span> <span class="o">=</span> <span class="n">inp_storage_refs</span><span class="p">[</span><span class="n">StorageWeakRef</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">())]</span>
                    <span class="n">is_exact_input</span> <span class="o">=</span> <span class="nb">id</span><span class="p">(</span><span class="n">o</span><span class="p">)</span> <span class="ow">in</span> <span class="n">inp_tensor_ids</span>
                    <span class="n">aliases_intermediate_and_not_input</span> <span class="o">=</span> <span class="kc">False</span>
                    <span class="n">aliased_out_idx</span><span class="p">[</span><span class="n">o</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="n">aliased_inp_idx</span><span class="p">,</span>
                        <span class="n">aliases_intermediate_and_not_input</span><span class="p">,</span>
                        <span class="n">is_exact_input</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Only return outputs that are not aliases of inputs.</span>
                    <span class="n">non_aliased_input_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">o</span><span class="p">)</span>
            <span class="c1"># If a function involves creating a tensor, and returning a view of it, such that its _base is the intermediiate,</span>
            <span class="c1"># We need to make sure our graph returns the _base as a graph output, and we manually recreate the view</span>
            <span class="c1"># to return to the user. Why? The backend compiler is free to (incorrectly) not set requires_grad</span>
            <span class="c1"># on the base tensor, but we are obligated to properly set requires-gradness on the real output.</span>
            <span class="n">non_aliased_outs</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">non_aliased_input_outs</span><span class="p">):</span>
                <span class="n">non_aliased_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">o</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">non_aliased_outs</span><span class="p">,</span> <span class="n">aliased_out_idx</span>

        <span class="n">non_aliased_outs</span><span class="p">,</span> <span class="n">aliased_out_to_inp_idx</span> <span class="o">=</span> <span class="n">filter_and_record_aliased_outs</span><span class="p">(</span><span class="n">outs</span><span class="p">)</span>

        <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="n">collect_grad_info</span><span class="p">,</span> <span class="n">non_aliased_outs</span><span class="p">)</span>

        <span class="c1"># Calling convention: the output is (mutated_input_values, original_outs)</span>
        <span class="c1"># We return all mutated inputs + outputs here, **except** for any mutated inputs or outputs</span>
        <span class="c1"># that alias original inputs.</span>
        <span class="c1"># See Note [Input mutation handling in aot autograd]</span>
        <span class="n">mutated_inps_and_outs</span> <span class="o">=</span> <span class="n">inputs_with_mutated_data</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">non_aliased_outs</span><span class="p">)</span>

        <span class="c1"># Our compiled forward function will return:</span>
        <span class="c1"># (1) non-aliased updated inputs</span>
        <span class="c1"># (2) non-aliased fw outputs</span>
        <span class="c1"># (3) size/stride/storage_offset metadata for updated aliased inputs</span>
        <span class="c1"># (4) size/stride/storage_offset metadata for aliased outputs</span>

        <span class="n">start_idx_for_aliased_output_metadata</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># First, gather the metadata info on mutated inputs (this only applies to inputs with metadata-only mutations))</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">maybe_aliased_updated_inp</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
            <span class="n">maybe_inputs_with_mutated_metadata</span>
        <span class="p">):</span>
            <span class="k">if</span> <span class="n">maybe_aliased_updated_inp</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">collect_metadata_mutation_input_info</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
                <span class="k">continue</span>
            <span class="c1"># Figure out where the sizes/strides/storage_offset are in the compiled fw output.</span>
            <span class="n">sizes_idx</span> <span class="o">=</span> <span class="n">start_idx_for_aliased_output_metadata</span>
            <span class="n">strides_idx</span> <span class="o">=</span> <span class="n">sizes_idx</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">maybe_aliased_updated_inp</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
            <span class="n">storage_offset_idx</span> <span class="o">=</span> <span class="n">strides_idx</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">maybe_aliased_updated_inp</span><span class="o">.</span><span class="n">stride</span><span class="p">())</span>
            <span class="c1"># update our offset for the next tensor</span>
            <span class="n">start_idx_for_aliased_output_metadata</span> <span class="o">=</span> <span class="n">storage_offset_idx</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="n">inp_info</span> <span class="o">=</span> <span class="n">InputAliasInfo</span><span class="p">(</span>
                <span class="n">base_idx</span><span class="o">=</span><span class="n">i</span><span class="p">,</span>
                <span class="n">sizes_idx</span><span class="o">=</span><span class="n">sizes_idx</span><span class="p">,</span>
                <span class="n">strides_idx</span><span class="o">=</span><span class="n">strides_idx</span><span class="p">,</span>
                <span class="n">storage_offset_idx</span><span class="o">=</span><span class="n">storage_offset_idx</span><span class="p">,</span>
                <span class="n">tensor_meta</span><span class="o">=</span><span class="n">maybe_aliased_updated_inp</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">collect_metadata_mutation_input_info</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inp_info</span><span class="p">)</span>

        <span class="c1"># Next, gather the metadata info on the user&#39;s outputs that alias (either inputs or graph outputs)</span>
        <span class="n">num_non_input_aliased_outputs</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">outs</span><span class="p">:</span>
            <span class="n">maybe_alias_info</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">aliased_out_to_inp_idx</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
                <span class="k">else</span> <span class="kc">None</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">maybe_alias_info</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">output_type</span> <span class="o">=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">non_alias</span>
                <span class="c1"># Here, alias_idx will tell us which output from the inner forward this corresponds to.</span>
                <span class="n">alias_idx</span> <span class="o">=</span> <span class="n">num_non_input_aliased_outputs</span>
                <span class="n">sizes_idx</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="n">strides_idx</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="n">storage_offset_idx</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="n">tensor_meta</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="p">(</span>
                    <span class="n">input_alias_idx</span><span class="p">,</span>
                    <span class="n">is_alias_of_intermediate_not_input</span><span class="p">,</span>
                    <span class="n">is_exact_input</span><span class="p">,</span>
                <span class="p">)</span> <span class="o">=</span> <span class="n">maybe_alias_info</span>
                <span class="k">if</span> <span class="n">is_exact_input</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="ow">not</span> <span class="n">is_alias_of_intermediate_not_input</span>
                    <span class="n">output_type</span> <span class="o">=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_input</span>
                    <span class="n">alias_idx</span> <span class="o">=</span> <span class="n">input_alias_idx</span>
                    <span class="n">sizes_idx</span> <span class="o">=</span> <span class="kc">None</span>
                    <span class="n">strides_idx</span> <span class="o">=</span> <span class="kc">None</span>
                    <span class="n">storage_offset_idx</span> <span class="o">=</span> <span class="kc">None</span>
                    <span class="n">tensor_meta</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">is_alias_of_intermediate_not_input</span><span class="p">:</span>
                        <span class="n">output_type</span> <span class="o">=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_intermediate</span>
                        <span class="n">alias_idx</span> <span class="o">=</span> <span class="n">num_non_input_aliased_outputs</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">output_type</span> <span class="o">=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_input</span>
                        <span class="n">alias_idx</span> <span class="o">=</span> <span class="n">input_alias_idx</span>
                    <span class="n">tensor_meta</span> <span class="o">=</span> <span class="n">o</span>
                    <span class="c1"># Figure out where the sizes/strides/storage_offset are in the compiled fw output.</span>
                    <span class="n">sizes_idx</span> <span class="o">=</span> <span class="n">start_idx_for_aliased_output_metadata</span>
                    <span class="n">strides_idx</span> <span class="o">=</span> <span class="n">sizes_idx</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">tensor_meta</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
                    <span class="n">storage_offset_idx</span> <span class="o">=</span> <span class="n">strides_idx</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">tensor_meta</span><span class="o">.</span><span class="n">stride</span><span class="p">())</span>
                    <span class="c1"># update our offset for the next tensor</span>
                    <span class="n">start_idx_for_aliased_output_metadata</span> <span class="o">=</span> <span class="n">storage_offset_idx</span> <span class="o">+</span> <span class="mi">1</span>

            <span class="k">if</span> <span class="n">output_type</span> <span class="o">!=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_input</span><span class="p">:</span>
                <span class="n">num_non_input_aliased_outputs</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="n">inp_info</span> <span class="o">=</span> <span class="n">OutputAliasInfo</span><span class="p">(</span>
                <span class="n">output_type</span><span class="o">=</span><span class="n">output_type</span><span class="p">,</span>
                <span class="n">base_idx</span><span class="o">=</span><span class="n">alias_idx</span><span class="p">,</span>
                <span class="n">sizes_idx</span><span class="o">=</span><span class="n">sizes_idx</span><span class="p">,</span>
                <span class="n">strides_idx</span><span class="o">=</span><span class="n">strides_idx</span><span class="p">,</span>
                <span class="n">storage_offset_idx</span><span class="o">=</span><span class="n">storage_offset_idx</span><span class="p">,</span>
                <span class="n">tensor_meta</span><span class="o">=</span><span class="n">tensor_meta</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">collect_aliased_output_info</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inp_info</span><span class="p">)</span>

        <span class="c1"># This is the total number of size/stride/storage_offset metadata outputs that we return in the forward,</span>
        <span class="c1"># used for regenerating aliases later.</span>
        <span class="n">num_aliasing_metadata_outs</span> <span class="o">=</span> <span class="n">start_idx_for_aliased_output_metadata</span>

        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">collect_metadata_mutation_input_info</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span>
            <span class="n">collect_mutated_input_info</span>
        <span class="p">)</span>

        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span>
            <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">collect_metadata_mutation_input_info</span> <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">]</span>
        <span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span>
            <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">collect_mutated_input_info</span> <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="n">MutationType</span><span class="o">.</span><span class="n">metadata_only</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">collect_aliased_output_info</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">outs</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">x</span>
                <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">collect_aliased_output_info</span>
                <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">output_type</span> <span class="o">!=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_input</span>
            <span class="p">]</span>
        <span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">non_aliased_outs</span><span class="p">)</span>

        <span class="c1"># Our autograd.Function.forward returns both mutated inputs and outputs,</span>
        <span class="c1"># so we need grad info on all of them.</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">collect_requires_grad_out_info</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">mutated_inps_and_outs</span><span class="p">)</span>

        <span class="n">metadata</span> <span class="o">=</span> <span class="n">ViewAndMutationMeta</span><span class="p">(</span>
            <span class="n">mutated_input_info</span><span class="o">=</span><span class="n">collect_mutated_input_info</span><span class="p">,</span>
            <span class="n">metadata_mutation_input_info</span><span class="o">=</span><span class="n">collect_metadata_mutation_input_info</span><span class="p">,</span>
            <span class="n">requires_grad_out_info</span><span class="o">=</span><span class="n">collect_requires_grad_out_info</span><span class="p">,</span>
            <span class="n">aliased_output_info</span><span class="o">=</span><span class="n">collect_aliased_output_info</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="n">metadata</span><span class="p">,</span>
            <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="n">from_fun</span><span class="p">,</span> <span class="n">mutated_inps_and_outs</span><span class="p">),</span>
            <span class="n">num_aliasing_metadata_outs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">inner</span>


<span class="c1"># This creates a functionalized joint forwards-backwards function given both</span>
<span class="c1"># the primals (to run forwards) and tangents (to run backwards).</span>
<span class="c1">#</span>
<span class="c1"># It uses the metadata that was created earlier to figure out what all of the outputs to the autograd.Function.forward are:</span>
<span class="c1"># (1) Which inputs received data mutations (and need to be passed as outputs into autograd.grad())</span>
<span class="c1"># (2) Which outputs are aliases of inputs (and should *not* be passed as outputs into autograd.grad())</span>
<span class="k">def</span> <span class="nf">create_joint_forward_backward_functionalized</span><span class="p">(</span>
    <span class="n">fn</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">meta</span><span class="p">:</span> <span class="n">ViewAndMutationMeta</span><span class="p">,</span>
    <span class="n">synthetic_base_info</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]]]]],</span>
<span class="p">):</span>
    <span class="c1"># NOTE: when we have synthetic base inputs, we need to clone them *before* creating views off of them.</span>
    <span class="c1"># This means that &quot;idx&quot; here represents the index of the (potentially) synthetic base.</span>
    <span class="c1"># What we need to do is:</span>
    <span class="c1"># (1) map the current (post-synthetic-base calling convention) input argument index</span>
    <span class="c1">#     to int index pre-synthetic-base-calling-convention.</span>
    <span class="c1"># (2) There could be multiple, if this index corresponds to a synthetic base</span>
    <span class="c1">#     that has multiple input aliases.</span>
    <span class="c1"># (3) If any of those corresponding inputs get metadata mutations, then we clone the base.</span>
    <span class="k">def</span> <span class="nf">maybe_to_fresh_input</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">t</span>

        <span class="k">if</span> <span class="n">synthetic_base_info</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">outer_aliased_indices_of_current_base_arg</span> <span class="o">=</span> <span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">outer_aliased_indices_of_current_base_arg</span> <span class="o">=</span> <span class="p">[</span>
                <span class="c1"># For every argument index in the outer calling convention (before synthetic bases)</span>
                <span class="c1"># find its index in the inner calling convention.</span>
                <span class="c1"># if it matches the index of our current arg (idx), track the outer argument&#39;s index (i)</span>
                <span class="n">i</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">outer_idx_or_lambda</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">synthetic_base_info</span><span class="p">)</span>
                <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">outer_idx_or_lambda</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">outer_idx_or_lambda</span> <span class="o">==</span> <span class="n">idx</span><span class="p">)</span>
                <span class="ow">or</span> <span class="p">(</span>
                    <span class="nb">isinstance</span><span class="p">(</span><span class="n">outer_idx_or_lambda</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span>
                    <span class="ow">and</span> <span class="n">outer_idx_or_lambda</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">idx</span>
                <span class="p">)</span>
            <span class="p">]</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span>
            <span class="n">meta</span><span class="o">.</span><span class="n">mutated_input_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">MutationType</span><span class="o">.</span><span class="n">data</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">outer_aliased_indices_of_current_base_arg</span>
        <span class="p">):</span>
            <span class="c1"># Make sure the primal we pass to autograd.grad()</span>
            <span class="c1"># seees the tensor before the mutation</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="k">elif</span> <span class="nb">any</span><span class="p">(</span>
            <span class="n">meta</span><span class="o">.</span><span class="n">mutated_input_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">MutationType</span><span class="o">.</span><span class="n">metadata_only</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">outer_aliased_indices_of_current_base_arg</span>
        <span class="p">):</span>
            <span class="c1"># Make sure the primal we pass to autograd.grad()</span>
            <span class="c1"># seees the tensor before the metadata mutation</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">t</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">unpack_synthetic_bases</span><span class="p">(</span><span class="n">primals</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]:</span>
        <span class="c1"># This is only not None if our graph mutates a graph input that aliases another graph input.</span>
        <span class="k">if</span> <span class="n">synthetic_base_info</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">primals</span>

        <span class="n">f_args_inner</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">outer_idx_or_lambda</span> <span class="ow">in</span> <span class="n">synthetic_base_info</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">outer_idx_or_lambda</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="n">f_args_inner</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">primals</span><span class="p">[</span><span class="n">outer_idx_or_lambda</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">outer_base_idx</span><span class="p">,</span> <span class="n">strided_args</span> <span class="o">=</span> <span class="n">outer_idx_or_lambda</span>
                <span class="n">outer_base</span> <span class="o">=</span> <span class="n">primals</span><span class="p">[</span><span class="n">outer_base_idx</span><span class="p">]</span>
                <span class="c1"># TODO: we could consider storing and executing view replay logic here,</span>
                <span class="c1"># instead of a general as_strided() call.</span>
                <span class="c1"># This could also improve perf, since today this will cause</span>
                <span class="c1"># more as_strided_scatter() ops in the graph.</span>
                <span class="n">view_arg</span> <span class="o">=</span> <span class="n">outer_base</span><span class="o">.</span><span class="n">as_strided</span><span class="p">(</span><span class="o">*</span><span class="n">strided_args</span><span class="p">)</span>
                <span class="n">f_args_inner</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">view_arg</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">f_args_inner</span>

    <span class="k">def</span> <span class="nf">joint_forward_backward</span><span class="p">(</span>
        <span class="n">primals</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">tangents</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]]:</span>
        <span class="c1"># Call the forward pass, making sure to clone any inputs that are mutated first.</span>
        <span class="c1"># We need to ensure that the inputs we pass to autograd.grad() are the *original*</span>
        <span class="c1"># inputs, and not their mutated values.</span>
        <span class="n">primals_no_input_mutations</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">maybe_to_fresh_input</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">primals</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="c1"># This is also where we handle the calling convention around synthetic bases.</span>
        <span class="c1"># We need to make sure that we convert any synthetic base arguments into views</span>
        <span class="c1"># *after* we do the cloning above, to preserve the view relationship.</span>
        <span class="n">primals_</span> <span class="o">=</span> <span class="n">unpack_synthetic_bases</span><span class="p">(</span><span class="n">primals_no_input_mutations</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">mutated_input_info</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">primals_</span><span class="p">)</span>
        <span class="n">all_outs</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">primals_</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">aliased_output_info</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_outs</span><span class="p">)</span>

        <span class="c1"># Pass any (non-aliased) outputs in as tangents, since they&#39;ll be returned as outputs in the fw</span>
        <span class="c1"># For outputs that are aliases of intermediates, we will have returned the output&#39;s _base as an output in the graph instead,</span>
        <span class="c1"># which we *should* send to grad()</span>
        <span class="n">outputs_for_grad</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">x</span>
            <span class="c1"># TODO: support ._base</span>
            <span class="c1"># x._base if meta.aliased_output_info[i].output_type == OutputType.alias_of_intermediate else x</span>
            <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">all_outs</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">meta</span><span class="o">.</span><span class="n">aliased_output_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">output_type</span> <span class="o">!=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_input</span>
        <span class="p">]</span>
        <span class="c1"># Pass any (non-aliased) mutated inputs in as tangents, since they&#39;ll be returned as outputs in the fw</span>
        <span class="c1"># Important: the traced joint fw/bw will return updated inputs with data mutations,</span>
        <span class="c1"># but *not* with metadata mutations.</span>
        <span class="c1"># Instead, we shunt the updated metadata around externally</span>
        <span class="c1"># and update the input&#39;s metadata outside of the autograd.Function</span>
        <span class="n">mutated_inputs_for_grad</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">x</span>
            <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">primals_</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">meta</span><span class="o">.</span><span class="n">mutated_input_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">MutationType</span><span class="o">.</span><span class="n">data</span>
        <span class="p">]</span>
        <span class="n">mutated_inputs_and_outs_to_grad</span> <span class="o">=</span> <span class="n">mutated_inputs_for_grad</span> <span class="o">+</span> <span class="n">outputs_for_grad</span>

        <span class="n">metadata_mutated_inps</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">x</span>
            <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">primals_</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">meta</span><span class="o">.</span><span class="n">mutated_input_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">MutationType</span><span class="o">.</span><span class="n">metadata_only</span>
        <span class="p">]</span>
        <span class="c1"># for user outputs that are aliases (either of inputs, or of graph intermediates)</span>
        <span class="c1"># figure out what metadata to return in the forward, which is needed to regenerate the output aliases</span>
        <span class="n">aliased_outs</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">x</span>
            <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">all_outs</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">meta</span><span class="o">.</span><span class="n">aliased_output_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">output_type</span> <span class="o">!=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">non_alias</span>
            <span class="ow">and</span> <span class="n">meta</span><span class="o">.</span><span class="n">aliased_output_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">tensor_meta</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">]</span>
        <span class="n">output_metadata_for_fw</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">curr_alias</span> <span class="ow">in</span> <span class="n">metadata_mutated_inps</span> <span class="o">+</span> <span class="n">aliased_outs</span><span class="p">:</span>
            <span class="n">size_</span> <span class="o">=</span> <span class="n">curr_alias</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
            <span class="n">stride_</span> <span class="o">=</span> <span class="n">curr_alias</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span>
            <span class="n">storage_offset_</span> <span class="o">=</span> <span class="n">curr_alias</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span>
            <span class="c1"># FX IR doesn&#39;t know about tuples, so we flatten the metadata into individual ints/symints,</span>
            <span class="c1"># and index into the final output list later.</span>
            <span class="n">output_metadata_for_fw</span> <span class="o">+=</span> <span class="n">size_</span> <span class="o">+</span> <span class="n">stride_</span> <span class="o">+</span> <span class="p">(</span><span class="n">storage_offset_</span><span class="p">,)</span>

        <span class="c1"># Take care to grab and sync the updated inputs from primals_ (the inputs we actually mutate!)</span>
        <span class="c1"># and not primals (the preserved inputs, pre-mutation, that we pass to grad())</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">arg</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">primals_</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
                <span class="k">continue</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">_sync</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span>

        <span class="c1"># Get the inputs that need gradients</span>
        <span class="n">grad_primals</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">inputs_needs_grads</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># Note that we&#39;re not using primals_ here, being carefully not to pass any mutated inputs into autograd.grad()</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">primals</span><span class="p">:</span>
            <span class="n">is_grad_tensor</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span>
            <span class="n">inputs_needs_grads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">is_grad_tensor</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">is_grad_tensor</span><span class="p">:</span>
                <span class="n">grad_primals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

        <span class="c1"># Get the outputs that need gradients</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">tangents</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">mutated_inputs_and_outs_to_grad</span><span class="p">)</span>
        <span class="n">needed_outs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">needed_tangents</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">out</span><span class="p">,</span> <span class="n">tangent</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">mutated_inputs_and_outs_to_grad</span><span class="p">,</span> <span class="n">tangents</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">out</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                <span class="c1"># A bit sketchy, but fixes e.g. test_aot_autograd_exhaustive_matmul_cpu_float32</span>
                <span class="c1"># The issue is that we are sensitive to decomps that don&#39;t accurately maintain</span>
                <span class="c1"># their output&#39;s _base.shape compared to eager mode, and this helps mitigate a bit.</span>
                <span class="n">needed_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">out</span> <span class="k">if</span> <span class="n">out</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">tangent</span><span class="o">.</span><span class="n">shape</span> <span class="k">else</span> <span class="n">out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">tangent</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">needed_tangents</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tangent</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">))</span>

        <span class="n">setup_stacktrace_preservation_hooks</span><span class="p">([</span><span class="n">out</span><span class="o">.</span><span class="n">grad_fn</span> <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">needed_outs</span><span class="p">])</span>

        <span class="n">backward_out</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># Call the backwards pass</span>
        <span class="k">if</span> <span class="n">grad_primals</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">fx_traceback</span><span class="o">.</span><span class="n">override_stack_trace</span><span class="p">():</span>
                <span class="n">backward_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span>
                    <span class="n">needed_outs</span><span class="p">,</span>
                    <span class="n">grad_primals</span><span class="p">,</span>
                    <span class="n">grad_outputs</span><span class="o">=</span><span class="n">needed_tangents</span><span class="p">,</span>
                    <span class="n">allow_unused</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>
        <span class="n">backward_out_iter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">backward_out</span><span class="p">)</span>
        <span class="n">all_fw_outs</span> <span class="o">=</span> <span class="n">mutated_inputs_and_outs_to_grad</span> <span class="o">+</span> <span class="n">output_metadata_for_fw</span>
        <span class="k">return</span> <span class="n">all_fw_outs</span><span class="p">,</span> <span class="p">[</span>
            <span class="nb">next</span><span class="p">(</span><span class="n">backward_out_iter</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="k">else</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">inputs_needs_grads</span>
        <span class="p">]</span>

    <span class="k">def</span> <span class="nf">to_fun</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_to_functional_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">mirror_autograd_meta</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">t</span>

    <span class="k">def</span> <span class="nf">from_fun</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">_is_functional_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">t</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_sync</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_from_functional_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">functionalized_joint</span><span class="p">(</span>
        <span class="n">primals</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">tangents</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]]:</span>

        <span class="c1"># Wrap inputs into functional wrappers</span>
        <span class="n">f_primals</span><span class="p">,</span> <span class="n">f_tangents</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="n">to_fun</span><span class="p">,</span> <span class="p">(</span><span class="n">primals</span><span class="p">,</span> <span class="n">tangents</span><span class="p">))</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_enable_functionalization</span><span class="p">(</span><span class="n">reapply_views</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Run the joint</span>
            <span class="n">outs</span> <span class="o">=</span> <span class="n">joint_forward_backward</span><span class="p">(</span><span class="n">f_primals</span><span class="p">,</span> <span class="n">f_tangents</span><span class="p">)</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">_disable_functionalization</span><span class="p">()</span>

        <span class="c1"># Syncing of inputs/outputs was already done directly in the joint call</span>
        <span class="k">return</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="n">from_fun</span><span class="p">,</span> <span class="n">outs</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">functionalized_joint</span>


<span class="k">def</span> <span class="nf">normalize_as_list</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>


<span class="n">aot_autograd_decompositions</span> <span class="o">=</span> <span class="p">{}</span>


<span class="c1"># This is a list since looking forward, we can have this arbitrarily nested.</span>
<span class="n">graph_being_compiled</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c1"># TODO: It would be nice to reset the numbering every time aot_id goes</span>
<span class="c1"># up, but this is annoying to do right now (because we don&#39;t know if</span>
<span class="c1"># an aot_id will come back from the dead), so right now this also happens</span>
<span class="c1"># to be a globally unique number too (at the cost of wobbling if you change</span>
<span class="c1"># how the graphs compile)</span>
<span class="n">nth_graph</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;model&quot;</span>


<span class="k">def</span> <span class="nf">set_model_name</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">model_name</span>
    <span class="n">model_name</span> <span class="o">=</span> <span class="n">name</span>


<span class="k">def</span> <span class="nf">get_aot_compilation_context</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">graph_being_compiled</span><span class="p">),</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">nth_graph</span>


<span class="k">def</span> <span class="nf">get_aot_graph_name</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the name of the graph being compiled.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">graph_being_compiled</span><span class="p">,</span> <span class="n">nth_graph</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">__</span><span class="si">{</span><span class="s1">&#39;_&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">graph_being_compiled</span><span class="p">)</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">nth_graph</span><span class="si">}</span><span class="s2">&quot;</span>


<span class="n">get_graph_being_compiled</span> <span class="o">=</span> <span class="n">get_aot_graph_name</span>


<span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">track_graph_compiling</span><span class="p">(</span><span class="n">aot_config</span><span class="p">,</span> <span class="n">graph_name</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">graph_being_compiled</span>
    <span class="c1"># TODO: Don&#39;t shove the aot_id in here; set it in the context</span>
    <span class="n">graph_being_compiled</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">aot_config</span><span class="o">.</span><span class="n">aot_id</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">graph_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span>
    <span class="k">yield</span>
    <span class="k">global</span> <span class="n">nth_graph</span>
    <span class="n">nth_graph</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">graph_being_compiled</span> <span class="o">=</span> <span class="p">[]</span>


<span class="k">def</span> <span class="nf">make_boxed_func</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">g</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>

    <span class="n">g</span><span class="o">.</span><span class="n">_boxed_call</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">g</span>


<span class="k">def</span> <span class="nf">make_boxed_compiler</span><span class="p">(</span><span class="n">compiler</span><span class="p">):</span>
    <span class="nd">@wraps</span><span class="p">(</span><span class="n">compiler</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">fx_g</span><span class="p">,</span> <span class="n">inps</span><span class="p">):</span>
        <span class="n">out_f</span> <span class="o">=</span> <span class="n">compiler</span><span class="p">(</span><span class="n">fx_g</span><span class="p">,</span> <span class="n">inps</span><span class="p">)</span>
        <span class="n">fx_g</span> <span class="o">=</span> <span class="n">make_boxed_func</span><span class="p">(</span><span class="n">out_f</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">fx_g</span>

    <span class="k">return</span> <span class="n">f</span>


<span class="k">def</span> <span class="nf">call_func_with_args</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">steal_args</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">disable_amp</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">steal_args</span><span class="p">:</span>
        <span class="n">args</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">disable_amp</span><span class="p">:</span>
        <span class="n">guard</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_DisableAutocast</span><span class="p">()</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="s2">&quot;_boxed_call&quot;</span><span class="p">):</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">normalize_as_list</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">args</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># TODO: Please remove soon</span>
            <span class="c1"># https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Your compiler for AOTAutograd is returning a a function that doesn&#39;t take boxed arguments. &quot;</span>
                <span class="s2">&quot;Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. &quot;</span>
                <span class="s2">&quot;See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.&quot;</span>
            <span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">normalize_as_list</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">))</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">disable_amp</span><span class="p">:</span>
            <span class="k">del</span> <span class="n">guard</span>
    <span class="k">return</span> <span class="n">out</span>


<span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">AOTConfig</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration for AOTDispatcher</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">fw_compiler</span><span class="p">:</span> <span class="n">Callable</span>
    <span class="n">bw_compiler</span><span class="p">:</span> <span class="n">Callable</span>
    <span class="n">partition_fn</span><span class="p">:</span> <span class="n">Callable</span>
    <span class="n">decompositions</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span>
    <span class="n">num_params_buffers</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">aot_id</span><span class="p">:</span> <span class="nb">int</span>


<span class="k">def</span> <span class="nf">aot_dispatch_base</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">aot_config</span><span class="p">:</span> <span class="n">AOTConfig</span><span class="p">):</span>
    <span class="n">fw_module</span> <span class="o">=</span> <span class="n">make_fx</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">,</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span><span class="p">)(</span><span class="o">*</span><span class="n">flat_args</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">debug_graphs</span><span class="p">:</span>
        <span class="n">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;====== Forward (only) graph </span><span class="si">{aot_config.aot_id}</span><span class="s2"> ======&quot;</span><span class="p">)</span>
        <span class="n">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="n">fw_module</span><span class="o">.</span><span class="n">print_readable</span><span class="p">(</span><span class="n">print_output</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

    <span class="n">disable_amp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_is_any_autocast_enabled</span><span class="p">()</span>
    <span class="n">context</span> <span class="o">=</span> <span class="n">disable_autocast_manager</span> <span class="k">if</span> <span class="n">disable_amp</span> <span class="k">else</span> <span class="n">nullcontext</span>

    <span class="k">with</span> <span class="n">context</span><span class="p">(),</span> <span class="n">track_graph_compiling</span><span class="p">(</span><span class="n">aot_config</span><span class="p">,</span> <span class="s2">&quot;inference&quot;</span><span class="p">):</span>
        <span class="n">compiled_fw</span> <span class="o">=</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">fw_compiler</span><span class="p">(</span><span class="n">fw_module</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">)</span>

    <span class="nd">@wraps</span><span class="p">(</span><span class="n">compiled_fw</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">new_fn</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="n">fw_outs</span> <span class="o">=</span> <span class="n">call_func_with_args</span><span class="p">(</span><span class="n">compiled_fw</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">disable_amp</span><span class="o">=</span><span class="n">disable_amp</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">fw_outs</span>
    <span class="n">new_fn</span><span class="o">.</span><span class="n">_boxed_call</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">return</span> <span class="n">new_fn</span>


<span class="k">def</span> <span class="nf">assert_functional_graph</span><span class="p">(</span><span class="n">fx_g</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">Graph</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">fx_g</span><span class="o">.</span><span class="n">nodes</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">n</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_ops</span><span class="o">.</span><span class="n">OpOverload</span><span class="p">):</span>
            <span class="k">assert</span> <span class="ow">not</span> <span class="n">n</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">_schema</span><span class="o">.</span><span class="n">is_mutable</span><span class="p">,</span> \
                <span class="sa">f</span><span class="s1">&#39;aot_autograd expected to have an entirely functional graph, but found </span><span class="si">{</span><span class="n">n</span><span class="o">.</span><span class="n">format_node</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span>


<span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">disable_autocast_manager</span><span class="p">():</span>
    <span class="n">guard</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_DisableAutocast</span><span class="p">()</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="k">del</span> <span class="n">guard</span>


<span class="k">def</span> <span class="nf">are_differentiable_views</span><span class="p">(</span><span class="n">view1</span><span class="p">,</span> <span class="n">view2</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">view1</span> <span class="ow">is</span> <span class="n">view2</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="k">if</span> <span class="n">view1</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">view2</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">view1</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="n">view2</span><span class="o">.</span><span class="n">_base</span> <span class="ow">or</span> <span class="n">view1</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="n">view2</span> <span class="ow">or</span> <span class="n">view1</span> <span class="ow">is</span> <span class="n">view2</span><span class="o">.</span><span class="n">_base</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="kc">False</span>


<span class="k">def</span> <span class="nf">same_dtype_views</span><span class="p">(</span><span class="n">view1</span><span class="p">,</span> <span class="n">view2</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">view1</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">view2</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">view1</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">view1</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">view1</span><span class="o">.</span><span class="n">_base</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">view2</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">view2</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">view2</span><span class="o">.</span><span class="n">_base</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="kc">True</span>


<span class="c1"># Note [Handling mutations on an input that aliases other inputs]</span>
<span class="c1"># The easiest example to show-case this edge case is here:</span>
<span class="c1">#</span>
<span class="c1"># def f(a, b):</span>
<span class="c1">#     a.mul_(2)</span>
<span class="c1">#     out = a + b</span>
<span class="c1">#     return out</span>
<span class="c1">#</span>
<span class="c1"># In this situation, if a and b happened to be aliased, we need to trace something different!</span>
<span class="c1"># Suppose we had b = a.view(-1)</span>
<span class="c1"># (In this case, that means that `a._base is b`)</span>
<span class="c1">#</span>
<span class="c1"># We need to ensure that the aliasing relationship between a and b is preserved.</span>
<span class="c1"># We do that detecting the specific situation above (mutate an input that aliases another input),</span>
<span class="c1"># and when we do that, we create a synthetic base argument. Then inside of the traced forward,</span>
<span class="c1"># we regenerate a and b off of that base.</span>
<span class="c1"># The complete example of the transformed function looks like this:</span>
<span class="c1">#</span>
<span class="c1"># // The traced forward takes in a synthetic base, and regenerates the aliased inputs as views</span>
<span class="c1"># // We could consider getting view-replay support here to minimize as_strided_scatter ops in the graph</span>
<span class="c1"># def traced_forward(base):</span>
<span class="c1">#     a = base.as_strided(...)</span>
<span class="c1">#     b = base.as_strided(...)</span>
<span class="c1">#     a_updated = a.mul(2)</span>
<span class="c1">#     base_updated = torch.as_strided_scatter(base, a_updated, ...)</span>
<span class="c1">#     b_updated = base_updated.as_strided(...)</span>
<span class="c1">#     out = a_updated + b_updated</span>
<span class="c1">#     return a_updated, out</span>
<span class="c1">#</span>
<span class="c1"># def compiled_fn(a, b):</span>
<span class="c1">#     // we detect that a is the &quot;differentiable base&quot; here</span>
<span class="c1">#     base = a</span>
<span class="c1">#     // In other situations, we might do either:</span>
<span class="c1">#     // (1) a and b are both views off of some larger differentiable base</span>
<span class="c1">#     //     assert a._base is b._base and a._base is not None</span>
<span class="c1">#     //     base = a._base</span>
<span class="c1">#     // (2) a and b both don&#39;t require gradients. Create a base from the storage</span>
<span class="c1">#     //     assert a._base is None and b._base is None</span>
<span class="c1">#     //     base = torch.Tensor(a.storage())</span>
<span class="c1">#     a_updated, out = traced_forward(base)</span>
<span class="c1">#     a.copy_(a_updated)</span>
<span class="c1">#     return out</span>
<span class="c1">#</span>
<span class="c1"># This function:</span>
<span class="c1"># (1) Merges input views into a synthetic base argument, when any of those input views are mutated</span>
<span class="c1"># (2) Returns metadata telling the autograd.Function how to modify their arguments properly,</span>
<span class="c1">#     to respect the new calling convention.</span>
<span class="c1">#</span>
<span class="c1"># The calling convention is as follows.</span>
<span class="c1"># Any inputs that were originally views of one another get yanked, and replaced with a synthetic base.</span>
<span class="c1"># The argument list ordering goes [base1, ..., baseN], [arg1, ..., argN],</span>
<span class="c1"># Where the ordering of the bases is determined from the ordering of the original view args.</span>
<span class="c1"># baseA will come before baseB if the earliest original argument coming from baseA</span>
<span class="c1"># showed up earlier in the argument list than the earliest original argument coming from baseB.</span>
<span class="c1">#</span>
<span class="c1"># Example, given some tensors a, b, c, d</span>
<span class="c1"># call site:</span>
<span class="c1">#   f(a, c.view(-1), b.view(-1), b, c, d)</span>
<span class="c1"># Modified argument list:</span>
<span class="c1">#   c_base comes first because the first c view came earlier in arg list than the first b view</span>
<span class="c1">#   b_base = torch.Tensor(b.storage())</span>
<span class="c1">#   c_base = torch.Tensor(c.storage())</span>
<span class="c1">#   f(c_base, b_base, a, d)</span>
<span class="k">def</span> <span class="nf">merge_view_inputs</span><span class="p">(</span>
    <span class="n">fwd_inputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">mutated_input_info</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">MutationType</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">]]]]]]:</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">fwd_inputs</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">mutated_input_info</span><span class="p">)</span>
    <span class="n">storage_ref_to_idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">StorageWeakRef</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
    <span class="n">base_args</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">other_args</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">inpt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">fwd_inputs</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inpt</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="n">storage_ref</span> <span class="o">=</span> <span class="n">StorageWeakRef</span><span class="p">(</span><span class="n">inpt</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">())</span>
            <span class="n">storage_ref_to_idx</span><span class="p">[</span><span class="n">storage_ref</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">other_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inpt</span><span class="p">)</span>
    <span class="c1"># This list contains metadata that tells you what the i&#39;th argument in the inner calling convention should be.</span>
    <span class="c1"># It&#39;s either:</span>
    <span class="c1"># - another int (corresponding to the index in the argument list of the element from the outer calling convention)</span>
    <span class="c1"># - idx, *args, where we can generate the new output with old_args[idx].as_strided(*args)</span>
    <span class="c1">#   idx corresponds to which synthetic base from the outer calling context to view</span>
    <span class="n">inner_calling_convention_meta</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]]]]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">aliased_input_indices</span> <span class="ow">in</span> <span class="n">storage_ref_to_idx</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">aliased_input_indices</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">any</span><span class="p">(</span>
            <span class="c1"># We only care about mutations that affect all aliases,</span>
            <span class="c1"># so metadata mutations on an input doesn&#39;t require us to do synthetic base handling.</span>
            <span class="n">mutated_input_info</span><span class="p">[</span><span class="n">inpt_idx</span><span class="p">]</span> <span class="o">==</span> <span class="n">MutationType</span><span class="o">.</span><span class="n">data</span>
            <span class="k">for</span> <span class="n">inpt_idx</span> <span class="ow">in</span> <span class="n">aliased_input_indices</span>
        <span class="p">):</span>
            <span class="c1"># We detected an input that was mutated, AND aliases with another input.</span>
            <span class="c1"># we need to replace this set of aliased inputs with a single synthetic base.</span>
            <span class="c1"># For now, I&#39;m banning a bunch of cases. We expect dynamo to properly detect these cases</span>
            <span class="c1"># and error out. We can fix them later.</span>
            <span class="k">for</span> <span class="n">idx1</span><span class="p">,</span> <span class="n">idx2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">aliased_input_indices</span><span class="p">,</span> <span class="n">aliased_input_indices</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
                <span class="n">view1</span> <span class="o">=</span> <span class="n">fwd_inputs</span><span class="p">[</span><span class="n">idx1</span><span class="p">]</span>
                <span class="n">view2</span> <span class="o">=</span> <span class="n">fwd_inputs</span><span class="p">[</span><span class="n">idx2</span><span class="p">]</span>
                <span class="c1"># The &quot;inputs that are aliased but have different differentiable bases&quot; case</span>
                <span class="c1"># is more complicated and hopefully pretty rare. Not currently handled.</span>
                <span class="k">assert</span> <span class="n">are_differentiable_views</span><span class="p">(</span>
                    <span class="n">view1</span><span class="p">,</span> <span class="n">view2</span>
                <span class="p">),</span> <span class="s2">&quot;aot_autograd() does not yet handle non-differentiable view input mutations.&quot;</span>
                <span class="c1"># Regenerating views when reinterpreting complex / real tensors seems non-trivial,</span>
                <span class="c1"># not handling for now</span>
                <span class="k">assert</span> <span class="n">same_dtype_views</span><span class="p">(</span>
                    <span class="n">view1</span><span class="p">,</span> <span class="n">view2</span>
                <span class="p">),</span> <span class="s2">&quot;aot_autograd() does not yet handle input mutations on views with different dtypes.&quot;</span>
            <span class="n">non_none_bases</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">fwd_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">_base</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">aliased_input_indices</span>
                <span class="k">if</span> <span class="n">fwd_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="p">]</span>
            <span class="n">aliases_with_none_bases</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">fwd_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">aliased_input_indices</span>
                <span class="k">if</span> <span class="n">fwd_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">_base</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="p">]</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">non_none_bases</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># Case where none of the aliases require gradients</span>
                <span class="n">example_idx</span> <span class="o">=</span> <span class="n">aliased_input_indices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">synthetic_base</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">fwd_inputs</span><span class="p">[</span><span class="n">example_idx</span><span class="p">]</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">())</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Case where all of the aliases require gradients, and have the same _base.</span>
                <span class="n">synthetic_base</span> <span class="o">=</span> <span class="n">non_none_bases</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">other_base</span> <span class="ow">in</span> <span class="n">non_none_bases</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
                    <span class="k">assert</span> <span class="p">(</span>
                        <span class="n">other_base</span> <span class="ow">is</span> <span class="n">synthetic_base</span>
                    <span class="p">),</span> <span class="s2">&quot;aot_autograd() does not yet handle non-differentiable view input mutations.&quot;</span>
                <span class="k">for</span> <span class="n">alias</span> <span class="ow">in</span> <span class="n">aliases_with_none_bases</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="p">(</span>
                        <span class="n">alias</span> <span class="ow">is</span> <span class="n">synthetic_base</span>
                    <span class="p">),</span> <span class="s2">&quot;aot_autograd() does not yet handle non-differentiable view input mutations.&quot;</span>
            <span class="n">base_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">synthetic_base</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">curr_view_idx</span> <span class="ow">in</span> <span class="n">aliased_input_indices</span><span class="p">:</span>
                <span class="n">curr_view</span> <span class="o">=</span> <span class="n">fwd_inputs</span><span class="p">[</span><span class="n">curr_view_idx</span><span class="p">]</span>
                <span class="n">base_idx</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">base_args</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
                <span class="n">size_</span> <span class="o">=</span> <span class="n">curr_view</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
                <span class="n">stride_</span> <span class="o">=</span> <span class="n">curr_view</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span>
                <span class="n">storage_offset_</span> <span class="o">=</span> <span class="n">curr_view</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span>
                <span class="c1"># We store just enough info here so that we can regenerate the view later.</span>
                <span class="c1"># Regeneration: args[base_idx].as_strided(size_, stride_, storage_offset_)</span>
                <span class="c1"># If we want view replay instead of as_strided() calls, this will need to change.</span>
                <span class="n">inner_calling_convention_meta</span><span class="p">[</span><span class="n">curr_view_idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">base_idx</span><span class="p">,</span>
                    <span class="p">(</span><span class="n">size_</span><span class="p">,</span> <span class="n">stride_</span><span class="p">,</span> <span class="n">storage_offset_</span><span class="p">),</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">curr_idx</span> <span class="ow">in</span> <span class="n">aliased_input_indices</span><span class="p">:</span>
                <span class="n">other_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fwd_inputs</span><span class="p">[</span><span class="n">curr_idx</span><span class="p">])</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">base_args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">other_args</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">fwd_inputs</span><span class="p">)</span>
        <span class="c1"># If no synthetic bases are necessary, just return the original inputs.</span>
        <span class="k">return</span> <span class="n">fwd_inputs</span><span class="p">,</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Otherwise, return:</span>
        <span class="c1"># (1) The new args according to the updated calling convention: (synthetic_bases, other_args)</span>
        <span class="c1"># (2) Metadata telling functionalization how to generate the inner argument list given the outer calling convention.</span>
        <span class="c1">#     We post-process it into a list, where meta[i] tells you info about the i&#39;th argument in the inner calling convention.</span>
        <span class="n">args_to_functionalization</span> <span class="o">=</span> <span class="n">base_args</span> <span class="o">+</span> <span class="n">other_args</span>
        <span class="n">arg_to_old_idx_map</span> <span class="o">=</span> <span class="p">{</span><span class="n">arg</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">arg</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">fwd_inputs</span><span class="p">)}</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">other_arg</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">other_args</span><span class="p">):</span>
            <span class="n">new_idx</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">base_args</span><span class="p">)</span> <span class="o">+</span> <span class="n">i</span>
            <span class="n">old_idx</span> <span class="o">=</span> <span class="n">arg_to_old_idx_map</span><span class="p">[</span><span class="n">other_arg</span><span class="p">]</span>
            <span class="n">inner_calling_convention_meta</span><span class="p">[</span><span class="n">old_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_idx</span>
        <span class="c1"># post process into a list</span>
        <span class="n">post_processed_calling_convention_meta</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[</span>
            <span class="o">-</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inner_calling_convention_meta</span><span class="p">))</span>
        <span class="p">]</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">inner_calling_convention_meta</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">post_processed_calling_convention_meta</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
        <span class="c1"># Quick assert: every argument in the inner calling convention should be accounted for.</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">post_processed_calling_convention_meta</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">x</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="k">return</span> <span class="n">args_to_functionalization</span><span class="p">,</span> <span class="n">post_processed_calling_convention_meta</span>


<span class="k">def</span> <span class="nf">format_guard_bug_msg</span><span class="p">(</span><span class="n">aot_config</span><span class="p">,</span> <span class="n">expected</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;At compilation time, graph </span><span class="si">{</span><span class="n">aot_config</span><span class="o">.</span><span class="n">aot_id</span><span class="si">}</span><span class="s2"> was compiled under the &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;assumption that </span><span class="si">{</span><span class="n">expected</span><span class="si">}</span><span class="s2">, but at runtime this was not the case.  &quot;</span>
        <span class="s2">&quot;This indicates a guard bug in AOTAutograd or Dynamo, please file a bug to PyTorch.&quot;</span>
    <span class="p">)</span>


<span class="c1"># MOTIVATION:</span>
<span class="c1">#</span>
<span class="c1"># When tracing functions for future execution, one must be careful not to pass</span>
<span class="c1"># in the same input tensor multiple times (e.g., f(x, x), as this can result</span>
<span class="c1"># in graphs that are ONLY valid if you later pass a new tensor in exactly the</span>
<span class="c1"># same way (e.g., f(y, y)).  (NB: we really mean duplicate; two distinct</span>
<span class="c1"># tensors that alias each other is a different situation that is covered by</span>
<span class="c1"># aot_dispatch_deduplicated_autograd). Here are two examples:</span>
<span class="c1">#</span>
<span class="c1"># (1) Suppose you have a function:</span>
<span class="c1">#</span>
<span class="c1">#   def f(x, y):</span>
<span class="c1">#       return x + y</span>
<span class="c1">#</span>
<span class="c1"># If you make_fx(f)(x, x), you will trace out:</span>
<span class="c1">#</span>
<span class="c1">#   def f(x, y):</span>
<span class="c1">#       return y + y</span>
<span class="c1">#</span>
<span class="c1"># Oops!</span>
<span class="c1">#</span>
<span class="c1"># (2) For most tensors x and y, you can compute f&#39;s gradient with respect to</span>
<span class="c1"># these to inputs by saying torch.autograd.grad(f(x, y), (x, y)).  However,</span>
<span class="c1"># if x is y, you will trace out a program that gets incorrect gradients:</span>
<span class="c1">#</span>
<span class="c1">#   &gt;&gt;&gt; x = torch.randn(1, requires_grad=True)</span>
<span class="c1">#   &gt;&gt;&gt; torch.autograd.grad(x + x, (x, x))</span>
<span class="c1">#   (tensor([2.]), tensor([2.]))</span>
<span class="c1">#</span>
<span class="c1"># In other words, the gradient is double-counted.  Deduplicating the arguments</span>
<span class="c1"># gives you an appropriate gradient:</span>
<span class="c1">#</span>
<span class="c1">#   &gt;&gt;&gt; y = torch.randn(1, requires_grad=True)</span>
<span class="c1">#   &gt;&gt;&gt; torch.autograd.grad(x + y, (x, y))</span>
<span class="c1">#   (tensor([1.]), tensor([1.]))</span>
<span class="c1">#</span>
<span class="c1"># HOW TO DEDUPLICATE:</span>
<span class="c1">#</span>
<span class="c1"># There are a few strategies, in order of preference:</span>
<span class="c1">#</span>
<span class="c1"># 1. For every duplicate argument to the function, detach it into</span>
<span class="c1">#    a separate leaf tensor, so that it is no longer duplicated.</span>
<span class="c1">#</span>
<span class="c1">#       PRO: The resulting compiled graph works for any configuration</span>
<span class="c1">#       of duplicated arguments.</span>
<span class="c1">#</span>
<span class="c1">#       CON: It does not (naively) work if you mutate the metadata of inputs:</span>
<span class="c1">#</span>
<span class="c1">#           def f(x, y):</span>
<span class="c1">#               x.transpose_(0, 1)</span>
<span class="c1">#               y.transpose_(0, 2)</span>
<span class="c1">#</span>
<span class="c1">#           x = torch.randn(2, 3, 4)</span>
<span class="c1">#           f(x, x)</span>
<span class="c1">#</span>
<span class="c1">#       The ordering of the transposes inside f dictates whether or not</span>
<span class="c1">#       you get [4, 2, 3] or [3, 4, 2].  This means that you cannot precompute</span>
<span class="c1">#       what metadata mutations should get applied to each input; you need to</span>
<span class="c1">#       assume they aren&#39;t duplicates (what we do today) or preserve</span>
<span class="c1">#       the original metadata mutations exactly in order, so that they work</span>
<span class="c1">#       for any duplicate configuration.</span>
<span class="c1">#</span>
<span class="c1">#       CON: It does not (naively) work if you mutate the data of inputs.</span>
<span class="c1">#       In particular, leaf tensors that require grad cannot be mutated,</span>
<span class="c1">#       this makes it impossible to differentiate with respect to the original</span>
<span class="c1">#       base.</span>
<span class="c1">#</span>
<span class="c1"># 2. For every duplicate argument to the function, remove it, so it is</span>
<span class="c1">#    no longer part of the &quot;true&quot; signature:</span>
<span class="c1">#</span>
<span class="c1">#       PRO: Implemented naively, it still works for metadata/data mutation.</span>
<span class="c1">#</span>
<span class="c1">#       CON: The resulting compiled graph is duplicate-specialized: it only</span>
<span class="c1">#       works if future calls duplicate arguments in exactly the same way.</span>
<span class="c1">#       Horribly, Dynamo doesn&#39;t guard on this at the moment.  But even if</span>
<span class="c1">#       it did, you could still end up recompiling a bunch of each duplicate.</span>
<span class="c1">#</span>
<span class="c1"># Our strategy is to do (1) if we can, and do (2) otherwise, erroring if</span>
<span class="c1"># Dynamo&#39;s guards are not enough.  In practice, this seems to cover</span>
<span class="c1"># everything.</span>
<span class="c1">#</span>
<span class="k">def</span> <span class="nf">aot_wrapper_dedupe</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">aot_config</span><span class="p">:</span> <span class="n">AOTConfig</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">compiler_fn</span><span class="p">):</span>
    <span class="c1"># Get information about whether or not flat_fn mutates its arguments</span>
    <span class="c1"># or not</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">enable_python_dispatcher</span><span class="p">():</span>
            <span class="n">fw_metadata</span><span class="p">,</span> <span class="n">_out</span><span class="p">,</span> <span class="n">_num_aliasing_metadata_outs</span> <span class="o">=</span> <span class="n">run_functionalized_fw_and_collect_metadata</span><span class="p">(</span>
                <span class="n">flat_fn</span>
            <span class="p">)(</span><span class="o">*</span><span class="n">flat_args</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">RuntimeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">log</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;Failed to collect metadata on function, produced code may be suboptimal.  &quot;</span>
            <span class="s2">&quot;Known situations this can occur are inference mode only compilation involving &quot;</span>
            <span class="s2">&quot;resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); &quot;</span>
            <span class="s2">&quot;if your situation looks different please file a bug to PyTorch.&quot;</span><span class="p">,</span>
            <span class="n">exc_info</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="c1"># Analysis failed, fall back to duplicate specialize</span>
        <span class="c1"># TODO: Known analysis problems:</span>
        <span class="c1">#   - resize_: TestInductorOpInfoCPU.test_comprehensive_resize__cpu_bool</span>
        <span class="c1">#   - prims: test_tmp_not_defined_issue1_cpu</span>
        <span class="k">pass</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Strategy 1: For any input that is not mutated, we can leafify it if we</span>
        <span class="c1"># need to remove a duplicate.</span>
        <span class="n">leaf_flat_args</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">args_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="n">ok</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">flat_args</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">a</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">args_set</span><span class="p">:</span>
                <span class="n">args_set</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
                <span class="n">leaf_flat_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">fw_metadata</span><span class="o">.</span><span class="n">mutated_input_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">MutationType</span><span class="o">.</span><span class="n">none</span><span class="p">:</span>
                <span class="n">leaf_flat_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">ok</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">break</span>

        <span class="k">if</span> <span class="n">ok</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">compiler_fn</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">,</span> <span class="n">leaf_flat_args</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">)</span>

    <span class="c1"># Strategy 2: Duplicate specialize.</span>
    <span class="c1">#</span>
    <span class="c1"># In Haskell types, suppose you have:</span>
    <span class="c1">#</span>
    <span class="c1">#   add_dupe_args :: DedupedArgs -&gt; Args</span>
    <span class="c1">#   remove_dupe_args :: Args -&gt; DedupedArgs</span>
    <span class="c1">#</span>
    <span class="c1">#   compiler_fn</span>
    <span class="c1">#       :: (DedupedArgs -&gt; R) -&gt; DedupedArgs -&gt; AOTConfig -&gt; (DedupedArgs -&gt; R)</span>
    <span class="c1">#   deped_compiler_fn</span>
    <span class="c1">#       :: (Args -&gt; R) -&gt; Args -&gt; AOTConfig -&gt; (Args -&gt; R)</span>
    <span class="c1">#</span>
    <span class="c1"># Then the code below can be written in point-free style as:</span>
    <span class="c1">#</span>
    <span class="c1">#   deduped_compiler_fn f a c =</span>
    <span class="c1">#       compiler_fn (f . add_dupe_args) (remove_dupe_args a) c . remove_dupe_args</span>
    <span class="c1">#</span>
    <span class="c1"># Suppose you have:</span>
    <span class="c1">#</span>
    <span class="c1">#   [a, b, a, c]</span>
    <span class="c1">#</span>
    <span class="c1"># We want:</span>
    <span class="c1">#</span>
    <span class="c1">#   remove_dupe_args([a, b, a, c]) == [a, b, c]</span>
    <span class="c1">#   add_dupe_args([a, b, c]) == [a, b, a, c]</span>
    <span class="c1">#</span>
    <span class="c1"># This is done via (respectively):</span>
    <span class="c1">#</span>
    <span class="c1">#   seen_args = {a: 0, b: 1, c: 2}</span>
    <span class="c1">#   add_dupe_map = {  # how to get args from the deduped list</span>
    <span class="c1">#       0: 0,</span>
    <span class="c1">#       1: 1,</span>
    <span class="c1">#       2: 0,</span>
    <span class="c1">#       3: 2,</span>
    <span class="c1">#   }</span>
    <span class="c1">#   keep_arg_mask = [True, True, False, True]</span>

    <span class="n">seen_args</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">keep_arg_mask</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">add_dupe_map</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">duped_arg_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)</span>

    <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># index into deduped_flat_args</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">flat_args</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">seen_args</span><span class="p">:</span>
            <span class="n">keep_arg_mask</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">add_dupe_map</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">seen_args</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
            <span class="k">continue</span>
        <span class="n">keep_arg_mask</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">seen_args</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">j</span>
        <span class="n">add_dupe_map</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">j</span>
        <span class="n">j</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">unique_args</span> <span class="o">=</span> <span class="n">j</span>

    <span class="c1"># NB: Hot path, avoid set lookups here</span>
    <span class="c1"># TODO: Can avoid the zip here too, probably</span>
    <span class="k">def</span> <span class="nf">remove_dupe_args</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">t</span> <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">keep</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">keep_arg_mask</span><span class="p">)</span> <span class="k">if</span> <span class="n">keep</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">add_dupe_args</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">args</span><span class="p">[</span><span class="n">add_dupe_map</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">duped_arg_len</span><span class="p">)]</span>

    <span class="n">deduped_flat_args</span> <span class="o">=</span> <span class="n">remove_dupe_args</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)</span>

    <span class="n">tracing_context</span> <span class="o">=</span> <span class="n">TracingContext</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">tracing_context</span><span class="p">:</span>
        <span class="c1"># TODO(voz): This structure is 1:1, we could consider an alternate structure like</span>
        <span class="c1"># kept_pos:[dupe_arg_pos], however, add_dupe_map is 1:1 so we would need a new structure there,</span>
        <span class="c1"># which feels like needless complexity for a tiny bit of efficiency at this point.</span>
        <span class="k">for</span> <span class="n">dupe_arg_pos</span><span class="p">,</span> <span class="n">kept_pos</span> <span class="ow">in</span> <span class="n">add_dupe_map</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="c1"># Edge case, only happens for identity</span>
            <span class="k">if</span> <span class="n">dupe_arg_pos</span> <span class="o">!=</span> <span class="n">kept_pos</span><span class="p">:</span>
                <span class="n">tracing_context</span><span class="o">.</span><span class="n">guards_context</span><span class="o">.</span><span class="n">aotautograd_guards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">DuplicateInputs</span><span class="p">(</span><span class="n">kept_pos</span><span class="p">,</span> <span class="n">dupe_arg_pos</span><span class="p">))</span>

    <span class="nd">@wraps</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">wrapped_flat_fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">flat_fn</span><span class="p">(</span><span class="o">*</span><span class="n">add_dupe_args</span><span class="p">(</span><span class="n">args</span><span class="p">))</span>

    <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">compiler_fn</span><span class="p">(</span><span class="n">wrapped_flat_fn</span><span class="p">,</span> <span class="n">deduped_flat_args</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">compiled_fn</span><span class="p">,</span> <span class="s2">&quot;_boxed_call&quot;</span><span class="p">):</span>
        <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">make_boxed_func</span><span class="p">(</span><span class="n">compiled_fn</span><span class="p">)</span>

    <span class="nd">@wraps</span><span class="p">(</span><span class="n">compiled_fn</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">wrapped_compiled_fn</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="n">deduped_args</span> <span class="o">=</span> <span class="n">remove_dupe_args</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
        <span class="n">args</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">compiled_fn</span><span class="p">(</span><span class="n">deduped_args</span><span class="p">)</span>
    <span class="n">wrapped_compiled_fn</span><span class="o">.</span><span class="n">_boxed_call</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="c1"># This can be uncommented when we properly guard for duplicates,</span>
    <span class="c1"># but right now we must not do it.</span>
    <span class="c1"># if not config.debug_assert:</span>
    <span class="c1">#     return wrapped_compiled_fn</span>

    <span class="nd">@wraps</span><span class="p">(</span><span class="n">wrapped_compiled_fn</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">debugged_compiled_fn</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="c1"># Test that the computed remove/add arg functions are an inverse</span>
        <span class="n">new_args</span> <span class="o">=</span> <span class="n">add_dupe_args</span><span class="p">(</span><span class="n">remove_dupe_args</span><span class="p">(</span><span class="n">args</span><span class="p">))</span>
        <span class="n">seen</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">new_args</span><span class="p">,</span> <span class="n">args</span><span class="p">)):</span>
            <span class="n">seen</span><span class="p">[</span><span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">assert</span> <span class="n">x</span> <span class="ow">is</span> <span class="n">y</span><span class="p">,</span> <span class="n">format_guard_bug_msg</span><span class="p">(</span>
                <span class="n">aot_config</span><span class="p">,</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">describe_input</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">)</span><span class="si">}</span><span class="s2"> would be a duplicate of &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">describe_input</span><span class="p">(</span><span class="n">add_dupe_map</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">aot_config</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="c1"># This is only an error if there is metadata mutation on both of</span>
        <span class="c1"># the duped arguments; in this case, we need to know what order</span>
        <span class="c1"># the metadata mutation applies in.  You&#39;ll get the correct result</span>
        <span class="c1"># otherwise, because a graph that assumes distinct inputs works if</span>
        <span class="c1"># you dupe the inputs (the gradient contributions from each input</span>
        <span class="c1"># will get summed up appropriately.)</span>
        <span class="c1">#</span>
        <span class="c1"># TODO: work out how to setup this assert correctly</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        assert len(seen) == unique_args, format_guard_bug_msg(aot_config,</span>
<span class="sd">            f&quot;there would be {unique_args} distinct arguments&quot;</span>
<span class="sd">        )</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">wrapped_compiled_fn</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
    <span class="n">debugged_compiled_fn</span><span class="o">.</span><span class="n">_boxed_call</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">return</span> <span class="n">debugged_compiled_fn</span>


<span class="k">def</span> <span class="nf">describe_input</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">num_params_buffers</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;parameter/buffer </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;input </span><span class="si">{</span><span class="n">i</span> <span class="o">-</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">num_params_buffers</span><span class="si">}</span><span class="s2">&quot;</span>


<span class="c1"># Has the precondition that there</span>
<span class="c1"># are no duplicate arguments in flat_args (e.g., the same Tensor</span>
<span class="c1"># object never shows up twice.  However, two tensor inputs MAY alias</span>
<span class="c1"># the same storage, so long as they have separate TensorImpls.)</span>
<span class="k">def</span> <span class="nf">aot_dispatch_autograd</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">aot_config</span><span class="p">:</span> <span class="n">AOTConfig</span><span class="p">):</span>

    <span class="k">with</span> <span class="n">enable_python_dispatcher</span><span class="p">():</span>
        <span class="p">(</span>
            <span class="n">_fw_metadata</span><span class="p">,</span>
            <span class="n">out</span><span class="p">,</span>
            <span class="n">_num_aliasing_metadata_outs</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">=</span> <span class="n">run_functionalized_fw_and_collect_metadata</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">)(</span><span class="o">*</span><span class="n">flat_args</span><span class="p">)</span>

    <span class="c1"># pre-compute, so we can bail out quickly in the hotpath</span>
    <span class="n">_num_outputs_aliased_to_inputs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">x</span>
            <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">_fw_metadata</span><span class="o">.</span><span class="n">aliased_output_info</span>
            <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">output_type</span> <span class="o">==</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_input</span>
        <span class="p">]</span>
    <span class="p">)</span>
    <span class="n">_num_outputs_aliased_to_intermediates</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">x</span>
            <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">_fw_metadata</span><span class="o">.</span><span class="n">aliased_output_info</span>
            <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">output_type</span> <span class="o">==</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_intermediate</span>
        <span class="p">]</span>
    <span class="p">)</span>
    <span class="n">_num_mutated_data_inputs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span>
        <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">_fw_metadata</span><span class="o">.</span><span class="n">mutated_input_info</span> <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="n">MutationType</span><span class="o">.</span><span class="n">data</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="n">_num_mutated_metadata_only_inputs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span>
        <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">_fw_metadata</span><span class="o">.</span><span class="n">metadata_mutation_input_info</span> <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="n">_num_mutated_inputs</span> <span class="o">=</span> <span class="n">_num_mutated_data_inputs</span> <span class="o">+</span> <span class="n">_num_mutated_metadata_only_inputs</span>


    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="n">_num_non_aliased_outs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="n">_num_mutated_data_inputs</span><span class="p">:])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">_num_non_aliased_outs</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">assert</span> <span class="p">(</span>
        <span class="nb">len</span><span class="p">(</span><span class="n">_fw_metadata</span><span class="o">.</span><span class="n">requires_grad_out_info</span><span class="p">)</span>
        <span class="o">==</span> <span class="n">_num_mutated_data_inputs</span> <span class="o">+</span> <span class="n">_num_non_aliased_outs</span>
    <span class="p">)</span>

    <span class="c1"># out here corresponds to the set of outputs that should be returned by the traced forward call.</span>
    <span class="c1"># It includes outputs of the original forward, *and* any updated inputs due to input mutations.</span>
    <span class="c1"># However, it does *not* include any outputs that are aliases of inputs, or any metadata-only input mutations.</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">x</span><span class="p">,</span>
        <span class="n">out</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># This code only executes if we have graph inputs that alias each other, and one of those inputs</span>
    <span class="c1"># gets its data mutated.</span>
    <span class="c1"># When that happens, we replace the aliased inputs with a synthetic base, and in the traced forward</span>
    <span class="c1"># we later generate the input views</span>
    <span class="n">flat_args_with_views_handled</span><span class="p">,</span> <span class="n">_synthetic_base_info</span> <span class="o">=</span> <span class="n">merge_view_inputs</span><span class="p">(</span>
        <span class="n">flat_args</span><span class="p">,</span> <span class="n">_fw_metadata</span><span class="o">.</span><span class="n">mutated_input_info</span>
    <span class="p">)</span>

    <span class="n">joint_forward_backward</span> <span class="o">=</span> <span class="n">create_joint_forward_backward_functionalized</span><span class="p">(</span>
        <span class="n">flat_fn</span><span class="p">,</span>
        <span class="n">meta</span><span class="o">=</span><span class="n">_fw_metadata</span><span class="p">,</span>
        <span class="n">synthetic_base_info</span><span class="o">=</span><span class="n">_synthetic_base_info</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">joint_inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">flat_args_with_views_handled</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>

    <span class="n">disable_amp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_is_any_autocast_enabled</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">use_functionalize</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">enable_python_dispatcher</span><span class="p">():</span>
            <span class="n">flattened_joints</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">(</span><span class="n">joint_inputs</span><span class="p">)</span>
            <span class="n">fx_g</span> <span class="o">=</span> <span class="n">make_fx</span><span class="p">(</span><span class="n">joint_forward_backward</span><span class="p">,</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span><span class="p">)(</span>
                <span class="o">*</span><span class="n">joint_inputs</span>
            <span class="p">)</span>

        <span class="c1"># There should be *NO* mutating ops in the graph at this point.</span>
        <span class="n">assert_functional_graph</span><span class="p">(</span><span class="n">fx_g</span><span class="o">.</span><span class="n">graph</span><span class="p">)</span>
        <span class="c1"># Redudant with the check above, but worth having in case tracing introduced</span>
        <span class="c1"># a fake tensor. Unlikely.</span>
        <span class="c1"># See Note: [Fake Modules and AOTAutograd]</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">assert_no_fake_params_or_buffers</span><span class="p">(</span><span class="n">fx_g</span><span class="p">)</span>
        <span class="n">fx_g</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">eliminate_dead_code</span><span class="p">()</span>
        <span class="n">fx_g</span><span class="o">.</span><span class="n">recompile</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># joint_forward_backward() now always runs with functionalization, and factoring it out</span>
        <span class="c1"># to make that toggleable is a bit painful.</span>
        <span class="c1"># aot autograd without functionalization is wrong anyway, so we error.</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
            <span class="s2">&quot;Graph partitioning without functionalization is not sound, we may introduce errors&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">debug_joint</span><span class="p">:</span>
        <span class="n">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;====== Joint graph </span><span class="si">{</span><span class="n">aot_config</span><span class="o">.</span><span class="n">aot_id</span><span class="si">}</span><span class="s2"> ======&quot;</span><span class="p">)</span>
        <span class="n">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="n">fx_g</span><span class="o">.</span><span class="n">print_readable</span><span class="p">(</span><span class="n">print_output</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">with</span> <span class="n">track_graph_compiling</span><span class="p">(</span><span class="n">aot_config</span><span class="p">,</span> <span class="s2">&quot;joint&quot;</span><span class="p">):</span>
            <span class="n">num_inner_fwd_outputs</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">_num_mutated_data_inputs</span>
                <span class="o">+</span> <span class="n">_num_non_aliased_outs</span>
                <span class="o">+</span> <span class="n">_num_aliasing_metadata_outs</span>
            <span class="p">)</span>
            <span class="n">fw_module</span><span class="p">,</span> <span class="n">bw_module</span> <span class="o">=</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">partition_fn</span><span class="p">(</span>
                <span class="n">fx_g</span><span class="p">,</span> <span class="n">joint_inputs</span><span class="p">,</span> <span class="n">num_fwd_outputs</span><span class="o">=</span><span class="n">num_inner_fwd_outputs</span>
            <span class="p">)</span>
            <span class="n">fw_outs</span> <span class="o">=</span> <span class="p">[</span><span class="n">n</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">fw_module</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span> <span class="k">if</span> <span class="n">n</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s2">&quot;output&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="c1"># we only need to bookkeep the symints that are saved for bw, not any symints</span>
            <span class="c1"># the user forward might have returned in its own output</span>
            <span class="n">fw_outs_saved_for_bw</span> <span class="o">=</span> <span class="n">fw_outs</span><span class="p">[</span><span class="n">num_inner_fwd_outputs</span><span class="p">:]</span>
            <span class="n">symint_outs_saved_for_bw</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">n</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">fw_outs_saved_for_bw</span> <span class="k">if</span> <span class="n">is_sym_node</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
            <span class="p">]</span>
            <span class="n">_num_symints_saved_for_bw</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">symint_outs_saved_for_bw</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">debug_graphs</span><span class="p">:</span>
            <span class="n">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;====== Forward graph </span><span class="si">{aot_config.aot_id}</span><span class="s2"> ======&quot;</span><span class="p">)</span>
            <span class="n">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="n">fw_module</span><span class="o">.</span><span class="n">print_readable</span><span class="p">(</span><span class="n">print_output</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

        <span class="k">with</span> <span class="n">track_graph_compiling</span><span class="p">(</span><span class="n">aot_config</span><span class="p">,</span> <span class="s2">&quot;forward&quot;</span><span class="p">):</span>
            <span class="n">compiled_fw_func</span> <span class="o">=</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">fw_compiler</span><span class="p">(</span>
                <span class="n">fw_module</span><span class="p">,</span> <span class="n">flat_args_with_views_handled</span>
            <span class="p">)</span>

    <span class="k">class</span> <span class="nc">CompiledFunction</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
        <span class="n">compiled_fw</span> <span class="o">=</span> <span class="n">compiled_fw_func</span>
        <span class="n">compiled_bw</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># Corresponds to number of outs (not including updated inputs returns as outs),</span>
        <span class="c1"># *and* not including outs that are aliases of inputs</span>
        <span class="n">num_non_aliased_outs</span> <span class="o">=</span> <span class="n">_num_non_aliased_outs</span>
        <span class="n">num_symints_saved_for_bw</span> <span class="o">=</span> <span class="n">_num_symints_saved_for_bw</span>
        <span class="c1"># Corresponds to number of inputs that are mutated (both metadata only, and data)</span>
        <span class="n">num_mutated_inputs</span> <span class="o">=</span> <span class="n">_num_mutated_inputs</span>
        <span class="c1"># Corresponds to number of inputs that only have their metadata mutated</span>
        <span class="n">num_mutated_data_inputs</span> <span class="o">=</span> <span class="n">_num_mutated_data_inputs</span>
        <span class="c1"># Corresponds to number of inputs that get their metadata (but not data) mutated</span>
        <span class="c1"># We don&#39;t return these in the compiled fw, and instead we stash enough info</span>
        <span class="c1"># to replay the metadata mutations later.</span>
        <span class="n">num_mutated_metadata_only_inputs</span> <span class="o">=</span> <span class="n">_num_mutated_metadata_only_inputs</span>
        <span class="c1"># Corresponds to number of outputs in the original fw that are aliases of inputs</span>
        <span class="c1"># (These are all not returned by the compiled forward, and instead they are manually</span>
        <span class="c1"># created in the epilogue)</span>
        <span class="n">num_outputs_aliased_to_inputs</span> <span class="o">=</span> <span class="n">_num_outputs_aliased_to_inputs</span>
        <span class="c1"># Corresponds to the number of user outputs that alias intermediates (aka graph outputs).</span>
        <span class="n">num_outputs_aliased_to_intermediates</span> <span class="o">=</span> <span class="n">_num_outputs_aliased_to_intermediates</span>
        <span class="c1"># For every output that aliases and input, and every input that gets only its metadata mutated,</span>
        <span class="c1"># we return that tensor&#39;s size/stride/storage_offset directly at the end of the compiled forward,</span>
        <span class="c1"># as a big list of ints.</span>
        <span class="c1"># The number is tracked here.</span>
        <span class="n">num_aliasing_metadata_outs</span> <span class="o">=</span> <span class="n">_num_aliasing_metadata_outs</span>
        <span class="n">synthetic_base_info</span> <span class="o">=</span> <span class="n">_synthetic_base_info</span>
        <span class="n">fw_metadata</span> <span class="o">=</span> <span class="n">_fw_metadata</span>

        <span class="nd">@staticmethod</span>
        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">deduped_flat_tensor_args</span><span class="p">):</span>

            <span class="c1"># There is a pretty complicated calling convention around what the compiled fw returns.</span>
            <span class="c1"># The full list of outputs and their relative order is:</span>
            <span class="c1"># (*mutated_data_inputs, *non_aliased_fw_outs, *saved_tensors, *saved_symints)</span>
            <span class="c1"># - Note that in the synthetic bases case, mutated_inputs will correspond to an updated version</span>
            <span class="c1">#   of the original view, and not the synthetic base</span>
            <span class="n">fw_outs</span> <span class="o">=</span> <span class="n">call_func_with_args</span><span class="p">(</span>
                <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">compiled_fw</span><span class="p">,</span>
                <span class="n">deduped_flat_tensor_args</span><span class="p">,</span>
                <span class="n">disable_amp</span><span class="o">=</span><span class="n">disable_amp</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">num_non_aliased_outs</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">num_non_aliased_outs</span>
            <span class="n">num_aliasing_metadata_outs</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">num_aliasing_metadata_outs</span>
            <span class="n">num_symints_saved_for_bw</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">num_symints_saved_for_bw</span>
            <span class="n">num_mutated_data_inputs</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">num_mutated_data_inputs</span>
            <span class="c1"># Our forward() returns both (mutated_inputs, outputs, output_alias_meta, saved_tensors, saved_symints)</span>
            <span class="n">num_forward_returns</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">num_mutated_data_inputs</span>
                <span class="o">+</span> <span class="n">num_non_aliased_outs</span>
                <span class="o">+</span> <span class="n">num_aliasing_metadata_outs</span>
            <span class="p">)</span>
            <span class="n">num_forward_returns_not_including_alias_meta</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">num_mutated_data_inputs</span> <span class="o">+</span> <span class="n">num_non_aliased_outs</span>
            <span class="p">)</span>

            <span class="c1"># Partitioners must put symint arguments at the end separate from tensor arguments</span>
            <span class="k">if</span> <span class="n">num_symints_saved_for_bw</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">tensors_saved_for_backwards</span> <span class="o">=</span> <span class="n">fw_outs</span><span class="p">[</span>
                    <span class="n">num_forward_returns</span><span class="p">:</span><span class="o">-</span><span class="n">num_symints_saved_for_bw</span>
                <span class="p">]</span>
                <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span>
                    <span class="p">[</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">tensors_saved_for_backwards</span><span class="p">]</span>
                <span class="p">)</span>
                <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="o">*</span><span class="n">tensors_saved_for_backwards</span><span class="p">)</span>
                <span class="n">symint_outs</span> <span class="o">=</span> <span class="n">fw_outs</span><span class="p">[</span><span class="o">-</span><span class="n">num_symints_saved_for_bw</span><span class="p">:]</span>
                <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">SymInt</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">SymFloat</span><span class="p">))</span>
                        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">symint_outs</span>
                    <span class="p">]</span>
                <span class="p">)</span>
                <span class="n">ctx</span><span class="o">.</span><span class="n">symints</span> <span class="o">=</span> <span class="n">symint_outs</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="o">*</span><span class="n">fw_outs</span><span class="p">[</span><span class="n">num_forward_returns</span><span class="p">:])</span>
                <span class="n">ctx</span><span class="o">.</span><span class="n">symints</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="n">fw_outs_not_requiring_grad</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">x</span>
                <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
                    <span class="n">fw_outs</span><span class="p">[:</span><span class="n">num_forward_returns_not_including_alias_meta</span><span class="p">]</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
                <span class="ow">and</span> <span class="ow">not</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">requires_grad_out_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="p">]</span>
            <span class="n">fw_out_ids_requiring_grad</span> <span class="o">=</span> <span class="p">[</span>
                <span class="nb">id</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
                    <span class="n">fw_outs</span><span class="p">[:</span><span class="n">num_forward_returns_not_including_alias_meta</span><span class="p">]</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
                <span class="ow">and</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">requires_grad_out_info</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="p">]</span>

            <span class="n">ctx</span><span class="o">.</span><span class="n">mark_non_differentiable</span><span class="p">(</span><span class="o">*</span><span class="n">fw_outs_not_requiring_grad</span><span class="p">)</span>

            <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">fw_outs</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">num_forward_returns</span><span class="p">])</span>

        <span class="nd">@staticmethod</span>
        <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">all_flat_args</span><span class="p">):</span>
            <span class="c1"># Calling convention: we expect a grad_out passed to the backward:</span>
            <span class="c1"># - for every output of the fw that does *not* alias an input</span>
            <span class="c1"># - for every updated_input generated by the fw that does *not* alias an input</span>
            <span class="c1"># - for every size/stride metadata value for aliased outputs.</span>
            <span class="c1">#   These are returned by the forward, but we just drop them in the backward.</span>
            <span class="c1">#   We need to return them in the forward, but unfortunately there&#39;s no way to specify</span>
            <span class="c1">#   in autograd.Function that certain non-tensor forward outputs shouldn&#39;t show up in the backward.</span>
            <span class="n">expected_grad_outs</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">num_non_aliased_outs</span>
                <span class="o">+</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">num_mutated_data_inputs</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">num_aliasing_metadata_outs</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">flat_args</span> <span class="o">=</span> <span class="n">all_flat_args</span><span class="p">[</span>
                    <span class="p">:</span> <span class="o">-</span><span class="n">CompiledFunction</span><span class="o">.</span><span class="n">num_aliasing_metadata_outs</span>
                <span class="p">]</span>
                <span class="n">metadata_args</span> <span class="o">=</span> <span class="n">all_flat_args</span><span class="p">[</span>
                    <span class="o">-</span><span class="n">CompiledFunction</span><span class="o">.</span><span class="n">num_aliasing_metadata_outs</span> <span class="p">:</span>
                <span class="p">]</span>
                <span class="c1"># metadata args are all ints/symints, which autograd will send Nones for as grad_outputs in the bw</span>
                <span class="k">assert</span> <span class="nb">all</span><span class="p">([</span><span class="n">x</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">metadata_args</span><span class="p">])</span>
                <span class="c1"># delete</span>
                <span class="c1"># for out_idx, (base_sizes, base_strides, base_storage_offset) in CompiledFunctions.fw_out_base_metadata.items():</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="n">flat_args</span> <span class="o">=</span> <span class="n">all_flat_args</span>

            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)</span> <span class="o">==</span> <span class="n">expected_grad_outs</span>
            <span class="n">contiguous_args</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">t</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">else</span> <span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">flat_args</span>
            <span class="p">]</span>
            <span class="n">all_args</span> <span class="o">=</span> <span class="p">(</span>
                <span class="nb">list</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">symints</span><span class="p">)</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span><span class="p">)</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">contiguous_args</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="k">del</span> <span class="n">contiguous_args</span>
            <span class="k">if</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">compiled_bw</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># TODO - pass in fake tensors ?</span>
                <span class="n">context</span> <span class="o">=</span> <span class="n">disable_autocast_manager</span> <span class="k">if</span> <span class="n">disable_amp</span> <span class="k">else</span> <span class="n">nullcontext</span>
                <span class="k">with</span> <span class="n">context</span><span class="p">(),</span> <span class="n">track_graph_compiling</span><span class="p">(</span><span class="n">aot_config</span><span class="p">,</span> <span class="s2">&quot;backward&quot;</span><span class="p">):</span>
                    <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">compiled_bw</span> <span class="o">=</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">bw_compiler</span><span class="p">(</span>
                        <span class="n">bw_module</span><span class="p">,</span> <span class="n">all_args</span>
                    <span class="p">)</span>

            <span class="n">ctx</span><span class="o">.</span><span class="n">maybe_clear_saved_tensors</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">call_func_with_args</span><span class="p">(</span>
                <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">compiled_bw</span><span class="p">,</span>
                <span class="n">all_args</span><span class="p">,</span>
                <span class="n">steal_args</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">disable_amp</span><span class="o">=</span><span class="n">disable_amp</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

    <span class="nd">@wraps</span><span class="p">(</span><span class="n">CompiledFunction</span><span class="o">.</span><span class="n">apply</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">compiled_function</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="c1"># Step 2: remove aliased inputs that are mutated, replace with synthetic bases</span>
        <span class="c1"># Only happens if our graph mutates an input that aliases another input.</span>
        <span class="k">if</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">synthetic_base_info</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Given: the original args, including at least one pair of inputs that are aliased</span>
            <span class="c1"># and get subsequently mutated.</span>
            <span class="c1"># Generate: the updated args, including (potentially multiple) synthetic bases</span>
            <span class="c1"># that replace the views. The input views are regenerated manually in the compiled function.</span>
            <span class="c1"># TODO: think harder about what happens if (a view of) one of these mutated input views is ALSO returned</span>
            <span class="n">new_inputs</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="n">merge_view_inputs</span><span class="p">(</span>
                <span class="n">args</span><span class="p">,</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">mutated_input_info</span>
            <span class="p">)</span>
            <span class="c1"># We&#39;re just re-running the original-args-to-synthetic-base transformation</span>
            <span class="c1"># that we ran during compilation.</span>
            <span class="c1"># This returns metadata that we use during tracing to recover the input views,</span>
            <span class="c1"># which we don&#39;t actually need at runtime.</span>
            <span class="k">assert</span> <span class="n">metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="n">args_with_synthetic_bases</span> <span class="o">=</span> <span class="n">new_inputs</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">args_with_synthetic_bases</span> <span class="o">=</span> <span class="n">args</span>

        <span class="n">all_outs</span> <span class="o">=</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="o">*</span><span class="n">args_with_synthetic_bases</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">num_aliasing_metadata_outs</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">outs</span> <span class="o">=</span> <span class="n">all_outs</span><span class="p">[:</span> <span class="o">-</span><span class="n">CompiledFunction</span><span class="o">.</span><span class="n">num_aliasing_metadata_outs</span><span class="p">]</span>
            <span class="n">aliasing_metadata_outs</span> <span class="o">=</span> <span class="n">all_outs</span><span class="p">[</span>
                <span class="o">-</span><span class="n">CompiledFunction</span><span class="o">.</span><span class="n">num_aliasing_metadata_outs</span> <span class="p">:</span>
            <span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">outs</span> <span class="o">=</span> <span class="n">all_outs</span>
            <span class="n">aliasing_metadata_outs</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">assert</span> <span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">all_outs</span><span class="p">)</span>
            <span class="o">==</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">num_mutated_data_inputs</span>
            <span class="o">+</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">num_non_aliased_outs</span>
            <span class="o">+</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">num_aliasing_metadata_outs</span>
        <span class="p">)</span>

        <span class="c1"># Step 3: After running the compiled fw, apply updates to mutated inputs</span>
        <span class="k">if</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">num_mutated_inputs</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Calling convention: (mutated_inputs, real_outs, aliasing_metadata)</span>

            <span class="k">if</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">num_mutated_data_inputs</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">updated_inputs</span> <span class="o">=</span> <span class="n">outs</span><span class="p">[:</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">num_mutated_data_inputs</span><span class="p">]</span>
                <span class="n">fw_outs</span> <span class="o">=</span> <span class="n">outs</span><span class="p">[</span><span class="n">CompiledFunction</span><span class="o">.</span><span class="n">num_mutated_data_inputs</span> <span class="p">:]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">updated_inputs</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">fw_outs</span> <span class="o">=</span> <span class="n">outs</span>

            <span class="n">curr_mutated_inpt_idx</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">inpt_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">mutation_type</span><span class="p">,</span> <span class="n">metadata_mutation_info</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
                <span class="nb">zip</span><span class="p">(</span>
                    <span class="c1"># TODO: I should merge these two pieces of state</span>
                    <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">mutated_input_info</span><span class="p">,</span>
                    <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">metadata_mutation_input_info</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">):</span>
                <span class="k">if</span> <span class="n">mutation_type</span> <span class="o">==</span> <span class="n">MutationType</span><span class="o">.</span><span class="n">none</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="n">original_inpt</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="n">inpt_idx</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">mutation_type</span> <span class="o">==</span> <span class="n">MutationType</span><span class="o">.</span><span class="n">metadata_only</span><span class="p">:</span>
                    <span class="c1"># We need to grab the size/stride/storage_offset from the compiled forward,</span>
                    <span class="c1"># and use that to mutate the metadata of the input</span>
                    <span class="n">expected_meta</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">metadata_mutation_input_info</span><span class="p">[</span>
                            <span class="n">inpt_idx</span>
                        <span class="p">]</span>
                    <span class="p">)</span>
                    <span class="k">assert</span> <span class="n">expected_meta</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                    <span class="n">fake_meta</span> <span class="o">=</span> <span class="n">expected_meta</span><span class="o">.</span><span class="n">tensor_meta</span>
                    <span class="n">size_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">fake_meta</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
                    <span class="n">stride_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">fake_meta</span><span class="o">.</span><span class="n">stride</span><span class="p">())</span>
                    <span class="n">size_</span> <span class="o">=</span> <span class="n">aliasing_metadata_outs</span><span class="p">[</span>
                        <span class="n">expected_meta</span><span class="o">.</span><span class="n">sizes_idx</span> <span class="p">:</span> <span class="n">expected_meta</span><span class="o">.</span><span class="n">sizes_idx</span> <span class="o">+</span> <span class="n">size_len</span>
                    <span class="p">]</span>
                    <span class="n">stride_</span> <span class="o">=</span> <span class="n">aliasing_metadata_outs</span><span class="p">[</span>
                        <span class="n">expected_meta</span><span class="o">.</span><span class="n">strides_idx</span> <span class="p">:</span> <span class="n">expected_meta</span><span class="o">.</span><span class="n">strides_idx</span>
                        <span class="o">+</span> <span class="n">stride_len</span>
                    <span class="p">]</span>
                    <span class="n">storage_offset_</span> <span class="o">=</span> <span class="n">aliasing_metadata_outs</span><span class="p">[</span>
                        <span class="n">expected_meta</span><span class="o">.</span><span class="n">storage_offset_idx</span>
                    <span class="p">]</span>
                    <span class="n">original_inpt</span><span class="o">.</span><span class="n">as_strided_</span><span class="p">(</span><span class="n">size_</span><span class="p">,</span> <span class="n">stride_</span><span class="p">,</span> <span class="n">storage_offset_</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">updated_inpt</span> <span class="o">=</span> <span class="n">updated_inputs</span><span class="p">[</span><span class="n">curr_mutated_inpt_idx</span><span class="p">]</span>
                    <span class="n">curr_mutated_inpt_idx</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="c1"># TODO: handle resize_() on inputs to a larger size.</span>
                    <span class="c1"># This is actually non-trivial to detect, so we should probably just handle it</span>
                    <span class="c1"># (or make dynamo detect).</span>
                    <span class="c1"># We can&#39;t just check of original_inpt.storage_size != updated_inpt.storage_size,</span>
                    <span class="c1"># Because the original_inpt might be a view of some larger tensor,</span>
                    <span class="c1"># and updated_inpt is always densely packed.</span>
                    <span class="k">if</span> <span class="p">(</span>
                        <span class="n">original_inpt</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">!=</span> <span class="n">updated_inpt</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
                        <span class="ow">or</span> <span class="n">original_inpt</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span> <span class="o">!=</span> <span class="n">updated_inpt</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span>
                        <span class="ow">or</span> <span class="n">original_inpt</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span>
                        <span class="o">!=</span> <span class="n">updated_inpt</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span>
                    <span class="p">):</span>
                        <span class="c1"># Functionalization can&#39;t easily tell us if an input had BOTH its metadata actual data mutated.</span>
                        <span class="c1"># So we check if metadata needs to be mutated here manually.</span>
                        <span class="n">original_inpt</span><span class="o">.</span><span class="n">as_strided_</span><span class="p">(</span>
                            <span class="n">updated_inpt</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span>
                            <span class="n">updated_inpt</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span>
                            <span class="n">updated_inpt</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">(),</span>
                        <span class="p">)</span>
                    <span class="n">original_inpt</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">updated_inpt</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">fw_outs</span> <span class="o">=</span> <span class="n">outs</span>

        <span class="c1"># Step 4: Manually regenerate any outputs that are aliased to inputs, instead of</span>
        <span class="c1"># compiling them.</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">num_outputs_aliased_to_inputs</span> <span class="o">&gt;</span> <span class="mi">0</span>
            <span class="ow">or</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">num_outputs_aliased_to_intermediates</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="p">):</span>
            <span class="k">assert</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">num_outputs_aliased_to_inputs</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">fw_outs</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span>
                <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">aliased_output_info</span>
            <span class="p">)</span>
            <span class="n">fw_outs_including_aliases</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="p">(</span>
                <span class="n">aliased_out_metadata</span>
            <span class="p">)</span> <span class="ow">in</span> <span class="n">CompiledFunction</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">aliased_output_info</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">aliased_out_metadata</span><span class="o">.</span><span class="n">output_type</span> <span class="o">==</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">non_alias</span><span class="p">:</span>
                    <span class="n">fw_outs_including_aliases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">fw_outs</span><span class="p">[</span><span class="n">aliased_out_metadata</span><span class="o">.</span><span class="n">base_idx</span><span class="p">]</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">aliased_out_metadata</span><span class="o">.</span><span class="n">output_type</span> <span class="o">==</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_input</span><span class="p">:</span>
                        <span class="n">aliased_base_tensor</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="n">aliased_out_metadata</span><span class="o">.</span><span class="n">base_idx</span><span class="p">]</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">assert</span> <span class="p">(</span>
                            <span class="n">aliased_out_metadata</span><span class="o">.</span><span class="n">output_type</span>
                            <span class="o">==</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">alias_of_intermediate</span>
                        <span class="p">)</span>
                        <span class="n">aliased_base_tensor</span> <span class="o">=</span> <span class="n">fw_outs</span><span class="p">[</span><span class="n">aliased_out_metadata</span><span class="o">.</span><span class="n">base_idx</span><span class="p">]</span>
                    <span class="c1"># Note: here, we manually regenerate the output, using an as_strided() call,</span>
                    <span class="c1"># OR if the aliased output came from a custom autograd.function, we replay it.</span>
                    <span class="c1"># The as_strided() in the normal case is good for perf (this is hot-path code,</span>
                    <span class="c1"># and we&#39;re consolidating potential chains of views into a single view op).</span>
                    <span class="c1"># But we might need to figure out view replaying for e.g. XLA.</span>
                    <span class="c1"># TODO: handle the custom autograd function case here.</span>
                    <span class="c1"># We need a way to check whether a tensor came from a custom autograd fn from python,</span>
                    <span class="c1"># AND a way to replay that custom view fn.</span>
                    <span class="n">fake_meta</span> <span class="o">=</span> <span class="n">aliased_out_metadata</span><span class="o">.</span><span class="n">tensor_meta</span>
                    <span class="k">if</span> <span class="n">fake_meta</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="c1"># This handles the specific case where the user returns an output that *was* an input. Don&#39;t create a view.</span>
                        <span class="n">fw_outs_including_aliases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">aliased_base_tensor</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="c1"># We need to grab the size/stride/storage_offset from the compiled forward,</span>
                        <span class="c1"># and use that to create a view off of the right input</span>
                        <span class="n">fake_meta</span> <span class="o">=</span> <span class="n">aliased_out_metadata</span><span class="o">.</span><span class="n">tensor_meta</span>
                        <span class="n">size_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">fake_meta</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
                        <span class="n">stride_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">fake_meta</span><span class="o">.</span><span class="n">stride</span><span class="p">())</span>
                        <span class="n">size_</span> <span class="o">=</span> <span class="n">aliasing_metadata_outs</span><span class="p">[</span>
                            <span class="n">aliased_out_metadata</span><span class="o">.</span><span class="n">sizes_idx</span> <span class="p">:</span> <span class="n">aliased_out_metadata</span><span class="o">.</span><span class="n">sizes_idx</span>
                            <span class="o">+</span> <span class="n">size_len</span>
                        <span class="p">]</span>
                        <span class="n">stride_</span> <span class="o">=</span> <span class="n">aliasing_metadata_outs</span><span class="p">[</span>
                            <span class="n">aliased_out_metadata</span><span class="o">.</span><span class="n">strides_idx</span> <span class="p">:</span> <span class="n">aliased_out_metadata</span><span class="o">.</span><span class="n">strides_idx</span>
                            <span class="o">+</span> <span class="n">stride_len</span>
                        <span class="p">]</span>
                        <span class="n">storage_offset_</span> <span class="o">=</span> <span class="n">aliasing_metadata_outs</span><span class="p">[</span>
                            <span class="n">aliased_out_metadata</span><span class="o">.</span><span class="n">storage_offset_idx</span>
                        <span class="p">]</span>
                        <span class="c1"># Create the output alias</span>
                        <span class="n">aliased_out</span> <span class="o">=</span> <span class="n">gen_alias_from_base</span><span class="p">(</span>
                            <span class="n">aliased_base_tensor</span><span class="p">,</span>
                            <span class="n">size_</span><span class="p">,</span>
                            <span class="n">stride_</span><span class="p">,</span>
                            <span class="n">storage_offset_</span><span class="p">,</span>
                            <span class="n">fake_meta</span><span class="p">,</span>
                        <span class="p">)</span>
                        <span class="n">fw_outs_including_aliases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">aliased_out</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">inner_out</span><span class="p">,</span> <span class="n">user_out</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">fw_outs</span><span class="p">,</span> <span class="n">fw_outs_including_aliases</span><span class="p">):</span>
                <span class="c1"># Sanity check assert</span>
                <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">inner_out</span><span class="p">)</span> <span class="o">==</span> <span class="nb">type</span><span class="p">(</span><span class="n">user_out</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">fw_outs_including_aliases</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">fw_outs</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">config</span><span class="o">.</span><span class="n">debug_assert</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">compiled_function</span>

    <span class="n">flat_requires_grad</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">flat_args</span>
    <span class="p">]</span>

    <span class="nd">@wraps</span><span class="p">(</span><span class="n">compiled_function</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">debug_compiled_function</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="c1"># TODO: Check aliasing relationships</span>
        <span class="c1"># TODO: Check strides for metadata mutation</span>
        <span class="c1"># (NB: ideally, this logic is factored out of this function and</span>
        <span class="c1"># you move these debug checks there)</span>

        <span class="c1"># Check requires grad.  Bad case is when we compiled with</span>
        <span class="c1"># requires_grad = False, but input requires_grad = True</span>
        <span class="c1"># (vice versa is OK; we compute a gradient and then throw</span>
        <span class="c1"># it away when it hits the input.)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
            <span class="n">can_require_grad</span> <span class="o">=</span> <span class="n">flat_requires_grad</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">can_require_grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">assert</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
            <span class="k">elif</span> <span class="ow">not</span> <span class="n">can_require_grad</span><span class="p">:</span>
                <span class="k">assert</span> <span class="ow">not</span> <span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">format_guard_bug_msg</span><span class="p">(</span>
                    <span class="n">aot_config</span><span class="p">,</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">describe_input</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">)</span><span class="si">}</span><span class="s2"> would not require grad&quot;</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="k">return</span> <span class="n">compiled_function</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">debug_compiled_function</span>


<span class="nd">@dynamo_timed</span>
<span class="k">def</span> <span class="nf">create_aot_dispatcher_function</span><span class="p">(</span>
    <span class="n">flat_fn</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">aot_config</span><span class="p">:</span> <span class="n">AOTConfig</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Traces the forward and backward graphs of the attr:`flat_fn` to generate a</span>
<span class="sd">    joint graph. The joint graph is an Fx graph with Aten ops. Please refer to</span>
<span class="sd">    the tracing mechanism to understand the graph capturing details.</span>

<span class="sd">    The joint graph is then passed through attr:`partition_fn` to isolate the</span>
<span class="sd">    forward and backward portions, which are then respectively compiled via the</span>
<span class="sd">    provided attr:`fw_compiler` and attr:`bw_compiler`.</span>

<span class="sd">    The resulting compiled forward and backward graphs are then wrapped up in a</span>
<span class="sd">    ``torch.autograd.Function`` object.</span>

<span class="sd">    The calling convention here is that the first aot_config.num_params_buffers</span>
<span class="sd">    inputs in flat_args are parameters and buffers, and the rest are inputs.</span>

<span class="sd">    We use this to assume that parameters/buffer&#39;s shapes don&#39;t change.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># This is the main entry point.</span>
    <span class="c1"># TODO: Chillee argues that dynamo itself should pass in fake tensors to</span>
    <span class="c1"># the list of arguments when compiling; at the moment we do not do this</span>

    <span class="k">if</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span> <span class="o">=</span> <span class="p">{</span>
        <span class="o">**</span><span class="n">aot_autograd_decompositions</span><span class="p">,</span>
        <span class="o">**</span><span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="n">log</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">log_level</span><span class="p">)</span>

    <span class="c1"># NB: don&#39;t bother setting allow_fallback_kernels; this should not actually</span>
    <span class="c1"># be configurable in fake tensor, we should automatically do the right</span>
    <span class="c1"># thing</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">debug_fake_cross_ref</span><span class="p">:</span>
        <span class="c1"># This is a little messy but TorchDynamo directly changes `use_fake_tensor`</span>
        <span class="c1"># so it&#39;s not enough for user to change the config manually</span>
        <span class="c1"># TODO: have TorchDynamo read in `use_fake_tensor` from os environ /</span>
        <span class="c1"># coordinate flags</span>
        <span class="n">config</span><span class="o">.</span><span class="n">use_fake_tensor</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">use_dynamic_shapes</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">config</span><span class="o">.</span><span class="n">use_fake_tensor</span><span class="p">,</span> <span class="s2">&quot;Dynamic shapes only works with fake tensor&quot;</span>

    <span class="c1"># Check flat_args to see if they&#39;re already fake.  If so, use that fake</span>
    <span class="c1"># mode instead.</span>

    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">flat_args</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">FakeTensor</span><span class="p">):</span>
            <span class="n">fake_mode</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">fake_mode</span>
            <span class="k">break</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">shape_env</span> <span class="o">=</span> <span class="n">ShapeEnv</span><span class="p">()</span> <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">use_dynamic_shapes</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">fake_mode</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">FakeTensorMode</span><span class="p">(</span><span class="n">shape_env</span><span class="o">=</span><span class="n">shape_env</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">use_fake_tensor</span>
            <span class="k">else</span> <span class="n">nullcontext</span><span class="p">()</span>
        <span class="p">)</span>

    <span class="n">cross_ref</span> <span class="o">=</span> <span class="n">CrossRefFakeMode</span><span class="p">()</span> <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">debug_fake_cross_ref</span> <span class="k">else</span> <span class="n">nullcontext</span><span class="p">()</span>
    <span class="n">python_dispatcher_mode</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">enable_python_dispatcher</span><span class="p">()</span> <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">use_dynamic_shapes</span> <span class="k">else</span> <span class="n">nullcontext</span><span class="p">()</span>
    <span class="p">)</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">set_multithreading_enabled</span><span class="p">(</span>
        <span class="kc">False</span>
    <span class="p">),</span> <span class="n">preserve_rng_state</span><span class="p">(),</span> <span class="n">cross_ref</span><span class="p">,</span> <span class="n">fake_mode</span><span class="p">,</span> <span class="n">python_dispatcher_mode</span><span class="p">:</span>

        <span class="k">def</span> <span class="nf">process_inputs</span><span class="p">(</span><span class="n">flat_args</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">use_fake_tensor</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">fake_mode</span><span class="p">,</span> <span class="n">FakeTensorMode</span><span class="p">):</span>

                <span class="k">def</span> <span class="nf">convert</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                        <span class="k">return</span> <span class="n">x</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">FakeTensor</span><span class="p">):</span>
                        <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">fake_mode</span> <span class="ow">is</span> <span class="n">fake_mode</span>
                        <span class="k">return</span> <span class="n">x</span>
                    <span class="k">if</span> <span class="p">(</span>
                        <span class="n">idx</span> <span class="o">&lt;</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">num_params_buffers</span>
                        <span class="ow">and</span> <span class="n">config</span><span class="o">.</span><span class="n">static_weight_shapes</span>
                    <span class="p">):</span>
                        <span class="k">return</span> <span class="n">fake_mode</span><span class="o">.</span><span class="n">from_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">static_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                    <span class="k">return</span> <span class="n">fake_mode</span><span class="o">.</span><span class="n">from_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">static_shapes</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

                <span class="k">return</span> <span class="p">[</span><span class="n">convert</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">flat_args</span>

        <span class="n">fake_flat_tensor_args</span> <span class="o">=</span> <span class="n">process_inputs</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)</span>

        <span class="n">needs_autograd</span> <span class="o">=</span> <span class="p">(</span>
            <span class="nb">any</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span>
                    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">fake_flat_tensor_args</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
                <span class="p">]</span>
            <span class="p">)</span>
            <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_grad_enabled</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="c1"># crappy version of dispatcher</span>
        <span class="c1"># TODO: Do this properly</span>
        <span class="k">if</span> <span class="n">needs_autograd</span><span class="p">:</span>
            <span class="n">compiler_fn</span> <span class="o">=</span> <span class="n">aot_dispatch_autograd</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">compiler_fn</span> <span class="o">=</span> <span class="n">aot_dispatch_base</span>

        <span class="n">compiler_fn</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">aot_wrapper_dedupe</span><span class="p">,</span> <span class="n">compiler_fn</span><span class="o">=</span><span class="n">compiler_fn</span><span class="p">)</span>
        <span class="c1"># You can put more passes here</span>

        <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">compiler_fn</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">,</span> <span class="n">fake_flat_tensor_args</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">compiled_fn</span><span class="p">,</span> <span class="s1">&#39;_boxed_call&#39;</span><span class="p">):</span>
            <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">make_boxed_func</span><span class="p">(</span><span class="n">compiled_fn</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">compiled_fn</span>


<span class="c1"># Inspired by autodidax (thanks!)</span>
<span class="k">class</span> <span class="nc">PytreeThunk</span><span class="p">:</span>
    <span class="n">spec</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># These are some kinda dumb microoptimizations that save about 3-4 us of overhead.</span>
    <span class="n">is_simple</span> <span class="o">=</span> <span class="p">(</span>
        <span class="kc">None</span>  <span class="c1"># if the output spec is a tuple/list, we won&#39;t bother unflattening it.</span>
    <span class="p">)</span>
    <span class="n">is_really_simple</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># if the output spec is a LeafSpec</span>

    <span class="k">def</span> <span class="nf">set</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spec</span><span class="p">):</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">spec</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">spec</span> <span class="o">==</span> <span class="n">spec</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">spec</span> <span class="o">=</span> <span class="n">spec</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">spec</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">]</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">pytree</span><span class="o">.</span><span class="n">LeafSpec</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">spec</span><span class="o">.</span><span class="n">children_specs</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_simple</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">spec</span><span class="p">,</span> <span class="n">pytree</span><span class="o">.</span><span class="n">LeafSpec</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_really_simple</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">unflatten</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_really_simple</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_simple</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x</span>
        <span class="k">return</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_unflatten</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">spec</span><span class="p">)</span>


<div class="viewcode-block" id="aot_function"><a class="viewcode-back" href="../../../generated/functorch.compile.aot_function.html#functorch.compile.aot_function">[docs]</a><span class="k">def</span> <span class="nf">aot_function</span><span class="p">(</span>
    <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">fw_compiler</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">bw_compiler</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">partition_fn</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">default_partition</span><span class="p">,</span>
    <span class="n">decompositions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_params_buffers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">hasher_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># deprecated</span>
    <span class="n">static_argnums</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># deprecated</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Traces the forward and backward graph of :attr:`fn` using torch dispatch</span>
<span class="sd">    mechanism, and then compiles the generated forward and backward graphs</span>
<span class="sd">    through :attr:`fw_compiler` and :attr:`bw_compiler`.</span>

<span class="sd">    :func:`aot_function` traces the forward and backward graph ahead of time,</span>
<span class="sd">    and generates a joint forward and backward graph.  :attr:`partition_fn` is</span>
<span class="sd">    then used to separate out forward and backward graphs. The partitioner</span>
<span class="sd">    function can be used to perform optimizations such as recomputation. One can</span>
<span class="sd">    set `decompositions` dictionary to decompose the operators into a sequence</span>
<span class="sd">    of core or simpler operators supported by the backend compilers.</span>

<span class="sd">    :func:`aot_function` uses a compilation cache, based on input tensor</span>
<span class="sd">    properties, to detect when there is a need of recompilation.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This API is experimental and likely to change.</span>

<span class="sd">    Args:</span>
<span class="sd">        fn (Callable): A Python function that takes one ore more arguments. Must</span>
<span class="sd">            return one or more Tensors.</span>
<span class="sd">        fw_compiler (Callable): A Python function that accepts an Fx graph with</span>
<span class="sd">            Aten ops and input args, and returns a Callable that semantically is</span>
<span class="sd">            equivalent to the input Fx graph.</span>
<span class="sd">        bw_compiler (Optional[Callable]): A Python function that accepts an</span>
<span class="sd">            Fx graph with Aten ops and input args, and returns a Callable that</span>
<span class="sd">            semantically is equivalent to the input Fx graph.  Default: None</span>
<span class="sd">            (when None, it defaults to the :attr:`fw_compiler`)</span>
<span class="sd">        partition_fn (Callable): A Python function that takes a joint forward</span>
<span class="sd">            and backward graph, and partitions it into separate forward and</span>
<span class="sd">            backward graphs.</span>
<span class="sd">        decompositions (Dict): A dictionary to define the decomposition of</span>
<span class="sd">            larger Aten ops into simpler or core Aten ops.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Returns a ``Callable`` that retains the eager behavior of the original</span>
<span class="sd">        :attr:`fn`, but with forward and backward graph compiled via</span>
<span class="sd">        :attr:`fw_compile` and :attr:`bw_compile`.</span>

<span class="sd">    A simple example usage of :func:`aot_function` is as follows. This example</span>
<span class="sd">    will print the forward and backward graphs of the function ``fn``</span>

<span class="sd">        &gt;&gt;&gt; fn = lambda x : x.sin().cos()</span>
<span class="sd">        &gt;&gt;&gt; def print_compile_fn(fx_module, args):</span>
<span class="sd">        &gt;&gt;&gt;     print(fx_module)</span>
<span class="sd">        &gt;&gt;&gt;     return fx_module</span>
<span class="sd">        &gt;&gt;&gt; aot_fn = aot_function(fn, print_compile_fn)</span>
<span class="sd">        &gt;&gt;&gt; x = torch.randn(4, 5, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; aot_fn(x)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">static_argnums</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;static_argnums has been deprecated - manually wrap your function or use torchdynamo.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">bw_compiler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">bw_compiler</span> <span class="o">=</span> <span class="n">fw_compiler</span>
    <span class="n">aot_config</span> <span class="o">=</span> <span class="n">AOTConfig</span><span class="p">(</span>
        <span class="n">fw_compiler</span><span class="o">=</span><span class="n">fw_compiler</span><span class="p">,</span>
        <span class="n">bw_compiler</span><span class="o">=</span><span class="n">bw_compiler</span><span class="p">,</span>
        <span class="n">partition_fn</span><span class="o">=</span><span class="n">partition_fn</span><span class="p">,</span>
        <span class="n">decompositions</span><span class="o">=</span><span class="n">decompositions</span><span class="p">,</span>
        <span class="n">num_params_buffers</span><span class="o">=</span><span class="n">num_params_buffers</span><span class="p">,</span>
        <span class="n">aot_id</span><span class="o">=</span><span class="nb">next</span><span class="p">(</span><span class="n">AOT_COUNTER</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="n">cached_res</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="nd">@wraps</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">returned_function</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">nonlocal</span> <span class="n">cached_res</span>
        <span class="c1"># Now flatten the tensor args</span>
        <span class="n">flat_args</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">((</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>

        <span class="c1"># Compile the function and save it in the cache</span>
        <span class="k">if</span> <span class="n">cached_res</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Save the args_spec for flat_tensor_args to unflatten while tracing</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">tensor_args_spec</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">((</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>
            <span class="n">out_spec</span> <span class="o">=</span> <span class="n">PytreeThunk</span><span class="p">()</span>

            <span class="k">def</span> <span class="nf">flat_fn</span><span class="p">(</span><span class="o">*</span><span class="n">flat_args</span><span class="p">):</span>
                <span class="c1"># The input are flattened tensor args. Prepare the args in the</span>
                <span class="c1"># order that original function expects. Add static args as well.</span>
                <span class="c1"># They will appear as tensor constants in the traced graph.</span>
                <span class="k">nonlocal</span> <span class="n">out_spec</span>
                <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_unflatten</span><span class="p">(</span><span class="n">flat_args</span><span class="p">,</span> <span class="n">tensor_args_spec</span><span class="p">)</span>
                <span class="n">tree_out</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
                <span class="n">flat_out</span><span class="p">,</span> <span class="n">spec</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">(</span><span class="n">tree_out</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">flat_out</span><span class="p">:</span>
                    <span class="n">is_known_type</span> <span class="o">=</span> <span class="kc">False</span>
                    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">KNOWN_TYPES</span><span class="p">:</span>
                        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">):</span>
                            <span class="n">is_known_type</span> <span class="o">=</span> <span class="kc">True</span>
                            <span class="k">break</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_known_type</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;Found </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="si">}</span><span class="s2"> in output, which is not a known type. &quot;</span>
                            <span class="s2">&quot;If this type holds tensors, you need to register a pytree for it. &quot;</span>
                            <span class="s2">&quot;See https://github.com/pytorch/functorch/issues/475 for a brief &quot;</span>
                            <span class="s2">&quot;explanation why. If you don&#39;t need to register a pytree, please &quot;</span>
                            <span class="s2">&quot;leave a comment explaining your use case and we&#39;ll make this more &quot;</span>
                            <span class="s2">&quot;ergonomic to deal with&quot;</span>
                        <span class="p">)</span>
                <span class="n">out_spec</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">spec</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">flat_out</span>

            <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">create_aot_dispatcher_function</span><span class="p">(</span>
                <span class="n">flat_fn</span><span class="p">,</span>
                <span class="n">flat_args</span><span class="p">,</span>
                <span class="n">aot_config</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">cached_res</span> <span class="o">=</span> <span class="p">(</span><span class="n">compiled_fn</span><span class="p">,</span> <span class="n">out_spec</span><span class="p">)</span>

        <span class="n">cached_fn</span><span class="p">,</span> <span class="n">out_spec</span> <span class="o">=</span> <span class="n">cached_res</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">cached_fn</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out_spec</span><span class="o">.</span><span class="n">unflatten</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">returned_function</span></div>


<div class="viewcode-block" id="aot_module"><a class="viewcode-back" href="../../../generated/functorch.compile.aot_module.html#functorch.compile.aot_module">[docs]</a><span class="k">def</span> <span class="nf">aot_module</span><span class="p">(</span><span class="n">mod</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Traces the forward and backward graph of :attr:`mod` using torch dispatch</span>
<span class="sd">    tracing mechanism. It is wrapper function, that underneath uses</span>
<span class="sd">    :func:`aot_function` to perform tracing and compilation.</span>

<span class="sd">    :func:`aot_module` lifts the parameters and buffers of ``nn.Module`` as inputs</span>
<span class="sd">    to a new callable which is then compiled through :func:`aot_function`.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This API is experimental and likely to change.</span>

<span class="sd">    Args:</span>
<span class="sd">        mod (Callable): A ``nn.Module`` module.</span>
<span class="sd">        args : args to be passed to :func:`aot_function`</span>
<span class="sd">        kwargs : kwargs to be passed to :func:`aot_function`</span>

<span class="sd">    Returns:</span>
<span class="sd">        Returns a ``nn.Module`` that retains the eager behavior of the original</span>
<span class="sd">        :attr:`mod`, but with forward and backward graph compiled.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># See Note: [Fake Modules and AOTAutograd]</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">assert_no_fake_params_or_buffers</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">functional_call</span><span class="p">(</span><span class="n">named_params</span><span class="p">,</span> <span class="n">named_buffers</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">params_and_buffers</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="n">named_params</span><span class="p">,</span> <span class="o">**</span><span class="n">named_buffers</span><span class="p">}</span>
        <span class="k">return</span> <span class="n">stateless</span><span class="o">.</span><span class="n">functional_call</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">params_and_buffers</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>

    <span class="n">named_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">_named_parameters</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">remove_duplicate</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
    <span class="n">named_buffers</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">_named_buffers</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">remove_duplicate</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
    <span class="n">num_params_buffers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">named_params</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">named_buffers</span><span class="p">)</span>
    <span class="n">compiled_f</span> <span class="o">=</span> <span class="n">aot_function</span><span class="p">(</span>
        <span class="n">functional_call</span><span class="p">,</span> <span class="n">num_params_buffers</span><span class="o">=</span><span class="n">num_params_buffers</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span>

    <span class="k">class</span> <span class="nc">AOTModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="nb">super</span><span class="p">(</span><span class="n">AOTModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">orig_module</span> <span class="o">=</span> <span class="n">mod</span>

        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">compiled_f</span><span class="p">(</span>
                <span class="n">named_params</span><span class="p">,</span>
                <span class="n">named_buffers</span><span class="p">,</span>
                <span class="o">*</span><span class="n">args</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">return</span> <span class="n">AOTModule</span><span class="p">()</span></div>


<span class="k">def</span> <span class="nf">aot_module_simplified</span><span class="p">(</span>
    <span class="n">mod</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="n">fw_compiler</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">bw_compiler</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">partition_fn</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">default_partition</span><span class="p">,</span>
    <span class="n">decompositions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">hasher_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">static_argnums</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This is the simplified or low overhead version of aot_module. For frontends</span>
<span class="sd">    like TorchDynamo, the input functions/modules to AOT are static and have</span>
<span class="sd">    unpacked inputs/outputs. This gives us an opportunity to remove the</span>
<span class="sd">        (1) pytree overhead to parse inputs/outputs,</span>
<span class="sd">        (2) AOT Autograd cache,</span>
<span class="sd">        (3) Reading of params/buffers in every forward call</span>

<span class="sd">    :func:`aot_module_simplified` removes these overheads.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1">#########################################################</span>

    <span class="c1"># Redudant with dynamo, but worth having in case this gets invoked elsewhere.</span>

    <span class="c1"># Note [Fake Modules and AOTAutograd]</span>
    <span class="c1">#</span>
    <span class="c1"># A simple heuristic for when to use fake versus real tensors is that fake tensors are for compile time</span>
    <span class="c1"># (when we don&#39;t want to actually run the compute, but we do want to know about metadata),</span>
    <span class="c1"># and real tensors are for runtime (when we actually want to do the compute.) However, in AOTAutograd,</span>
    <span class="c1"># modules are the exception: we always pass AOTAutograd modules with real tensors.</span>
    <span class="c1"># This is because AOTAutograd will produce a compiled function which needs to directly access any</span>
    <span class="c1"># parameters the compiled function may need, but these parameters will NOT be passed in by the caller (aka Dynamo).</span>
    <span class="c1"># So at compile time, the compiled function we produce must close over any parameters, and those parameters must be</span>
    <span class="c1"># real parameters, and we cannot do this unless at compile time we get a module with real tensors.</span>

    <span class="c1"># Even if Dynamo did pass all parameters explicitly at runtime, which would eliminate the need to close over</span>
    <span class="c1"># the parameters, it would still be profitable to pass real tensor parameters to the compiler at compile time,</span>
    <span class="c1"># because some compilation strategies like CUDA graphs want to burn in the pointer addresses where the parameter data live,</span>
    <span class="c1"># and of course we can&#39;t do that unless we give the backend a real tensor.</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">assert_no_fake_params_or_buffers</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span>

    <span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
        <span class="o">**</span><span class="nb">dict</span><span class="p">(</span><span class="n">_named_parameters</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">remove_duplicate</span><span class="o">=</span><span class="kc">False</span><span class="p">)),</span>
        <span class="o">**</span><span class="nb">dict</span><span class="p">(</span><span class="n">_named_buffers</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">remove_duplicate</span><span class="o">=</span><span class="kc">False</span><span class="p">)),</span>
    <span class="p">}</span>
    <span class="n">params_flat</span><span class="p">,</span> <span class="n">params_spec</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">params_flat</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">params_flat</span><span class="p">)</span>
    <span class="n">params_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">params_flat</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">functional_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">stateless</span><span class="o">.</span><span class="n">_reparametrize_module</span><span class="p">(</span>
            <span class="n">mod</span><span class="p">,</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_unflatten</span><span class="p">(</span><span class="n">args</span><span class="p">[:</span><span class="n">params_len</span><span class="p">],</span> <span class="n">params_spec</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">):</span>
                <span class="k">with</span> <span class="n">fx_traceback</span><span class="o">.</span><span class="n">override_stack_trace</span><span class="p">(),</span> <span class="n">warnings</span><span class="o">.</span><span class="n">catch_warnings</span><span class="p">():</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span>
                        <span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="s2">&quot;Anomaly Detection has been enabled.&quot;</span>
                    <span class="p">)</span>
                    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">detect_anomaly</span><span class="p">(</span><span class="n">check_nan</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
                        <span class="n">out</span> <span class="o">=</span> <span class="n">Interpreter</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">[</span><span class="n">params_len</span><span class="p">:],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">out</span> <span class="o">=</span> <span class="n">mod</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">[</span><span class="n">params_len</span><span class="p">:],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Graph output must be a tuple(). This is so that we can avoid &quot;</span>
                <span class="s2">&quot;pytree processing of the ouputs. Please change the module to &quot;</span>
                <span class="s2">&quot;have tuple outputs or use aot_module instead.&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">assert</span> <span class="n">static_argnums</span> <span class="ow">is</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">bw_compiler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">bw_compiler</span> <span class="o">=</span> <span class="n">fw_compiler</span>
    <span class="n">aot_config</span> <span class="o">=</span> <span class="n">AOTConfig</span><span class="p">(</span>
        <span class="n">fw_compiler</span><span class="o">=</span><span class="n">fw_compiler</span><span class="p">,</span>
        <span class="n">bw_compiler</span><span class="o">=</span><span class="n">bw_compiler</span><span class="p">,</span>
        <span class="n">partition_fn</span><span class="o">=</span><span class="n">partition_fn</span><span class="p">,</span>
        <span class="n">decompositions</span><span class="o">=</span><span class="n">decompositions</span><span class="p">,</span>
        <span class="n">num_params_buffers</span><span class="o">=</span><span class="n">params_len</span><span class="p">,</span>
        <span class="n">aot_id</span><span class="o">=</span><span class="nb">next</span><span class="p">(</span><span class="n">AOT_COUNTER</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="n">full_args</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">full_args</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">params_flat</span><span class="p">)</span>
    <span class="n">full_args</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>

    <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">create_aot_dispatcher_function</span><span class="p">(</span>
        <span class="n">functional_call</span><span class="p">,</span>
        <span class="n">full_args</span><span class="p">,</span>
        <span class="n">aot_config</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># TODO: There is something deeply wrong here; compiled_fn running with</span>
    <span class="c1"># the boxed calling convention, but aot_module_simplified somehow</span>
    <span class="c1"># historically returned a function that was not the boxed calling</span>
    <span class="c1"># convention.  This should get fixed...</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="o">*</span><span class="n">runtime_args</span><span class="p">):</span>
        <span class="n">full_args</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">full_args</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">params_flat</span><span class="p">)</span>
        <span class="n">full_args</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">runtime_args</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">compiled_fn</span><span class="p">(</span><span class="n">full_args</span><span class="p">)</span>

    <span class="c1"># Just for convenience</span>
    <span class="n">forward</span><span class="o">.</span><span class="n">zero_grad</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">zero_grad</span>
    <span class="n">forward</span><span class="o">.</span><span class="n">named_parameters</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">named_parameters</span>
    <span class="n">forward</span><span class="o">.</span><span class="n">named_buffers</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">named_buffers</span>

    <span class="k">return</span> <span class="n">forward</span>


<span class="n">compiled_function</span> <span class="o">=</span> <span class="n">aot_function</span>
<span class="n">compiled_module</span> <span class="o">=</span> <span class="n">aot_module</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/clipboard.min.js"></script>
         <script src="../../../_static/copybutton.js"></script>
         <script >let toggleHintShow = 'Click to show';</script>
         <script >let toggleHintHide = 'Click to hide';</script>
         <script >let toggleOpenOnPrint = 'true';</script>
         <script src="../../../_static/togglebutton.js"></script>
         <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>